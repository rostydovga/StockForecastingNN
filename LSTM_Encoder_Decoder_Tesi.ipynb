{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWVVPkLD71vx"
      },
      "source": [
        "# Installazioni ed import necessari"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir images/\n",
        "!mkdir save_mat/"
      ],
      "metadata": {
        "id": "2z9nYEkFDFky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qeDfyPb342Ml",
        "outputId": "bc43aac9-cffd-4527-9faa-18d4c7a3468f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (4.9.1)\n",
            "Collecting yfinance\n",
            "  Downloading yfinance-0.1.74-py2.py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from yfinance) (0.0.11)\n",
            "Collecting requests>=2.26\n",
            "  Downloading requests-2.28.1-py3-none-any.whl (62 kB)\n",
            "\u001b[K     |████████████████████████████████| 62 kB 1.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from yfinance) (1.21.6)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from yfinance) (1.3.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->yfinance) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->yfinance) (2022.2.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.24.0->yfinance) (1.15.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.26->yfinance) (2.10)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.26->yfinance) (2.1.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.26->yfinance) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.26->yfinance) (2022.6.15)\n",
            "Installing collected packages: requests, yfinance\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "Successfully installed requests-2.28.1 yfinance-0.1.74\n"
          ]
        }
      ],
      "source": [
        "!pip install lxml yfinance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6l764w5tZh1H",
        "outputId": "c26bf1ea-6720-425b-ae3c-629712634d8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pandas_ta\n",
            "  Downloading pandas_ta-0.3.14b.tar.gz (115 kB)\n",
            "\u001b[K     |████████████████████████████████| 115 kB 4.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from pandas_ta) (1.3.5)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas->pandas_ta) (1.21.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->pandas_ta) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->pandas_ta) (2022.2.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->pandas_ta) (1.15.0)\n",
            "Building wheels for collected packages: pandas-ta\n",
            "  Building wheel for pandas-ta (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pandas-ta: filename=pandas_ta-0.3.14b0-py3-none-any.whl size=218923 sha256=9f1d6c4dc75da91244875986070c1bbf723506fc0872f345d8b81b55d748b548\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/81/f0/cca85757840e4616a2c6b9fe12569d97d324c27cac60724c58\n",
            "Successfully built pandas-ta\n",
            "Installing collected packages: pandas-ta\n",
            "Successfully installed pandas-ta-0.3.14b0\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas_ta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_73-YuWDYtnF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "import pandas_ta as ta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BB1OnNjqBZ3G"
      },
      "outputs": [],
      "source": [
        "def get_categorical_tickers():\n",
        "    '''\n",
        "    This Function returns a dictionary of tickers for different industry types\n",
        "    :return:\n",
        "    ticker_dict: Dictionary of 9 different industry types with over 8 tickers each\n",
        "    tickerSymbols: Set of three tickers\n",
        "    '''\n",
        "    ticker_dict = {}\n",
        "    all_tickers = []\n",
        "    ticker_dict['energy'] = ['XOM', 'CVX', 'SHEL', 'PTR', 'TTE', 'BP', 'PBR', 'SNP', 'SLB', 'VLO']\n",
        "    ticker_dict['materials'] = ['BHP', 'LIN', 'RIO', 'DD', 'SHW', 'CTA-PB', 'APD']\n",
        "    ticker_dict['industrials'] = ['UPS', 'HON', 'LMT', 'BA', 'GE', 'MMM', 'RTX', 'CAT', 'WM', 'ABB', 'ETN', 'EMR',\n",
        "                                  'FDX', 'TRI']\n",
        "    ticker_dict['utilities'] = ['NEE', 'DUK', 'NGG', 'AEP', 'XEL','AWK' ,'ETR', 'PCG']\n",
        "    ticker_dict['healthcare'] = ['UNH', 'JNJ', 'PFE', 'NVO', 'TMO', 'MRK', 'AZN', 'NVS', 'DHR', 'AMGN', 'CVS', 'GSK',\n",
        "                                 'ZTS', 'GILD']\n",
        "    ticker_dict['financials'] = ['BRK-A', 'V', 'JPM', 'BAC', 'MA', 'WFC', 'C-PJ', 'MS', 'RY', 'AXP']\n",
        "    ticker_dict['discretionary'] = ['AMZN', 'TSLA', 'HD', 'BABA', 'TM', 'NKE', 'MCD', 'SBUX', 'F', 'MAR', 'GM', 'ORLY',\n",
        "                                     'HMC', 'CMG', 'HLT'] # tolgo 'LILI'\n",
        "    ticker_dict['staples'] = ['WMT', 'PG', 'KO', 'COST', 'PEP', 'BUD', 'UL', 'TGT', 'MDLZ', 'CL', 'DG', 'KHC', 'KDP',\n",
        "                              'HSY']\n",
        "    ticker_dict['IT'] = ['AAPL', 'MSFT', 'TSM', 'NVDA', 'AVGO', 'CSCO', 'ORCL', 'ACN', 'ADBE', 'INTC', 'CRM', 'TXN',\n",
        "                         'QCOM', 'AMD', 'IBM', 'SONY', 'AMAT', 'INFY', 'ADI', 'MU', 'LRCX']\n",
        "    ticker_dict['communication'] = ['GOOG', 'DIS', 'VZ', 'CMCSA', 'TMUS', 'T', 'NFLX', 'SNAP', 'VOD',\n",
        "                                    'TWTR', 'EA'] # rimuovo FB, BAIDU\n",
        "    ticker_dict['estate'] = ['PLD', 'AMT', 'CCI', 'EQIX', 'SPG', 'DLR', 'WELL', 'EQR', 'AVB', 'WY', 'INVH', 'MAA']\n",
        "    ticker_keys = []\n",
        "    for key in ticker_dict.keys():\n",
        "        ticker_keys.append(key)\n",
        "        all_tickers.append(ticker_dict[key])\n",
        "    ticker_dict['all'] = all_tickers\n",
        "    tickerSymbols = ['BRK-A', 'GOOG', 'MSFT']\n",
        "    return ticker_dict, tickerSymbols\n",
        "\n",
        "def get_company_names():\n",
        "    '''\n",
        "    Get a dictionary of search strings corresponding to different ticker labels\n",
        "    :return:\n",
        "    ticker_dict: Dictionary of search strings given a stock ticker\n",
        "    '''\n",
        "    ticker_dict = {}\n",
        "    all_tickers = []\n",
        "    ticker_dict['energy'] = {'XOM': 'Exxon Mobil', 'CVX': 'Chevron', 'SHEL': 'Shell', 'PTR': 'PetroChina',\n",
        "                             'TTE': 'TotalEnergies', 'BP': 'BP', 'PBR': 'Petroleo Brasileiro',\n",
        "                             'SNP': 'China Petroleum', 'SLB': 'Schlumberger', 'VLO': 'Valero'}\n",
        "    '''\n",
        "    ticker_dict['materials'] = ['BHP', 'LIN', 'RIO', 'DD', 'SHW', 'CTA-PB', 'APD']\n",
        "    ticker_dict['industrials'] = ['UPS', 'HON', 'LMT', 'BA', 'GE', 'MMM', 'RTX', 'CAT', 'WM', 'ABB', 'ETN', 'EMR',\n",
        "                                  'FDX', 'TRI']\n",
        "    ticker_dict['utilities'] = ['NEE', 'DUK', 'NGG', 'AEP', 'XEL','AWK' ,'ETR', 'PCG']\n",
        "    ticker_dict['healthcare'] = ['UNH', 'JNJ', 'PFE', 'NVO', 'TMO', 'MRK', 'AZN', 'NVS', 'DHR', 'AMGN', 'CVS', 'GSK',\n",
        "                                 'ZTS', 'GILD']\n",
        "    ticker_dict['financials'] = ['BRK-A', 'V', 'JPM', 'BAC', 'MA', 'WFC', 'C-PJ', 'MS', 'RY', 'AXP']\n",
        "    ticker_dict['discretionary'] = ['AMZN', 'TSLA', 'HD', 'BABA', 'TM', 'NKE', 'MCD', 'SBUX', 'F', 'MAR', 'GM', 'ORLY',\n",
        "                                    'LILI', 'HMC', 'CMG', 'HLT']\n",
        "    ticker_dict['staples'] = ['WMT', 'PG', 'KO', 'COST', 'PEP', 'BUD', 'UL', 'TGT', 'MDLZ', 'CL', 'DG', 'KHC', 'KDP',\n",
        "                              'HSY']\n",
        "    ticker_dict['IT'] = ['AAPL', 'MSFT', 'TSM', 'NVDA', 'AVGO', 'CSCO', 'ORCL', 'ACN', 'ADBE', 'INTC', 'CRM', 'TXN',\n",
        "                         'QCOM', 'AMD', 'IBM', 'SONY', 'AMAT', 'INFY', 'ADI', 'MU', 'LRCX']\n",
        "    ticker_dict['communication'] = ['GOOG', 'FB', 'DIS', 'VZ', 'CMCSA', 'TMUS', 'T', 'NFLX', 'SNAP', 'VOD', 'BAIDU',\n",
        "                                    'TWTR', 'EA']\n",
        "    ticker_dict['estate'] = ['PLD', 'AMT', 'CCI', 'EQIX', 'SPG', 'DLR', 'WELL', 'EQR', 'AVB', 'WY', 'INVH', 'MAA']\n",
        "    ticker_keys = []\n",
        "    for key in ticker_dict.keys():\n",
        "        ticker_keys.append(key)\n",
        "        all_tickers.append(ticker_dict[key])\n",
        "    ticker_dict['all'] = all_tickers\n",
        "    '''\n",
        "    return ticker_dict\n",
        "\n",
        "def cross_corr(a,b):\n",
        "    '''\n",
        "    Compute the cross-correlation between\n",
        "    :param a: Time-series data of first stock\n",
        "    :param b: Time-series data of second stock\n",
        "    :return: Cross-correlation of the two stocks that are input\n",
        "    '''\n",
        "    return (a*b).sum()/((a**2).sum()*(b**2).sum())**0.5\n",
        "\n",
        "def get_tick_values(tickerSymbol, start, end):\n",
        "    '''\n",
        "    Function to extract the time series data\n",
        "    :param tickerSymbol: String of stock ticker\n",
        "    :param start: String of starting date of the time-series data\n",
        "    :param end: String of ending date of the time-series data\n",
        "    :return: type(list): Time series data\n",
        "    '''\n",
        "    tickerData = yf.Ticker(tickerSymbol)\n",
        "    tickerDf = yf.download(tickerSymbol, start=start, end=end)\n",
        "    tickerDf = tickerDf['Adj Close']\n",
        "    data = tickerDf\n",
        "    return data.values\n",
        "\n",
        "def get_control_vector(val):\n",
        "    '''\n",
        "    Returns the mask of day instances where stock purchase/sell decisions are to be made\n",
        "    :param val: Input array of stock values\n",
        "    :return: np.array of decisions maks labels (-2/0/2)\n",
        "    '''\n",
        "    return np.diff(np.sign(np.diff(val)))\n",
        "\n",
        "def buy_and_sell_bot(val,controls):\n",
        "    '''\n",
        "    Returns the growth of investment over time as function of the input decision mask and the stock values\n",
        "    :param val: np.array of the actual stock value over time\n",
        "    :param controls: np.array of the control mask to make purchase/sell decisions\n",
        "    :return: np.array of percentage growth value of the invested stock\n",
        "    '''\n",
        "    inv = []\n",
        "    curr_val = 100\n",
        "    inds = np.where(controls)[0]\n",
        "    buy_inds = np.where(controls>0)[0]\n",
        "    sell_inds = np.where(controls<0)[0]\n",
        "    max_limit = sell_inds[-1] if sell_inds[-1]>buy_inds[-1] else buy_inds[-1]\n",
        "    for i in range(buy_inds[0]+2):\n",
        "        inv.append(curr_val)\n",
        "    for i in range(buy_inds[0],max_limit+1):\n",
        "        if controls[i]>0:\n",
        "            buy_val = val[i+1]\n",
        "        elif controls[i]<0:\n",
        "            sell_val = val[i+1]\n",
        "            curr_val = curr_val*sell_val/buy_val\n",
        "        inv.append(curr_val)\n",
        "    if max_limit+1!=len(controls):\n",
        "        for i in range(len(controls)-max_limit-1):\n",
        "            inv.append(curr_val)\n",
        "    return inv\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LCNTjPHt7wPy"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Dense\n",
        "from keras.layers import TimeDistributed\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras import optimizers\n",
        "from keras.callbacks import History\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Dropout, LSTM, Input, Activation, concatenate\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM"
      ],
      "metadata": {
        "id": "02q2F_ZWyK-X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM_ED_Model():\n",
        "    '''\n",
        "    Class to train and infer stock price for one particular stock using the encoder decoder model\n",
        "    '''\n",
        "    def __init__(self,tickerSymbol, start, end,\n",
        "                 past_history = 60, forward_look = 1, train_test_split = 0.8, batch_size = 30,\n",
        "                 epochs = 50, steps_per_epoch = 200, validation_steps = 50, verbose = 0,\n",
        "                 depth = 1, naive = False, values = 200, tickerSymbolList = None, LSTM_latent_dim = 20):\n",
        "        '''\n",
        "        Initialize parameters for the class\n",
        "        :param tickerSymbol: String of Ticker symbol to train on\n",
        "        :param start: String of start date of time-series data\n",
        "        :param end: String of end date of time-series data\n",
        "        :param past_history: Int of past number of days to look at\n",
        "        :param forward_look: Int of future days to predict at a time\n",
        "        :param train_test_split: Float of fraction train-test split\n",
        "        :param batch_size: Int of mini-batch size\n",
        "        :param epochs: Int of total number of epochs in training\n",
        "        :param steps_per_epoch: Int for total number of mini-batches to run over per epoch\n",
        "        :param validation_steps: Int of total number of steps to use while validating with the dev set\n",
        "        :param verbose: Int to decide to print training stage results\n",
        "        :param infer_train: Flag to carry out prediction on training set\n",
        "        :param depth: Int to decide depth of stacked LSTM\n",
        "        :param naive: Flag for deciding if we need a Vanila model\n",
        "        :param values: Int for number of days to predict for by iteratively updating the time-series histroy\n",
        "        :param plot_values: Flag to plot\n",
        "        :param plot_bot: Flag to plot the investment growth by the decision making bot\n",
        "        :param tickerSymbolList: List of tickers to train the model on\n",
        "        '''\n",
        "        self.tickerSymbol = tickerSymbol\n",
        "        self.start = start\n",
        "        self.end = end\n",
        "        self.past_history = past_history\n",
        "        self.forward_look = forward_look\n",
        "        self.train_test_split = train_test_split\n",
        "        self.batch_size = batch_size\n",
        "        self.epochs = epochs\n",
        "        self.steps_per_epoch = steps_per_epoch\n",
        "        self.validation_steps = validation_steps\n",
        "        self.verbose = verbose\n",
        "        self.values = values\n",
        "        self.depth = depth\n",
        "        self.naive = naive\n",
        "        self.custom_loss = False\n",
        "        self.LSTM_latent_dim = LSTM_latent_dim;\n",
        "        if tickerSymbolList == None:\n",
        "            self.tickerSymbolList = [tickerSymbol]\n",
        "        else:\n",
        "            self.tickerSymbolList = tickerSymbolList\n",
        "        tf.random.set_seed(1728)\n",
        "\n",
        "    def data_preprocess(self, dataset, iStart, iEnd, sHistory, forward_look=1):\n",
        "        '''\n",
        "        Preprocess the data to make either the test set or the train set\n",
        "        :param dataset: np.array of time-series data\n",
        "        :param iStart: int of index start\n",
        "        :param iEnd: int of index end\n",
        "        :param sHistory: int number of days in history that we need to look at\n",
        "        :param forward_look: int of number of days in the future that needs to predicted\n",
        "        :return: returns a list of test/train data\n",
        "        '''\n",
        "        self.data_enc = []\n",
        "        self.data_dec = []\n",
        "        self.target = []\n",
        "        iStart += sHistory\n",
        "        if iEnd is None:\n",
        "            iEnd = len(dataset) - forward_look\n",
        "        for i in range(iStart, iEnd):\n",
        "            indices_x = range(i - sHistory, i)  # set the order\n",
        "            indices_x_dec = range(i-1, i + forward_look-1)\n",
        "            indices_y_dec = range(i, i + forward_look)\n",
        "            reshape_entity_x = np.asarray([])\n",
        "            reshape_entity_x = np.append(reshape_entity_x, dataset[indices_x])  # Comment this out if there are multiple identifiers in the feature vector\n",
        "            reshape_entity_x_dec = np.asarray([])\n",
        "            reshape_entity_x_dec = np.append(reshape_entity_x_dec, dataset[indices_x_dec])  # Comment this out if there are multiple identifiers in the feature vector\n",
        "            reshape_entity_y_dec = np.asarray([])\n",
        "            reshape_entity_y_dec = np.append(reshape_entity_y_dec, dataset[indices_y_dec])  # Comment this out if there are multiple identifiers in the feature vector\n",
        "            self.data_enc.append(np.reshape(reshape_entity_x, (sHistory, 1)))  #\n",
        "            self.data_dec.append(np.reshape(reshape_entity_x_dec, (forward_look, 1)))\n",
        "            self.target.append(np.reshape(reshape_entity_y_dec, (forward_look, 1)))\n",
        "        self.data_enc = np.array(self.data_enc)\n",
        "        self.data_dec = np.array(self.data_dec)\n",
        "        self.target = np.array(self.target)\n",
        "\n",
        "    def plot_history_values(self):\n",
        "        '''\n",
        "        Plots time-series data of the chosen ticker\n",
        "        '''\n",
        "        tickerData = yf.Ticker(self.tickerSymbol)\n",
        "        tickerDf = yf.download(self.tickerSymbol, start=self.start, end=self.end)\n",
        "        tickerDf = tickerDf['Adj Close']\n",
        "        data = tickerDf\n",
        "        y = data\n",
        "        y.index = data.index\n",
        "        y.plot()\n",
        "        plt.title(f\"{self.tickerSymbol}\")\n",
        "        plt.ylabel(\"price\")\n",
        "        plt.show()\n",
        "\n",
        "    def get_ticker_values(self, option = 0):\n",
        "        '''\n",
        "        Get ticker values in a list\n",
        "        '''\n",
        "        if option == 0:\n",
        "            tickerData = yf.Ticker(self.tickerSymbol)\n",
        "            tickerDf = yf.download(self.tickerSymbol, start=self.start, end=self.end)\n",
        "            tickerDf = tickerDf['Close']\n",
        "            data = tickerDf\n",
        "            self.y = data.values\n",
        "        else:\n",
        "            # Write code for multiple tickers. Code below is not enough\n",
        "            tickerData = yf.Ticker(self.tickerSymbol)\n",
        "            tickerDf = yf.download(self.tickerSymbol, start=self.start, end=self.end)\n",
        "            tickerDf = tickerDf['Close']\n",
        "            data = tickerDf\n",
        "            self.y = data.values\n",
        "\n",
        "\n",
        "    def prepare_test_train(self):\n",
        "        '''\n",
        "        Create the dataset from the extracted time-series data\n",
        "        '''\n",
        "        training_size = int(self.y.size * self.train_test_split)\n",
        "        training_mean = self.y[:training_size].mean()  # get the average\n",
        "        training_std = self.y[:training_size].std()  # std = a measure of how far away individual measurements tend to be from the mean value of a data set.\n",
        "        self.y = (self.y - training_mean) / training_std  # prep data, use mean and standard deviation to maintain distribution and ratios\n",
        "        self.data_preprocess(self.y, 0, training_size, self.past_history, forward_look = self.forward_look)\n",
        "        self.xtrain, self.xtrain_dec, self.ytrain = self.data_enc, self.data_dec, self.target\n",
        "        self.data_preprocess(self.y, training_size, None, self.past_history, forward_look = self.forward_look)\n",
        "        self.xtest, self.xtest_dec, self.ytest = self.data_enc, self.data_dec, self.target\n",
        "\n",
        "        self.training_mean_normalization = training_mean\n",
        "        self.training_std_normalization = training_std\n",
        "\n",
        "    def normalization_variables(self):\n",
        "      return self.training_mean_normalization, self.training_std_normalization\n",
        "\n",
        "\n",
        "    def create_p_test_train(self):\n",
        "        '''\n",
        "        Prepare shuffled train and test data\n",
        "        '''\n",
        "        BATCH_SIZE = self.batch_size\n",
        "        BUFFER_SIZE = self.y.size\n",
        "        p_train = tf.data.Dataset.from_tensor_slices(((self.xtrain, self.xtrain_dec), self.ytrain))\n",
        "        self.p_train = p_train.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True).repeat()\n",
        "        p_test = tf.data.Dataset.from_tensor_slices(((self.xtest, self.xtest_dec), self.ytest))\n",
        "        self.p_test = p_test.batch(BATCH_SIZE).repeat()\n",
        "\n",
        "    def model_LSTM(self):\n",
        "        '''\n",
        "        Create the stacked LSTM model and train it using the shuffled train set\n",
        "        '''\n",
        "        latent_dim = self.LSTM_latent_dim\n",
        "        encoder_inputs = keras.Input(shape=(None, 1))\n",
        "        encoder = keras.layers.LSTM(latent_dim, return_state=True)  # Number of latent dimensions, defaults to 20\n",
        "        encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "\n",
        "        # We discard `encoder_outputs` and only keep the states.\n",
        "        encoder_states = [state_h, state_c]\n",
        "\n",
        "        # Set up the decoder, using `encoder_states` as initial state.\n",
        "        decoder_inputs = keras.Input(shape=(None, 1))\n",
        "        # We set up our decoder to return full output sequences,\n",
        "        # and to return internal states as well. We don't use the\n",
        "        # return states in the training model, but we will use them in inference.\n",
        "        decoder_lstm = keras.layers.LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "        decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
        "        decoder_dense = keras.layers.Dense(1, activation=None)\n",
        "        decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "        # Define the model that will turn\n",
        "        # `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
        "        self.model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "        self.model.compile(optimizer='Adam',\n",
        "                           loss='mse', metrics=['mse'])\n",
        "        self.create_p_test_train()\n",
        "        self.history  = self.model.fit(self.p_train,\n",
        "                                 epochs=self.epochs,\n",
        "                                 batch_size=self.batch_size,\n",
        "                                 steps_per_epoch=self.steps_per_epoch,\n",
        "                                 validation_data=self.p_test,\n",
        "                                 validation_steps=self.validation_steps,\n",
        "                                 verbose=self.verbose)\n",
        "\n",
        "    def model_inference_LSTM(self):\n",
        "        latent_dim = self.LSTM_latent_dim\n",
        "\n",
        "        encoder_inputs = self.model.input[0]  # input_1\n",
        "        encoder_outputs, state_h_enc, state_c_enc = self.model.layers[2].output  # lstm_1\n",
        "        encoder_states = [state_h_enc, state_c_enc]\n",
        "        self.encoder_model = keras.Model(encoder_inputs, encoder_states)\n",
        "\n",
        "        decoder_inputs = self.model.input[1]  # input_2\n",
        "        decoder_state_input_h = keras.Input(shape=(latent_dim,))\n",
        "        decoder_state_input_c = keras.Input(shape=(latent_dim,))\n",
        "        decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "        decoder_lstm = self.model.layers[3]\n",
        "        decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(\n",
        "            decoder_inputs, initial_state=decoder_states_inputs\n",
        "        )\n",
        "        decoder_states = [state_h_dec, state_c_dec]\n",
        "        decoder_dense = self.model.layers[4]\n",
        "        decoder_outputs = decoder_dense(decoder_outputs)\n",
        "        self.decoder_model = keras.Model(\n",
        "            [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states\n",
        "        )\n",
        "\n",
        "    def infer_values(self, xtest, ytest, ts):\n",
        "        '''\n",
        "        Infer values by using the test set\n",
        "        :param xtest: test dataset\n",
        "        :param ytest: actual value dataset\n",
        "        :param ts: tikcer symbol\n",
        "        :return: model variables that store predicted data\n",
        "        '''\n",
        "        self.pred = []\n",
        "        self.pred_update = []\n",
        "        self.usetest = xtest.copy()\n",
        "\n",
        "        # Predict encoder for x_test to get decoder inputs. Iteratively predict decoder output.\n",
        "        # This one is incomplete.\n",
        "        states_value = self.encoder_model.predict(xtest[0:1,:,:])\n",
        "        decoder_input = xtest[0:1, -1, :]  # choosing the most recent value to feed the decoder\n",
        "        for i in range(self.values):\n",
        "            new_pred, h, c = self.decoder_model.predict([decoder_input] + states_value)\n",
        "            y_pred = new_pred.reshape((-1, 1))\n",
        "            decoder_input = new_pred\n",
        "            states_value = [h, c]\n",
        "            self.pred_update.append(y_pred)\n",
        "        self.pred_update = np.array(self.pred_update)\n",
        "\n",
        "        # Predict encoder for x_test to get decoder inputs and use it for the decoder for one extra day.\n",
        "        states_value = self.encoder_model.predict(xtest)\n",
        "        decoder_input = xtest[:, -1, :]  # choosing the most recent value to feed the decoder\n",
        "        new_pred, h, c = self.decoder_model.predict([decoder_input] + states_value)\n",
        "        y_pred = new_pred[:self.values,0:1,0:1]\n",
        "        self.pred = y_pred\n",
        "\n",
        "\n",
        "        if self.forward_look>1:\n",
        "            self.RMS_error = self.history.history['mse'][-1]\n",
        "        else:\n",
        "            self.RMS_error = self.history.history['mse'][-1]\n",
        "\n",
        "    def infer_all_values(self, xtest, ytest, ts):\n",
        "        '''\n",
        "        Infer values by using the test set\n",
        "        :param xtest: test dataset\n",
        "        :param ytest: actual value dataset\n",
        "        :param ts: tikcer symbol\n",
        "        :return: model variables that store predicted data\n",
        "        '''\n",
        "        self.pred = []\n",
        "        self.pred_update = []\n",
        "        self.usetest = xtest.copy()\n",
        "\n",
        "        # Predict encoder for x_test to get decoder inputs. Iteratively predict decoder output.\n",
        "        # This one is incomplete.\n",
        "        states_value = self.encoder_model.predict(xtest[0:1,:,:])\n",
        "        decoder_input = xtest[0:1, -1, :]  # choosing the most recent value to feed the decoder\n",
        "        print(f'Infering {len(xtest)} values')\n",
        "        for i in range(len(xtest)):\n",
        "            new_pred, h, c = self.decoder_model.predict([decoder_input] + states_value)\n",
        "            y_pred = new_pred.reshape((-1, 1))\n",
        "            decoder_input = new_pred\n",
        "            states_value = [h, c]\n",
        "            self.pred_update.append(y_pred)\n",
        "        self.pred_update = np.array(self.pred_update)\n",
        "\n",
        "        # Predict encoder for x_test to get decoder inputs and use it for the decoder for one extra day.\n",
        "        states_value = self.encoder_model.predict(xtest)\n",
        "        decoder_input = xtest[:, -1, :]  # choosing the most recent value to feed the decoder\n",
        "        new_pred, h, c = self.decoder_model.predict([decoder_input] + states_value)\n",
        "        y_pred = new_pred[:len(xtest),0:1,0:1]\n",
        "        self.pred = y_pred\n",
        "\n",
        "\n",
        "        if self.forward_look>1:\n",
        "            self.RMS_error = self.history.history['mse'][-1]\n",
        "        else:\n",
        "            self.RMS_error = self.history.history['mse'][-1]\n",
        "\n",
        "    def get_predicted_values(self):\n",
        "      return self.xt, self.yt, self.pred, self.pred_update\n",
        "\n",
        "\n",
        "    def plot_test_values(self):\n",
        "        '''\n",
        "        Plot predicted values against actual values\n",
        "        '''\n",
        "        plt.figure()\n",
        "        if self.forward_look > 1:\n",
        "            print(\"Sorry, still working on this\\n\")\n",
        "            # plt.plot(self.ytest[:self.values, 0, 0],'*-', label='actual (%s)' % self.ts)\n",
        "            # plt.plot(self.pred[:,0,0],'o-', label='predicted (%s)' % self.ts)\n",
        "            # # plt.plot(self.pred_update[1:, 0], label='predicted (update)')\n",
        "            # plt.xlabel(\"Days\")\n",
        "            # plt.ylabel(\"Normalized stock price\")\n",
        "            # plt.title('The relative RMS error is %f' % self.RMS_error)\n",
        "            # plt.legend()\n",
        "            # plt.savefig('../images/ED_Stock_prediction_%d_%d_%d_%d_%s.png' % (\n",
        "            #     self.depth, int(self.naive), self.past_history, self.forward_look, self.ts))\n",
        "            # plt.figure()\n",
        "            # plt.plot(self.pred[1:, 0]-self.pred_update[1:,0], label='difference (%s)' % self.ts)\n",
        "        else:\n",
        "            plt.plot(self.ytest[:self.values, 0, 0], '-', label='actual (%s)' % self.ts)\n",
        "            plt.plot(self.pred[:, 0, 0], '-', label='predicted (%s)' % self.ts)\n",
        "            #plt.plot(self.pred_update[:, 0, 0], label='predicted (update)')\n",
        "            plt.xlabel(\"Days\")\n",
        "            plt.ylabel(\"Normalized stock price\")\n",
        "            plt.title('The relative RMS error is %f' % self.RMS_error)\n",
        "            plt.legend()\n",
        "            plt.savefig('images/ED_Stock_prediction_%d_%d_%d_%d_%s.png' % (\n",
        "                self.depth, int(self.naive), self.past_history, self.forward_look, self.ts))\n",
        "            plt.clf()\n",
        "            np.savez('save_mat/EDstore_%d_%d_%d_%d_%s_%s.png' % (\n",
        "                self.depth, int(self.naive), self.past_history, self.forward_look, self.ts, int(self.custom_loss)),\n",
        "                     y=self.yt[:self.values - 1, 0, 0], pred=self.pred[1:, 0], pred_up=self.pred_update[1:, 0])\n",
        "\n",
        "        print('The relative RMS error is %f' % self.RMS_error)\n",
        "\n",
        "    def arch_plot(self):\n",
        "        '''\n",
        "        Plot the network architecture\n",
        "        '''\n",
        "        dot_img_file = 'images/LSTM_ED_arch_depth%d_naive%d.png' %( self.depth, int(self.naive))\n",
        "        tf.keras.utils.plot_model(self.model, to_file=dot_img_file, show_shapes=True)\n",
        "    \t        \n",
        "    def full_workflow(self, model=None):\n",
        "        self.get_ticker_values()\n",
        "        self.prepare_test_train()\n",
        "        self.model_LSTM()\n",
        "        self.model_inference_LSTM()\n",
        "        if model is None:\n",
        "            self.xt = self.xtest\n",
        "            self.yt = self.ytest\n",
        "            self.ts = self.tickerSymbol\n",
        "        else:\n",
        "            self.xt = model.xtest\n",
        "            self.yt = model.ytest\n",
        "            self.ts = model.tickerSymbol\n",
        "        #self.infer_values(self.xt, self.yt, self.ts)\n",
        "        self.infer_all_values(self.xt, self.yt, self.ts)\n",
        "        # self.arch_plot()\n",
        "\n",
        "    def full_workflow_and_plot(self, model=None):\n",
        "        '''\n",
        "        Workflow to carry out the entire process end-to-end\n",
        "        :param model: Choose which model to use to predict inferred values\n",
        "        :return:\n",
        "        '''\n",
        "        self.full_workflow(model=model)\n",
        "        self.plot_test_values()\n",
        "\n",
        "    def get_model(self):\n",
        "      return self.model, self.encoder_model, self.decoder_model\n",
        "\n",
        "    def plot_bot_decision(self):\n",
        "        '''\n",
        "        calculate investment growth from the inferred prediction value and plot the resulting growth\n",
        "        '''\n",
        "        if self.forward_look > 1:\n",
        "            ideal = self.ytest[:self.values - 1, 0, 0]\n",
        "            pred = np.asarray(self.pred[0,1:, 0]).reshape(-1, )\n",
        "        else:\n",
        "            ideal = self.ytest[:self.values - 1,0,0]\n",
        "            pred = np.asarray(self.pred[0,1:,0]).reshape(-1, )\n",
        "        control_ideal = get_control_vector(ideal)\n",
        "        control_pred = get_control_vector(pred)\n",
        "        bot_ideal = buy_and_sell_bot(ideal, control_ideal)\n",
        "        bot_pred = buy_and_sell_bot(ideal, control_pred)\n",
        "        plt.figure()\n",
        "        plt.plot(bot_ideal, label='Ideal case (%.2f)' % bot_ideal[-1])\n",
        "        plt.plot(bot_pred, label='From prediction (%.2f)' % bot_pred[-1])\n",
        "        plt.plot(ideal / ideal[0] * 100.0, label='Stock value(%s)' % self.ts)\n",
        "        plt.xlabel(\"Days\")\n",
        "        plt.ylabel(\"Percentage growth\")\n",
        "        plt.legend()\n",
        "        plt.savefig('images/ED_Bot_prediction_%d_%d_%d_%d.png' % (\n",
        "        self.depth, int(self.naive), self.past_history, self.forward_look))\n",
        "        np.savez('save_mat/EDbot_%d_%d_%d_%d_%s_%s.png' % (\n",
        "            self.depth, int(self.naive), self.past_history, self.forward_look, self.ts, int(self.custom_loss)),\n",
        "                 ideal=bot_ideal, pred=bot_pred, pred_up=bot_pred_update)\n",
        "        plt.clf()\n"
      ],
      "metadata": {
        "id": "hAy3SMy3yM_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Funzione Investimento\n"
      ],
      "metadata": {
        "id": "CT-W6lOON32_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_investment(dataset ,initial_investment=1_000):\n",
        "  '''\n",
        "  Return the final value of investment\n",
        "  '''\n",
        "  open_close_prices = dataset[['Open', 'Close']]\n",
        "  decisions = dataset['operation'] # if 1 -> buy, else sell\n",
        "  total_cash = initial_investment\n",
        "  price_decision = 1\n",
        "\n",
        "  #creo il dizionario di analisi\n",
        "  stats = {}\n",
        "  stats['cash invested'] = []\n",
        "  stats['Buy at'] = []\n",
        "  stats['Close at'] = []\n",
        "  stats['n stock buyed'] = []\n",
        "  stats['Bot decision'] = []\n",
        "  stats['cash returned'] = []\n",
        "\n",
        "\n",
        "\n",
        "  # per le operazioni buy\n",
        "  for i in open_close_prices.index:\n",
        "    \n",
        "    open_price = open_close_prices['Open'].loc[i]\n",
        "    close_price = open_close_prices['Close'].loc[i]\n",
        "    if(decisions.loc[i] == 1):\n",
        "      #print(f'\\nBuy {n_stock} stock for {open_price}')\n",
        "      # a fine giornata chiudo la posizione aperta in precedenza\n",
        "      price_decision = close_price\n",
        "      price_open_operation = open_price\n",
        "      #print(f'Close with return at {cash}')\n",
        "    else:\n",
        "      # se decision sell\n",
        "      price_decision = open_price\n",
        "      price_open_operation = close_price\n",
        "\n",
        "    n_stock = total_cash / price_open_operation\n",
        "    cash = n_stock * price_decision\n",
        "\n",
        "    stats['cash invested'].append(total_cash)\n",
        "    stats['Buy at'].append(open_price)\n",
        "    stats['Close at'].append(close_price)\n",
        "    stats['n stock buyed'].append(n_stock)\n",
        "    stats['Bot decision'].append(decisions.loc[i])\n",
        "    stats['cash returned'].append(cash)\n",
        "    \n",
        "    total_cash = cash\n",
        "\n",
        "\n",
        "  return total_cash, stats\n"
      ],
      "metadata": {
        "id": "rIxDTKsRN7yg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TEST su indici intenazionali"
      ],
      "metadata": {
        "id": "ofNFzXy4y3Ne"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stock_list_exam = ['^GSPC', '^FTSE', '^GDAXI', '^NSEI']\n",
        "start = '2010-01-01'\n",
        "end = '2022-01-01'"
      ],
      "metadata": {
        "id": "GBB-xAV_ysNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stock = 'AAPL'"
      ],
      "metadata": {
        "id": "Gmg5SVKPy_4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = yf.download(stock, start, end)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UOPl8GrVzl5W",
        "outputId": "c3cde539-4fbd-4258-9630-071f51376ae9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "split = 0.8967\n",
        "index = int(len(df)*split)\n",
        "df.iloc[index]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MmbwD4WhzqA_",
        "outputId": "b0f3f9cb-135c-43a3-cb07-fcb9697ddb90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Open         1.157000e+02\n",
              "High         1.161200e+02\n",
              "Low          1.122500e+02\n",
              "Close        1.131600e+02\n",
              "Adj Close    1.118103e+02\n",
              "Volume       1.614982e+08\n",
              "Name: 2020-10-06 00:00:00, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "LSTM_1 = LSTM_ED_Model(tickerSymbol = stock , start = start, end = end, depth = 0, epochs=50, naive = True, verbose = True, train_test_split=0.8967)\n",
        "LSTM_1.full_workflow()"
      ],
      "metadata": {
        "id": "3XJMmA0Xy7pD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modello, encoder, decoder = LSTM_1.get_model()"
      ],
      "metadata": {
        "id": "vQeCZgXRW8-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LSTM_1.RMS_error"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ctxBaKwZKjv",
        "outputId": "a36fd063-f400-44ed-8e28-65ed057b002b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0016842910554260015"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.utils.plot_model(modello)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "id": "NyEaLLcXXCT0",
        "outputId": "94461f2e-4167-483d-f841-7c7f72336164"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xt, yt, pred, pred_update = LSTM_1.get_predicted_values()"
      ],
      "metadata": {
        "id": "r5ref_Sv2Yl0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xt.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBHkM9KX2yFP",
        "outputId": "a6d7ad79-3314-425f-9429-c20c6c7c0d0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(252, 60, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred.reshape(-1,1).shape, yt.reshape(-1,1).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iVXSourT3ogP",
        "outputId": "ef392c07-0127-4c1f-8f12-9c621c2ee4b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((252, 1), (252, 1))"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "yt[0], pred[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ivm1IhBwFoAp",
        "outputId": "75a4ac6b-602a-4108-8824-04caacda79ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[2.66075555]]), array([[2.625412]], dtype=float32))"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(18,10))\n",
        "plt.plot(yt.reshape(-1,1))\n",
        "plt.plot(pred.reshape(-1,1))"
      ],
      "metadata": {
        "id": "RSXaNOId2rT_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.iloc[-252:]['Close'].plot(figsize=(18,10))"
      ],
      "metadata": {
        "id": "wh1kQehl4p2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test = df.iloc[-252:].copy()"
      ],
      "metadata": {
        "id": "7c_uoKqfKWIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "close_norm = pd.Series(yt.reshape(-1), index=df_test.index)"
      ],
      "metadata": {
        "id": "ZJf_Ste4MKVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_r = pd.Series(pred.reshape(-1), index=df_test.index)"
      ],
      "metadata": {
        "id": "nIUZ_IAjKyMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test['pred'] = pred_r\n",
        "df_test['close_norm'] = close_norm"
      ],
      "metadata": {
        "id": "8LR80AXaLpVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "-hekrlukLwEq",
        "outputId": "2d8085b2-946d-44ed-99e8-f702e62b768d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                   Open         High          Low        Close    Adj Close  \\\n",
              "Date                                                                          \n",
              "2021-01-04  3764.610107  3769.989990  3662.709961  3700.649902  3700.649902   \n",
              "2021-01-05  3698.020020  3737.830078  3695.070068  3726.860107  3726.860107   \n",
              "2021-01-06  3712.199951  3783.040039  3705.340088  3748.139893  3748.139893   \n",
              "2021-01-07  3764.709961  3811.550049  3764.709961  3803.790039  3803.790039   \n",
              "2021-01-08  3815.050049  3826.689941  3783.600098  3824.679932  3824.679932   \n",
              "\n",
              "                Volume      pred  close_norm  \n",
              "Date                                          \n",
              "2021-01-04  5006680000  2.625412    2.660756  \n",
              "2021-01-05  4582620000  2.652255    2.574635  \n",
              "2021-01-06  6049970000  2.596833    2.615364  \n",
              "2021-01-07  5080870000  2.611685    2.648432  \n",
              "2021-01-08  4764180000  2.641599    2.734911  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c4e4dbc8-3305-48c4-9050-66aa49ca06b7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Adj Close</th>\n",
              "      <th>Volume</th>\n",
              "      <th>pred</th>\n",
              "      <th>close_norm</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2021-01-04</th>\n",
              "      <td>3764.610107</td>\n",
              "      <td>3769.989990</td>\n",
              "      <td>3662.709961</td>\n",
              "      <td>3700.649902</td>\n",
              "      <td>3700.649902</td>\n",
              "      <td>5006680000</td>\n",
              "      <td>2.625412</td>\n",
              "      <td>2.660756</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-01-05</th>\n",
              "      <td>3698.020020</td>\n",
              "      <td>3737.830078</td>\n",
              "      <td>3695.070068</td>\n",
              "      <td>3726.860107</td>\n",
              "      <td>3726.860107</td>\n",
              "      <td>4582620000</td>\n",
              "      <td>2.652255</td>\n",
              "      <td>2.574635</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-01-06</th>\n",
              "      <td>3712.199951</td>\n",
              "      <td>3783.040039</td>\n",
              "      <td>3705.340088</td>\n",
              "      <td>3748.139893</td>\n",
              "      <td>3748.139893</td>\n",
              "      <td>6049970000</td>\n",
              "      <td>2.596833</td>\n",
              "      <td>2.615364</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-01-07</th>\n",
              "      <td>3764.709961</td>\n",
              "      <td>3811.550049</td>\n",
              "      <td>3764.709961</td>\n",
              "      <td>3803.790039</td>\n",
              "      <td>3803.790039</td>\n",
              "      <td>5080870000</td>\n",
              "      <td>2.611685</td>\n",
              "      <td>2.648432</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-01-08</th>\n",
              "      <td>3815.050049</td>\n",
              "      <td>3826.689941</td>\n",
              "      <td>3783.600098</td>\n",
              "      <td>3824.679932</td>\n",
              "      <td>3824.679932</td>\n",
              "      <td>4764180000</td>\n",
              "      <td>2.641599</td>\n",
              "      <td>2.734911</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c4e4dbc8-3305-48c4-9050-66aa49ca06b7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c4e4dbc8-3305-48c4-9050-66aa49ca06b7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c4e4dbc8-3305-48c4-9050-66aa49ca06b7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mean_open = df[:-252].Open.mean()\n",
        "std_open = df[:-252].Open.std()"
      ],
      "metadata": {
        "id": "ERtwvdZ1M4dO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "open_norm = (df_test.Open - mean_open) / std_open"
      ],
      "metadata": {
        "id": "4hIB4z6DMhuo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_close = df[:-252].Close.mean()\n",
        "std_close = df[:-252].Close.std()"
      ],
      "metadata": {
        "id": "93taBJLuPFdz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "close_norm = (df_test.Close - mean_close) / std_close"
      ],
      "metadata": {
        "id": "OCCW9uB7Oxu4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test['open_norm'] = open_norm\n",
        "df_test['close_norm'] = close_norm"
      ],
      "metadata": {
        "id": "_9xQkqhuNNQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "PuBnPBisNdBx",
        "outputId": "2ffb8874-7b62-411b-8f23-332e4a7f31d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                   Open         High          Low        Close    Adj Close  \\\n",
              "Date                                                                          \n",
              "2021-01-04  3764.610107  3769.989990  3662.709961  3700.649902  3700.649902   \n",
              "2021-01-05  3698.020020  3737.830078  3695.070068  3726.860107  3726.860107   \n",
              "2021-01-06  3712.199951  3783.040039  3705.340088  3748.139893  3748.139893   \n",
              "2021-01-07  3764.709961  3811.550049  3764.709961  3803.790039  3803.790039   \n",
              "2021-01-08  3815.050049  3826.689941  3783.600098  3824.679932  3824.679932   \n",
              "\n",
              "                Volume      pred  close_norm  open_norm  decision  operation  \\\n",
              "Date                                                                           \n",
              "2021-01-04  5006680000  2.625412    2.405951   2.500958         1          1   \n",
              "2021-01-05  4582620000  2.652255    2.444797   2.402282         1          1   \n",
              "2021-01-06  6049970000  2.596833    2.476335   2.423295         1          1   \n",
              "2021-01-07  5080870000  2.611685    2.558814   2.501106         1          1   \n",
              "2021-01-08  4764180000  2.641599    2.589774   2.575702         1          1   \n",
              "\n",
              "            close_nomr  \n",
              "Date                    \n",
              "2021-01-04    2.574635  \n",
              "2021-01-05    2.615364  \n",
              "2021-01-06    2.648432  \n",
              "2021-01-07    2.734911  \n",
              "2021-01-08    2.767373  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d2b1c387-8772-4097-a878-eed23ea9476b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Adj Close</th>\n",
              "      <th>Volume</th>\n",
              "      <th>pred</th>\n",
              "      <th>close_norm</th>\n",
              "      <th>open_norm</th>\n",
              "      <th>decision</th>\n",
              "      <th>operation</th>\n",
              "      <th>close_nomr</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2021-01-04</th>\n",
              "      <td>3764.610107</td>\n",
              "      <td>3769.989990</td>\n",
              "      <td>3662.709961</td>\n",
              "      <td>3700.649902</td>\n",
              "      <td>3700.649902</td>\n",
              "      <td>5006680000</td>\n",
              "      <td>2.625412</td>\n",
              "      <td>2.405951</td>\n",
              "      <td>2.500958</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2.574635</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-01-05</th>\n",
              "      <td>3698.020020</td>\n",
              "      <td>3737.830078</td>\n",
              "      <td>3695.070068</td>\n",
              "      <td>3726.860107</td>\n",
              "      <td>3726.860107</td>\n",
              "      <td>4582620000</td>\n",
              "      <td>2.652255</td>\n",
              "      <td>2.444797</td>\n",
              "      <td>2.402282</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2.615364</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-01-06</th>\n",
              "      <td>3712.199951</td>\n",
              "      <td>3783.040039</td>\n",
              "      <td>3705.340088</td>\n",
              "      <td>3748.139893</td>\n",
              "      <td>3748.139893</td>\n",
              "      <td>6049970000</td>\n",
              "      <td>2.596833</td>\n",
              "      <td>2.476335</td>\n",
              "      <td>2.423295</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2.648432</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-01-07</th>\n",
              "      <td>3764.709961</td>\n",
              "      <td>3811.550049</td>\n",
              "      <td>3764.709961</td>\n",
              "      <td>3803.790039</td>\n",
              "      <td>3803.790039</td>\n",
              "      <td>5080870000</td>\n",
              "      <td>2.611685</td>\n",
              "      <td>2.558814</td>\n",
              "      <td>2.501106</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2.734911</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-01-08</th>\n",
              "      <td>3815.050049</td>\n",
              "      <td>3826.689941</td>\n",
              "      <td>3783.600098</td>\n",
              "      <td>3824.679932</td>\n",
              "      <td>3824.679932</td>\n",
              "      <td>4764180000</td>\n",
              "      <td>2.641599</td>\n",
              "      <td>2.589774</td>\n",
              "      <td>2.575702</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2.767373</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d2b1c387-8772-4097-a878-eed23ea9476b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d2b1c387-8772-4097-a878-eed23ea9476b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d2b1c387-8772-4097-a878-eed23ea9476b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 178
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_test['operation'] = np.where(df_test['pred'] > df_test['open_norm'], 1, 0 )"
      ],
      "metadata": {
        "id": "GK3PcLulNjLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tot, stats = calculate_investment(df_test)"
      ],
      "metadata": {
        "id": "t2PyMObLNv5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tot"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-YhnkDrPUc7",
        "outputId": "98eb17d2-5e0a-4ab0-d694-986e99023d4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1180.0420935826521"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(stats)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "doRaVObjOIzi",
        "outputId": "028ff38f-17f2-4bc9-ac85-e7cc980a6160"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     cash invested      Buy at    Close at  n stock buyed  Bot decision  \\\n",
              "0      1000.000000  133.520004  129.410004       7.489514             1   \n",
              "1       969.218091  128.889999  131.009995       7.519731             1   \n",
              "2       985.159884  127.720001  126.599998       7.713435             1   \n",
              "3       976.520816  128.360001  130.919998       7.607672             1   \n",
              "4       995.996438  132.429993  132.050003       7.520928             1   \n",
              "..             ...         ...         ...            ...           ...   \n",
              "247    1075.682431  177.089996  180.330002       5.965077             0   \n",
              "248    1056.355547  180.160004  179.289993       5.891882             0   \n",
              "249    1061.481546  179.330002  179.380005       5.917502             0   \n",
              "250    1061.185652  179.470001  178.199997       5.955026             0   \n",
              "251    1068.748561  178.089996  177.570007       6.018745             0   \n",
              "\n",
              "     cash returned  \n",
              "0       969.218091  \n",
              "1       985.159884  \n",
              "2       976.520816  \n",
              "3       995.996438  \n",
              "4       993.138563  \n",
              "..             ...  \n",
              "247    1056.355547  \n",
              "248    1061.481546  \n",
              "249    1061.185652  \n",
              "250    1068.748561  \n",
              "251    1071.878242  \n",
              "\n",
              "[252 rows x 6 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2b0b11b9-5e3c-424a-a49c-44089e15e8d0\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cash invested</th>\n",
              "      <th>Buy at</th>\n",
              "      <th>Close at</th>\n",
              "      <th>n stock buyed</th>\n",
              "      <th>Bot decision</th>\n",
              "      <th>cash returned</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1000.000000</td>\n",
              "      <td>133.520004</td>\n",
              "      <td>129.410004</td>\n",
              "      <td>7.489514</td>\n",
              "      <td>1</td>\n",
              "      <td>969.218091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>969.218091</td>\n",
              "      <td>128.889999</td>\n",
              "      <td>131.009995</td>\n",
              "      <td>7.519731</td>\n",
              "      <td>1</td>\n",
              "      <td>985.159884</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>985.159884</td>\n",
              "      <td>127.720001</td>\n",
              "      <td>126.599998</td>\n",
              "      <td>7.713435</td>\n",
              "      <td>1</td>\n",
              "      <td>976.520816</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>976.520816</td>\n",
              "      <td>128.360001</td>\n",
              "      <td>130.919998</td>\n",
              "      <td>7.607672</td>\n",
              "      <td>1</td>\n",
              "      <td>995.996438</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>995.996438</td>\n",
              "      <td>132.429993</td>\n",
              "      <td>132.050003</td>\n",
              "      <td>7.520928</td>\n",
              "      <td>1</td>\n",
              "      <td>993.138563</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>247</th>\n",
              "      <td>1075.682431</td>\n",
              "      <td>177.089996</td>\n",
              "      <td>180.330002</td>\n",
              "      <td>5.965077</td>\n",
              "      <td>0</td>\n",
              "      <td>1056.355547</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>248</th>\n",
              "      <td>1056.355547</td>\n",
              "      <td>180.160004</td>\n",
              "      <td>179.289993</td>\n",
              "      <td>5.891882</td>\n",
              "      <td>0</td>\n",
              "      <td>1061.481546</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>249</th>\n",
              "      <td>1061.481546</td>\n",
              "      <td>179.330002</td>\n",
              "      <td>179.380005</td>\n",
              "      <td>5.917502</td>\n",
              "      <td>0</td>\n",
              "      <td>1061.185652</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>250</th>\n",
              "      <td>1061.185652</td>\n",
              "      <td>179.470001</td>\n",
              "      <td>178.199997</td>\n",
              "      <td>5.955026</td>\n",
              "      <td>0</td>\n",
              "      <td>1068.748561</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>251</th>\n",
              "      <td>1068.748561</td>\n",
              "      <td>178.089996</td>\n",
              "      <td>177.570007</td>\n",
              "      <td>6.018745</td>\n",
              "      <td>0</td>\n",
              "      <td>1071.878242</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>252 rows × 6 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2b0b11b9-5e3c-424a-a49c-44089e15e8d0')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2b0b11b9-5e3c-424a-a49c-44089e15e8d0 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2b0b11b9-5e3c-424a-a49c-44089e15e8d0');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prelevo i dati\n",
        "td, _ = get_categorical_tickers()\n",
        "td.pop('all')\n",
        "diz = {}\n",
        "diz['stock'] = []\n",
        "diz['roi'] = []\n",
        "diz['gain'] = []\n",
        "\n",
        "for k in td.keys():\n",
        "\n",
        "  for stock in td[k][:3]:\n",
        "    # scarico i dati\n",
        "    df = yf.download(stock, start, end)\n",
        "\n",
        "    # creo il modello\n",
        "    LSTM_1 = LSTM_ED_Model(tickerSymbol = stock , start = start, end = end, depth = 0, epochs=50, naive = True, verbose = True, train_test_split=0.8967)\n",
        "    LSTM_1.full_workflow()\n",
        "\n",
        "    xt, yt, pred, pred_update = LSTM_1.get_predicted_values()\n",
        "    df_test = df[-pred.shape[0]:].copy()\n",
        "\n",
        "    pred_r = pd.Series(pred.reshape(-1), index=df_test.index)\n",
        "    df_test['pred'] = pred_r\n",
        "\n",
        "    mean_open = df[:-pred.shape[0]].Open.mean()\n",
        "    std_open = df[:-pred.shape[0]].Open.std()\n",
        "\n",
        "    open_norm = (df_test.Open - mean_open) / std_open\n",
        "    df_test['open_norm'] = open_norm\n",
        "    df_test['operation'] = np.where(df_test['pred'] > df_test['open_norm'], 1, 0 )\n",
        "    tot, stats = calculate_investment(df_test,df_test.Open.iloc[0])\n",
        "\n",
        "    roi = (tot - df_test.Open.iloc[0])/ df_test.Open.iloc[0]\n",
        "\n",
        "    diz['stock'].append(stock)\n",
        "    diz['roi'].append(roi)\n",
        "    diz['gain'].append(tot)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GEu5dHqw1f-T",
        "outputId": "9d9d9997-d8e4-444e-e40f-2bb090881472"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Epoch 1/50\n",
            "200/200 [==============================] - 12s 23ms/step - loss: 0.1909 - mse: 0.1909 - val_loss: 0.0633 - val_mse: 0.0633\n",
            "Epoch 2/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0156 - mse: 0.0156 - val_loss: 0.0391 - val_mse: 0.0391\n",
            "Epoch 3/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0116 - mse: 0.0116 - val_loss: 0.0134 - val_mse: 0.0134\n",
            "Epoch 4/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0149 - val_mse: 0.0149\n",
            "Epoch 5/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0086 - mse: 0.0086 - val_loss: 0.0106 - val_mse: 0.0106\n",
            "Epoch 6/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0078 - mse: 0.0078 - val_loss: 0.0109 - val_mse: 0.0109\n",
            "Epoch 7/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0074 - mse: 0.0074 - val_loss: 0.0088 - val_mse: 0.0088\n",
            "Epoch 8/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0070 - mse: 0.0070 - val_loss: 0.0127 - val_mse: 0.0127\n",
            "Epoch 9/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0068 - mse: 0.0068 - val_loss: 0.0081 - val_mse: 0.0081\n",
            "Epoch 10/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0066 - mse: 0.0066 - val_loss: 0.0105 - val_mse: 0.0105\n",
            "Epoch 11/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0065 - mse: 0.0065 - val_loss: 0.0075 - val_mse: 0.0075\n",
            "Epoch 12/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0063 - mse: 0.0063 - val_loss: 0.0088 - val_mse: 0.0088\n",
            "Epoch 13/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0063 - mse: 0.0063 - val_loss: 0.0109 - val_mse: 0.0109\n",
            "Epoch 14/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0062 - mse: 0.0062 - val_loss: 0.0084 - val_mse: 0.0084\n",
            "Epoch 15/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0061 - mse: 0.0061 - val_loss: 0.0072 - val_mse: 0.0072\n",
            "Epoch 16/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0060 - mse: 0.0060 - val_loss: 0.0087 - val_mse: 0.0087\n",
            "Epoch 17/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0061 - mse: 0.0061 - val_loss: 0.0078 - val_mse: 0.0078\n",
            "Epoch 18/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0060 - mse: 0.0060 - val_loss: 0.0098 - val_mse: 0.0098\n",
            "Epoch 19/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0059 - mse: 0.0059 - val_loss: 0.0074 - val_mse: 0.0074\n",
            "Epoch 20/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0060 - mse: 0.0060 - val_loss: 0.0075 - val_mse: 0.0075\n",
            "Epoch 21/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0061 - mse: 0.0061 - val_loss: 0.0080 - val_mse: 0.0080\n",
            "Epoch 22/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0059 - mse: 0.0059 - val_loss: 0.0069 - val_mse: 0.0069\n",
            "Epoch 23/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0060 - mse: 0.0060 - val_loss: 0.0109 - val_mse: 0.0109\n",
            "Epoch 24/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0061 - mse: 0.0061 - val_loss: 0.0090 - val_mse: 0.0090\n",
            "Epoch 25/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0060 - mse: 0.0060 - val_loss: 0.0075 - val_mse: 0.0075\n",
            "Epoch 26/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0058 - mse: 0.0058 - val_loss: 0.0072 - val_mse: 0.0072\n",
            "Epoch 27/50\n",
            "200/200 [==============================] - 5s 27ms/step - loss: 0.0059 - mse: 0.0059 - val_loss: 0.0070 - val_mse: 0.0070\n",
            "Epoch 28/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0060 - mse: 0.0060 - val_loss: 0.0071 - val_mse: 0.0071\n",
            "Epoch 29/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0059 - mse: 0.0059 - val_loss: 0.0089 - val_mse: 0.0089\n",
            "Epoch 30/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0059 - mse: 0.0059 - val_loss: 0.0085 - val_mse: 0.0085\n",
            "Epoch 31/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0060 - mse: 0.0060 - val_loss: 0.0081 - val_mse: 0.0081\n",
            "Epoch 32/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0059 - mse: 0.0059 - val_loss: 0.0082 - val_mse: 0.0082\n",
            "Epoch 33/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0059 - mse: 0.0059 - val_loss: 0.0111 - val_mse: 0.0111\n",
            "Epoch 34/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0059 - mse: 0.0059 - val_loss: 0.0072 - val_mse: 0.0072\n",
            "Epoch 35/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0059 - mse: 0.0059 - val_loss: 0.0072 - val_mse: 0.0072\n",
            "Epoch 36/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0060 - mse: 0.0060 - val_loss: 0.0074 - val_mse: 0.0074\n",
            "Epoch 37/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0059 - mse: 0.0059 - val_loss: 0.0070 - val_mse: 0.0070\n",
            "Epoch 38/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0059 - mse: 0.0059 - val_loss: 0.0070 - val_mse: 0.0070\n",
            "Epoch 39/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0058 - mse: 0.0058 - val_loss: 0.0074 - val_mse: 0.0074\n",
            "Epoch 40/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0059 - mse: 0.0059 - val_loss: 0.0071 - val_mse: 0.0071\n",
            "Epoch 41/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0059 - mse: 0.0059 - val_loss: 0.0106 - val_mse: 0.0106\n",
            "Epoch 42/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0058 - mse: 0.0058 - val_loss: 0.0086 - val_mse: 0.0086\n",
            "Epoch 43/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0059 - mse: 0.0059 - val_loss: 0.0077 - val_mse: 0.0077\n",
            "Epoch 44/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0058 - mse: 0.0058 - val_loss: 0.0073 - val_mse: 0.0073\n",
            "Epoch 45/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0059 - mse: 0.0059 - val_loss: 0.0072 - val_mse: 0.0072\n",
            "Epoch 46/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0058 - mse: 0.0058 - val_loss: 0.0237 - val_mse: 0.0237\n",
            "Epoch 47/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0060 - mse: 0.0060 - val_loss: 0.0073 - val_mse: 0.0073\n",
            "Epoch 48/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0058 - mse: 0.0058 - val_loss: 0.0080 - val_mse: 0.0080\n",
            "Epoch 49/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0059 - mse: 0.0059 - val_loss: 0.0100 - val_mse: 0.0100\n",
            "Epoch 50/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0058 - mse: 0.0058 - val_loss: 0.0081 - val_mse: 0.0081\n",
            "Infering 252 values\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Epoch 1/50\n",
            "200/200 [==============================] - 7s 21ms/step - loss: 0.1749 - mse: 0.1749 - val_loss: 0.0264 - val_mse: 0.0264\n",
            "Epoch 2/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0232 - mse: 0.0232 - val_loss: 0.0186 - val_mse: 0.0186\n",
            "Epoch 3/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0165 - mse: 0.0165 - val_loss: 0.0148 - val_mse: 0.0148\n",
            "Epoch 4/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0140 - mse: 0.0140 - val_loss: 0.0140 - val_mse: 0.0140\n",
            "Epoch 5/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0128 - mse: 0.0128 - val_loss: 0.0123 - val_mse: 0.0123\n",
            "Epoch 6/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0118 - mse: 0.0118 - val_loss: 0.0146 - val_mse: 0.0146\n",
            "Epoch 7/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.0119 - val_mse: 0.0119\n",
            "Epoch 8/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0113 - mse: 0.0113 - val_loss: 0.0126 - val_mse: 0.0126\n",
            "Epoch 9/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0116 - mse: 0.0116 - val_loss: 0.0122 - val_mse: 0.0122\n",
            "Epoch 10/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0111 - mse: 0.0111 - val_loss: 0.0130 - val_mse: 0.0130\n",
            "Epoch 11/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0113 - mse: 0.0113 - val_loss: 0.0116 - val_mse: 0.0116\n",
            "Epoch 12/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0112 - mse: 0.0112 - val_loss: 0.0114 - val_mse: 0.0114\n",
            "Epoch 13/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0110 - mse: 0.0110 - val_loss: 0.0118 - val_mse: 0.0118\n",
            "Epoch 14/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0114 - mse: 0.0114 - val_loss: 0.0114 - val_mse: 0.0114\n",
            "Epoch 15/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0113 - val_mse: 0.0113\n",
            "Epoch 16/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0110 - mse: 0.0110 - val_loss: 0.0117 - val_mse: 0.0117\n",
            "Epoch 17/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0113 - mse: 0.0113 - val_loss: 0.0116 - val_mse: 0.0116\n",
            "Epoch 18/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0108 - mse: 0.0108 - val_loss: 0.0123 - val_mse: 0.0123\n",
            "Epoch 19/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0110 - mse: 0.0110 - val_loss: 0.0115 - val_mse: 0.0115\n",
            "Epoch 20/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0117 - val_mse: 0.0117\n",
            "Epoch 21/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0110 - mse: 0.0110 - val_loss: 0.0115 - val_mse: 0.0115\n",
            "Epoch 22/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0108 - mse: 0.0108 - val_loss: 0.0115 - val_mse: 0.0115\n",
            "Epoch 23/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0111 - mse: 0.0111 - val_loss: 0.0135 - val_mse: 0.0135\n",
            "Epoch 24/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0110 - mse: 0.0110 - val_loss: 0.0116 - val_mse: 0.0116\n",
            "Epoch 25/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0114 - val_mse: 0.0114\n",
            "Epoch 26/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0110 - mse: 0.0110 - val_loss: 0.0120 - val_mse: 0.0120\n",
            "Epoch 27/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0113 - val_mse: 0.0113\n",
            "Epoch 28/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0110 - mse: 0.0110 - val_loss: 0.0115 - val_mse: 0.0115\n",
            "Epoch 29/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0108 - mse: 0.0108 - val_loss: 0.0120 - val_mse: 0.0120\n",
            "Epoch 30/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0107 - mse: 0.0107 - val_loss: 0.0113 - val_mse: 0.0113\n",
            "Epoch 31/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0108 - mse: 0.0108 - val_loss: 0.0120 - val_mse: 0.0120\n",
            "Epoch 32/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0123 - val_mse: 0.0123\n",
            "Epoch 33/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0107 - mse: 0.0107 - val_loss: 0.0120 - val_mse: 0.0120\n",
            "Epoch 34/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0107 - mse: 0.0107 - val_loss: 0.0116 - val_mse: 0.0116\n",
            "Epoch 35/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.0116 - val_mse: 0.0116\n",
            "Epoch 36/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0116 - val_mse: 0.0116\n",
            "Epoch 37/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0105 - mse: 0.0105 - val_loss: 0.0115 - val_mse: 0.0115\n",
            "Epoch 38/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0117 - val_mse: 0.0117\n",
            "Epoch 39/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.0119 - val_mse: 0.0119\n",
            "Epoch 40/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0105 - mse: 0.0105 - val_loss: 0.0116 - val_mse: 0.0116\n",
            "Epoch 41/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0107 - mse: 0.0107 - val_loss: 0.0121 - val_mse: 0.0121\n",
            "Epoch 42/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0120 - val_mse: 0.0120\n",
            "Epoch 43/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0107 - mse: 0.0107 - val_loss: 0.0114 - val_mse: 0.0114\n",
            "Epoch 44/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0105 - mse: 0.0105 - val_loss: 0.0118 - val_mse: 0.0118\n",
            "Epoch 45/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.0116 - val_mse: 0.0116\n",
            "Epoch 46/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0105 - mse: 0.0105 - val_loss: 0.0135 - val_mse: 0.0135\n",
            "Epoch 47/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.0128 - val_mse: 0.0128\n",
            "Epoch 48/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.0114 - val_mse: 0.0114\n",
            "Epoch 49/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0122 - val_mse: 0.0122\n",
            "Epoch 50/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.0123 - val_mse: 0.0123\n",
            "Infering 252 values\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Epoch 1/50\n",
            "200/200 [==============================] - 8s 23ms/step - loss: 0.1946 - mse: 0.1946 - val_loss: 0.0269 - val_mse: 0.0269\n",
            "Epoch 2/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0191 - mse: 0.0191 - val_loss: 0.0233 - val_mse: 0.0233\n",
            "Epoch 3/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0148 - mse: 0.0148 - val_loss: 0.0097 - val_mse: 0.0097\n",
            "Epoch 4/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0119 - mse: 0.0119 - val_loss: 0.0115 - val_mse: 0.0115\n",
            "Epoch 5/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.0069 - val_mse: 0.0069\n",
            "Epoch 6/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0087 - mse: 0.0087 - val_loss: 0.0062 - val_mse: 0.0062\n",
            "Epoch 7/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0088 - mse: 0.0088 - val_loss: 0.0060 - val_mse: 0.0060\n",
            "Epoch 8/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0081 - mse: 0.0081 - val_loss: 0.0128 - val_mse: 0.0128\n",
            "Epoch 9/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0081 - mse: 0.0081 - val_loss: 0.0059 - val_mse: 0.0059\n",
            "Epoch 10/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0080 - mse: 0.0080 - val_loss: 0.0083 - val_mse: 0.0083\n",
            "Epoch 11/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0077 - mse: 0.0077 - val_loss: 0.0058 - val_mse: 0.0058\n",
            "Epoch 12/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0075 - mse: 0.0075 - val_loss: 0.0083 - val_mse: 0.0083\n",
            "Epoch 13/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0077 - mse: 0.0077 - val_loss: 0.0072 - val_mse: 0.0072\n",
            "Epoch 14/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0077 - mse: 0.0077 - val_loss: 0.0060 - val_mse: 0.0060\n",
            "Epoch 15/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0075 - mse: 0.0075 - val_loss: 0.0063 - val_mse: 0.0063\n",
            "Epoch 16/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0074 - mse: 0.0074 - val_loss: 0.0073 - val_mse: 0.0073\n",
            "Epoch 17/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0076 - mse: 0.0076 - val_loss: 0.0058 - val_mse: 0.0058\n",
            "Epoch 18/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0073 - mse: 0.0073 - val_loss: 0.0064 - val_mse: 0.0064\n",
            "Epoch 19/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0075 - mse: 0.0075 - val_loss: 0.0061 - val_mse: 0.0061\n",
            "Epoch 20/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0075 - mse: 0.0075 - val_loss: 0.0083 - val_mse: 0.0083\n",
            "Epoch 21/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0074 - mse: 0.0074 - val_loss: 0.0059 - val_mse: 0.0059\n",
            "Epoch 22/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0074 - mse: 0.0074 - val_loss: 0.0058 - val_mse: 0.0058\n",
            "Epoch 23/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0075 - mse: 0.0075 - val_loss: 0.0076 - val_mse: 0.0076\n",
            "Epoch 24/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0073 - mse: 0.0073 - val_loss: 0.0083 - val_mse: 0.0083\n",
            "Epoch 25/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0074 - mse: 0.0074 - val_loss: 0.0071 - val_mse: 0.0071\n",
            "Epoch 26/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0073 - mse: 0.0073 - val_loss: 0.0057 - val_mse: 0.0057\n",
            "Epoch 27/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0073 - mse: 0.0073 - val_loss: 0.0066 - val_mse: 0.0066\n",
            "Epoch 28/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0075 - mse: 0.0075 - val_loss: 0.0062 - val_mse: 0.0062\n",
            "Epoch 29/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0075 - mse: 0.0075 - val_loss: 0.0085 - val_mse: 0.0085\n",
            "Epoch 30/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0072 - mse: 0.0072 - val_loss: 0.0072 - val_mse: 0.0072\n",
            "Epoch 31/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0075 - mse: 0.0075 - val_loss: 0.0059 - val_mse: 0.0059\n",
            "Epoch 32/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0071 - mse: 0.0071 - val_loss: 0.0060 - val_mse: 0.0060\n",
            "Epoch 33/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0073 - mse: 0.0073 - val_loss: 0.0134 - val_mse: 0.0134\n",
            "Epoch 34/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0072 - mse: 0.0072 - val_loss: 0.0062 - val_mse: 0.0062\n",
            "Epoch 35/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0072 - mse: 0.0072 - val_loss: 0.0069 - val_mse: 0.0069\n",
            "Epoch 36/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0073 - mse: 0.0073 - val_loss: 0.0061 - val_mse: 0.0061\n",
            "Epoch 37/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0073 - mse: 0.0073 - val_loss: 0.0062 - val_mse: 0.0062\n",
            "Epoch 38/50\n",
            "200/200 [==============================] - 4s 17ms/step - loss: 0.0074 - mse: 0.0074 - val_loss: 0.0058 - val_mse: 0.0058\n",
            "Epoch 39/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0071 - mse: 0.0071 - val_loss: 0.0062 - val_mse: 0.0062\n",
            "Epoch 40/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0073 - mse: 0.0073 - val_loss: 0.0060 - val_mse: 0.0060\n",
            "Epoch 41/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0073 - mse: 0.0073 - val_loss: 0.0089 - val_mse: 0.0089\n",
            "Epoch 42/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0071 - mse: 0.0071 - val_loss: 0.0069 - val_mse: 0.0069\n",
            "Epoch 43/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0073 - mse: 0.0073 - val_loss: 0.0066 - val_mse: 0.0066\n",
            "Epoch 44/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0072 - mse: 0.0072 - val_loss: 0.0060 - val_mse: 0.0060\n",
            "Epoch 45/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0072 - mse: 0.0072 - val_loss: 0.0083 - val_mse: 0.0083\n",
            "Epoch 46/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0072 - mse: 0.0072 - val_loss: 0.0145 - val_mse: 0.0145\n",
            "Epoch 47/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0071 - mse: 0.0071 - val_loss: 0.0057 - val_mse: 0.0057\n",
            "Epoch 48/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0071 - mse: 0.0071 - val_loss: 0.0067 - val_mse: 0.0067\n",
            "Epoch 49/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0072 - mse: 0.0072 - val_loss: 0.0065 - val_mse: 0.0065\n",
            "Epoch 50/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0071 - mse: 0.0071 - val_loss: 0.0071 - val_mse: 0.0071\n",
            "Infering 252 values\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Epoch 1/50\n",
            "200/200 [==============================] - 7s 22ms/step - loss: 0.1722 - mse: 0.1722 - val_loss: 0.0198 - val_mse: 0.0198\n",
            "Epoch 2/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0108 - mse: 0.0108 - val_loss: 0.0152 - val_mse: 0.0152\n",
            "Epoch 3/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0089 - mse: 0.0089 - val_loss: 0.0120 - val_mse: 0.0120\n",
            "Epoch 4/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0071 - mse: 0.0071 - val_loss: 0.0102 - val_mse: 0.0102\n",
            "Epoch 5/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0063 - mse: 0.0063 - val_loss: 0.0089 - val_mse: 0.0089\n",
            "Epoch 6/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0056 - mse: 0.0056 - val_loss: 0.0081 - val_mse: 0.0081\n",
            "Epoch 7/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0051 - mse: 0.0051 - val_loss: 0.0078 - val_mse: 0.0078\n",
            "Epoch 8/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0050 - mse: 0.0050 - val_loss: 0.0074 - val_mse: 0.0074\n",
            "Epoch 9/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0047 - mse: 0.0047 - val_loss: 0.0072 - val_mse: 0.0072\n",
            "Epoch 10/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0046 - mse: 0.0046 - val_loss: 0.0069 - val_mse: 0.0069\n",
            "Epoch 11/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0046 - mse: 0.0046 - val_loss: 0.0070 - val_mse: 0.0070\n",
            "Epoch 12/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0043 - mse: 0.0043 - val_loss: 0.0068 - val_mse: 0.0068\n",
            "Epoch 13/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0069 - val_mse: 0.0069\n",
            "Epoch 14/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0066 - val_mse: 0.0066\n",
            "Epoch 15/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0043 - mse: 0.0043 - val_loss: 0.0078 - val_mse: 0.0078\n",
            "Epoch 16/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0042 - mse: 0.0042 - val_loss: 0.0067 - val_mse: 0.0067\n",
            "Epoch 17/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0043 - mse: 0.0043 - val_loss: 0.0065 - val_mse: 0.0065\n",
            "Epoch 18/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0041 - mse: 0.0041 - val_loss: 0.0065 - val_mse: 0.0065\n",
            "Epoch 19/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0042 - mse: 0.0042 - val_loss: 0.0065 - val_mse: 0.0065\n",
            "Epoch 20/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0042 - mse: 0.0042 - val_loss: 0.0067 - val_mse: 0.0067\n",
            "Epoch 21/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0041 - mse: 0.0041 - val_loss: 0.0065 - val_mse: 0.0065\n",
            "Epoch 22/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0042 - mse: 0.0042 - val_loss: 0.0064 - val_mse: 0.0064\n",
            "Epoch 23/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0043 - mse: 0.0043 - val_loss: 0.0067 - val_mse: 0.0067\n",
            "Epoch 24/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0041 - mse: 0.0041 - val_loss: 0.0063 - val_mse: 0.0063\n",
            "Epoch 25/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0041 - mse: 0.0041 - val_loss: 0.0064 - val_mse: 0.0064\n",
            "Epoch 26/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0041 - mse: 0.0041 - val_loss: 0.0064 - val_mse: 0.0064\n",
            "Epoch 27/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0042 - mse: 0.0042 - val_loss: 0.0065 - val_mse: 0.0065\n",
            "Epoch 28/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0041 - mse: 0.0041 - val_loss: 0.0064 - val_mse: 0.0064\n",
            "Epoch 29/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0042 - mse: 0.0042 - val_loss: 0.0063 - val_mse: 0.0063\n",
            "Epoch 30/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0040 - mse: 0.0040 - val_loss: 0.0063 - val_mse: 0.0063\n",
            "Epoch 31/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0043 - mse: 0.0043 - val_loss: 0.0066 - val_mse: 0.0066\n",
            "Epoch 32/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0041 - mse: 0.0041 - val_loss: 0.0067 - val_mse: 0.0067\n",
            "Epoch 33/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0041 - mse: 0.0041 - val_loss: 0.0064 - val_mse: 0.0064\n",
            "Epoch 34/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0040 - mse: 0.0040 - val_loss: 0.0064 - val_mse: 0.0064\n",
            "Epoch 35/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0041 - mse: 0.0041 - val_loss: 0.0067 - val_mse: 0.0067\n",
            "Epoch 36/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0040 - mse: 0.0040 - val_loss: 0.0064 - val_mse: 0.0064\n",
            "Epoch 37/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0041 - mse: 0.0041 - val_loss: 0.0064 - val_mse: 0.0064\n",
            "Epoch 38/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0040 - mse: 0.0040 - val_loss: 0.0064 - val_mse: 0.0064\n",
            "Epoch 39/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0040 - mse: 0.0040 - val_loss: 0.0064 - val_mse: 0.0064\n",
            "Epoch 40/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0039 - mse: 0.0039 - val_loss: 0.0063 - val_mse: 0.0063\n",
            "Epoch 41/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0041 - mse: 0.0041 - val_loss: 0.0064 - val_mse: 0.0064\n",
            "Epoch 42/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0040 - mse: 0.0040 - val_loss: 0.0064 - val_mse: 0.0064\n",
            "Epoch 43/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0041 - mse: 0.0041 - val_loss: 0.0063 - val_mse: 0.0063\n",
            "Epoch 44/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0041 - mse: 0.0041 - val_loss: 0.0064 - val_mse: 0.0064\n",
            "Epoch 45/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0040 - mse: 0.0040 - val_loss: 0.0063 - val_mse: 0.0063\n",
            "Epoch 46/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0039 - mse: 0.0039 - val_loss: 0.0063 - val_mse: 0.0063\n",
            "Epoch 47/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0040 - mse: 0.0040 - val_loss: 0.0063 - val_mse: 0.0063\n",
            "Epoch 48/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0040 - mse: 0.0040 - val_loss: 0.0063 - val_mse: 0.0063\n",
            "Epoch 49/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0042 - mse: 0.0042 - val_loss: 0.0063 - val_mse: 0.0063\n",
            "Epoch 50/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0040 - mse: 0.0040 - val_loss: 0.0064 - val_mse: 0.0064\n",
            "Infering 252 values\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Epoch 1/50\n",
            "200/200 [==============================] - 7s 22ms/step - loss: 0.1783 - mse: 0.1783 - val_loss: 2.5277 - val_mse: 2.5277\n",
            "Epoch 2/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0083 - mse: 0.0083 - val_loss: 1.4492 - val_mse: 1.4492\n",
            "Epoch 3/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0061 - mse: 0.0061 - val_loss: 1.0539 - val_mse: 1.0539\n",
            "Epoch 4/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0055 - mse: 0.0055 - val_loss: 0.8637 - val_mse: 0.8637\n",
            "Epoch 5/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0051 - mse: 0.0051 - val_loss: 0.6654 - val_mse: 0.6654\n",
            "Epoch 6/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0046 - mse: 0.0046 - val_loss: 0.5366 - val_mse: 0.5366\n",
            "Epoch 7/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.4701 - val_mse: 0.4701\n",
            "Epoch 8/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0041 - mse: 0.0041 - val_loss: 0.4333 - val_mse: 0.4333\n",
            "Epoch 9/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0041 - mse: 0.0041 - val_loss: 0.4091 - val_mse: 0.4091\n",
            "Epoch 10/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0039 - mse: 0.0039 - val_loss: 0.3402 - val_mse: 0.3402\n",
            "Epoch 11/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0038 - mse: 0.0038 - val_loss: 0.2948 - val_mse: 0.2948\n",
            "Epoch 12/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.2743 - val_mse: 0.2743\n",
            "Epoch 13/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0036 - mse: 0.0036 - val_loss: 0.2749 - val_mse: 0.2749\n",
            "Epoch 14/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0036 - mse: 0.0036 - val_loss: 0.2863 - val_mse: 0.2863\n",
            "Epoch 15/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.2362 - val_mse: 0.2362\n",
            "Epoch 16/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 17/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0033 - mse: 0.0033 - val_loss: 0.2564 - val_mse: 0.2564\n",
            "Epoch 18/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0032 - mse: 0.0032 - val_loss: 0.2284 - val_mse: 0.2284\n",
            "Epoch 19/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0033 - mse: 0.0033 - val_loss: 0.2195 - val_mse: 0.2195\n",
            "Epoch 20/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0031 - mse: 0.0031 - val_loss: 0.2100 - val_mse: 0.2100\n",
            "Epoch 21/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0032 - mse: 0.0032 - val_loss: 0.2196 - val_mse: 0.2196\n",
            "Epoch 22/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0033 - mse: 0.0033 - val_loss: 0.1933 - val_mse: 0.1933\n",
            "Epoch 23/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0032 - mse: 0.0032 - val_loss: 0.2278 - val_mse: 0.2278\n",
            "Epoch 24/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0032 - mse: 0.0032 - val_loss: 0.2081 - val_mse: 0.2081\n",
            "Epoch 25/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 0.1996 - val_mse: 0.1996\n",
            "Epoch 26/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0031 - mse: 0.0031 - val_loss: 0.2074 - val_mse: 0.2074\n",
            "Epoch 27/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0031 - mse: 0.0031 - val_loss: 0.1823 - val_mse: 0.1823\n",
            "Epoch 28/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0031 - mse: 0.0031 - val_loss: 0.1794 - val_mse: 0.1794\n",
            "Epoch 29/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0032 - mse: 0.0032 - val_loss: 0.2072 - val_mse: 0.2072\n",
            "Epoch 30/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0031 - mse: 0.0031 - val_loss: 0.1476 - val_mse: 0.1476\n",
            "Epoch 31/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0032 - mse: 0.0032 - val_loss: 0.1549 - val_mse: 0.1549\n",
            "Epoch 32/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0031 - mse: 0.0031 - val_loss: 0.1668 - val_mse: 0.1668\n",
            "Epoch 33/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 0.1493 - val_mse: 0.1493\n",
            "Epoch 34/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0031 - mse: 0.0031 - val_loss: 0.1483 - val_mse: 0.1483\n",
            "Epoch 35/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0031 - mse: 0.0031 - val_loss: 0.1708 - val_mse: 0.1708\n",
            "Epoch 36/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0032 - mse: 0.0032 - val_loss: 0.1725 - val_mse: 0.1725\n",
            "Epoch 37/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 0.1767 - val_mse: 0.1767\n",
            "Epoch 38/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 0.1629 - val_mse: 0.1629\n",
            "Epoch 39/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 0.1596 - val_mse: 0.1596\n",
            "Epoch 40/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 0.1835 - val_mse: 0.1835\n",
            "Epoch 41/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 0.1898 - val_mse: 0.1898\n",
            "Epoch 42/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0029 - mse: 0.0029 - val_loss: 0.2011 - val_mse: 0.2011\n",
            "Epoch 43/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 0.1455 - val_mse: 0.1455\n",
            "Epoch 44/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 0.1956 - val_mse: 0.1956\n",
            "Epoch 45/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 0.1687 - val_mse: 0.1687\n",
            "Epoch 46/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 0.2390 - val_mse: 0.2390\n",
            "Epoch 47/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 0.2143 - val_mse: 0.2143\n",
            "Epoch 48/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 0.1834 - val_mse: 0.1834\n",
            "Epoch 49/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0031 - mse: 0.0031 - val_loss: 0.2382 - val_mse: 0.2382\n",
            "Epoch 50/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0029 - mse: 0.0029 - val_loss: 0.2197 - val_mse: 0.2197\n",
            "Infering 252 values\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Epoch 1/50\n",
            "200/200 [==============================] - 8s 23ms/step - loss: 0.1978 - mse: 0.1978 - val_loss: 0.8147 - val_mse: 0.8147\n",
            "Epoch 2/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0237 - mse: 0.0237 - val_loss: 0.4913 - val_mse: 0.4913\n",
            "Epoch 3/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0181 - mse: 0.0181 - val_loss: 0.4225 - val_mse: 0.4225\n",
            "Epoch 4/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0152 - mse: 0.0152 - val_loss: 0.3584 - val_mse: 0.3584\n",
            "Epoch 5/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0140 - mse: 0.0140 - val_loss: 0.3512 - val_mse: 0.3512\n",
            "Epoch 6/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0134 - mse: 0.0134 - val_loss: 0.3203 - val_mse: 0.3203\n",
            "Epoch 7/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0128 - mse: 0.0128 - val_loss: 0.2862 - val_mse: 0.2862\n",
            "Epoch 8/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0126 - mse: 0.0126 - val_loss: 0.2773 - val_mse: 0.2773\n",
            "Epoch 9/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0124 - mse: 0.0124 - val_loss: 0.2616 - val_mse: 0.2616\n",
            "Epoch 10/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0122 - mse: 0.0122 - val_loss: 0.3241 - val_mse: 0.3241\n",
            "Epoch 11/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.2618 - val_mse: 0.2618\n",
            "Epoch 12/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0121 - mse: 0.0121 - val_loss: 0.2769 - val_mse: 0.2769\n",
            "Epoch 13/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.3125 - val_mse: 0.3125\n",
            "Epoch 14/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0119 - mse: 0.0119 - val_loss: 0.2764 - val_mse: 0.2764\n",
            "Epoch 15/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.2424 - val_mse: 0.2424\n",
            "Epoch 16/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0120 - mse: 0.0120 - val_loss: 0.2392 - val_mse: 0.2392\n",
            "Epoch 17/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0125 - mse: 0.0125 - val_loss: 0.2310 - val_mse: 0.2310\n",
            "Epoch 18/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0118 - mse: 0.0118 - val_loss: 0.2108 - val_mse: 0.2108\n",
            "Epoch 19/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0119 - mse: 0.0119 - val_loss: 0.2546 - val_mse: 0.2546\n",
            "Epoch 20/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0118 - mse: 0.0118 - val_loss: 0.2254 - val_mse: 0.2254\n",
            "Epoch 21/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0119 - mse: 0.0119 - val_loss: 0.2382 - val_mse: 0.2382\n",
            "Epoch 22/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0120 - mse: 0.0120 - val_loss: 0.2313 - val_mse: 0.2313\n",
            "Epoch 23/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0120 - mse: 0.0120 - val_loss: 0.2365 - val_mse: 0.2365\n",
            "Epoch 24/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0119 - mse: 0.0119 - val_loss: 0.2694 - val_mse: 0.2694\n",
            "Epoch 25/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0119 - mse: 0.0119 - val_loss: 0.2045 - val_mse: 0.2045\n",
            "Epoch 26/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0118 - mse: 0.0118 - val_loss: 0.2241 - val_mse: 0.2241\n",
            "Epoch 27/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0119 - mse: 0.0119 - val_loss: 0.2256 - val_mse: 0.2256\n",
            "Epoch 28/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0122 - mse: 0.0122 - val_loss: 0.2077 - val_mse: 0.2077\n",
            "Epoch 29/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0119 - mse: 0.0119 - val_loss: 0.2705 - val_mse: 0.2705\n",
            "Epoch 30/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0116 - mse: 0.0116 - val_loss: 0.2531 - val_mse: 0.2531\n",
            "Epoch 31/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0121 - mse: 0.0121 - val_loss: 0.2595 - val_mse: 0.2595\n",
            "Epoch 32/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0121 - mse: 0.0121 - val_loss: 0.2480 - val_mse: 0.2480\n",
            "Epoch 33/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0116 - mse: 0.0116 - val_loss: 0.2897 - val_mse: 0.2897\n",
            "Epoch 34/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0118 - mse: 0.0118 - val_loss: 0.2777 - val_mse: 0.2777\n",
            "Epoch 35/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0119 - mse: 0.0119 - val_loss: 0.2638 - val_mse: 0.2638\n",
            "Epoch 36/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0115 - mse: 0.0115 - val_loss: 0.3172 - val_mse: 0.3172\n",
            "Epoch 37/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0117 - mse: 0.0117 - val_loss: 0.2845 - val_mse: 0.2845\n",
            "Epoch 38/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0118 - mse: 0.0118 - val_loss: 0.3059 - val_mse: 0.3059\n",
            "Epoch 39/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0117 - mse: 0.0117 - val_loss: 0.3141 - val_mse: 0.3141\n",
            "Epoch 40/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0114 - mse: 0.0114 - val_loss: 0.3397 - val_mse: 0.3397\n",
            "Epoch 41/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0118 - mse: 0.0118 - val_loss: 0.3571 - val_mse: 0.3571\n",
            "Epoch 42/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0115 - mse: 0.0115 - val_loss: 0.2825 - val_mse: 0.2825\n",
            "Epoch 43/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0119 - mse: 0.0119 - val_loss: 0.2867 - val_mse: 0.2867\n",
            "Epoch 44/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0118 - mse: 0.0118 - val_loss: 0.3811 - val_mse: 0.3811\n",
            "Epoch 45/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0118 - mse: 0.0118 - val_loss: 0.3472 - val_mse: 0.3472\n",
            "Epoch 46/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0113 - mse: 0.0113 - val_loss: 0.3720 - val_mse: 0.3720\n",
            "Epoch 47/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0114 - mse: 0.0114 - val_loss: 0.3922 - val_mse: 0.3922\n",
            "Epoch 48/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0116 - mse: 0.0116 - val_loss: 0.3581 - val_mse: 0.3581\n",
            "Epoch 49/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0118 - mse: 0.0118 - val_loss: 0.3612 - val_mse: 0.3612\n",
            "Epoch 50/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0113 - mse: 0.0113 - val_loss: 0.4097 - val_mse: 0.4097\n",
            "Infering 252 values\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Epoch 1/50\n",
            "200/200 [==============================] - 7s 22ms/step - loss: 0.1906 - mse: 0.1906 - val_loss: 4.1857 - val_mse: 4.1857\n",
            "Epoch 2/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0127 - mse: 0.0127 - val_loss: 2.4455 - val_mse: 2.4455\n",
            "Epoch 3/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0091 - mse: 0.0091 - val_loss: 2.0686 - val_mse: 2.0686\n",
            "Epoch 4/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0084 - mse: 0.0084 - val_loss: 1.8156 - val_mse: 1.8156\n",
            "Epoch 5/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0073 - mse: 0.0073 - val_loss: 1.5452 - val_mse: 1.5452\n",
            "Epoch 6/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0067 - mse: 0.0067 - val_loss: 1.3439 - val_mse: 1.3439\n",
            "Epoch 7/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0062 - mse: 0.0062 - val_loss: 1.2001 - val_mse: 1.2001\n",
            "Epoch 8/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0060 - mse: 0.0060 - val_loss: 1.1600 - val_mse: 1.1600\n",
            "Epoch 9/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0057 - mse: 0.0057 - val_loss: 1.0068 - val_mse: 1.0068\n",
            "Epoch 10/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0054 - mse: 0.0054 - val_loss: 0.9527 - val_mse: 0.9527\n",
            "Epoch 11/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0058 - mse: 0.0058 - val_loss: 0.9050 - val_mse: 0.9050\n",
            "Epoch 12/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0055 - mse: 0.0055 - val_loss: 0.8031 - val_mse: 0.8031\n",
            "Epoch 13/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0054 - mse: 0.0054 - val_loss: 0.7943 - val_mse: 0.7943\n",
            "Epoch 14/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0053 - mse: 0.0053 - val_loss: 0.7709 - val_mse: 0.7709\n",
            "Epoch 15/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0053 - mse: 0.0053 - val_loss: 0.7698 - val_mse: 0.7698\n",
            "Epoch 16/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0055 - mse: 0.0055 - val_loss: 0.7738 - val_mse: 0.7738\n",
            "Epoch 17/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0051 - mse: 0.0051 - val_loss: 0.7626 - val_mse: 0.7626\n",
            "Epoch 18/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0051 - mse: 0.0051 - val_loss: 0.7498 - val_mse: 0.7498\n",
            "Epoch 19/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0050 - mse: 0.0050 - val_loss: 0.7332 - val_mse: 0.7332\n",
            "Epoch 20/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0049 - mse: 0.0049 - val_loss: 0.7146 - val_mse: 0.7146\n",
            "Epoch 21/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0053 - mse: 0.0053 - val_loss: 0.6573 - val_mse: 0.6573\n",
            "Epoch 22/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0052 - mse: 0.0052 - val_loss: 0.6699 - val_mse: 0.6699\n",
            "Epoch 23/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0051 - mse: 0.0051 - val_loss: 0.6911 - val_mse: 0.6911\n",
            "Epoch 24/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0050 - mse: 0.0050 - val_loss: 0.7039 - val_mse: 0.7039\n",
            "Epoch 25/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0048 - mse: 0.0048 - val_loss: 0.6629 - val_mse: 0.6629\n",
            "Epoch 26/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0052 - mse: 0.0052 - val_loss: 0.7763 - val_mse: 0.7763\n",
            "Epoch 27/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0050 - mse: 0.0050 - val_loss: 0.7954 - val_mse: 0.7954\n",
            "Epoch 28/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0048 - mse: 0.0048 - val_loss: 0.7473 - val_mse: 0.7473\n",
            "Epoch 29/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0052 - mse: 0.0052 - val_loss: 0.7594 - val_mse: 0.7594\n",
            "Epoch 30/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0050 - mse: 0.0050 - val_loss: 0.7546 - val_mse: 0.7546\n",
            "Epoch 31/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0050 - mse: 0.0050 - val_loss: 0.8086 - val_mse: 0.8086\n",
            "Epoch 32/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0047 - mse: 0.0047 - val_loss: 0.8371 - val_mse: 0.8371\n",
            "Epoch 33/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0051 - mse: 0.0051 - val_loss: 0.8224 - val_mse: 0.8224\n",
            "Epoch 34/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0050 - mse: 0.0050 - val_loss: 0.8705 - val_mse: 0.8705\n",
            "Epoch 35/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0050 - mse: 0.0050 - val_loss: 0.9075 - val_mse: 0.9075\n",
            "Epoch 36/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0050 - mse: 0.0050 - val_loss: 0.9860 - val_mse: 0.9860\n",
            "Epoch 37/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0047 - mse: 0.0047 - val_loss: 0.8900 - val_mse: 0.8900\n",
            "Epoch 38/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0051 - mse: 0.0051 - val_loss: 0.9890 - val_mse: 0.9890\n",
            "Epoch 39/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0048 - mse: 0.0048 - val_loss: 1.0257 - val_mse: 1.0257\n",
            "Epoch 40/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0050 - mse: 0.0050 - val_loss: 0.9806 - val_mse: 0.9806\n",
            "Epoch 41/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0050 - mse: 0.0050 - val_loss: 0.9655 - val_mse: 0.9655\n",
            "Epoch 42/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0047 - mse: 0.0047 - val_loss: 1.0895 - val_mse: 1.0895\n",
            "Epoch 43/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0051 - mse: 0.0051 - val_loss: 1.0843 - val_mse: 1.0843\n",
            "Epoch 44/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0049 - mse: 0.0049 - val_loss: 1.0435 - val_mse: 1.0435\n",
            "Epoch 45/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0048 - mse: 0.0048 - val_loss: 1.1560 - val_mse: 1.1560\n",
            "Epoch 46/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0048 - mse: 0.0048 - val_loss: 1.1721 - val_mse: 1.1721\n",
            "Epoch 47/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0051 - mse: 0.0051 - val_loss: 1.2010 - val_mse: 1.2010\n",
            "Epoch 48/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0049 - mse: 0.0049 - val_loss: 1.3310 - val_mse: 1.3310\n",
            "Epoch 49/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0049 - mse: 0.0049 - val_loss: 1.3799 - val_mse: 1.3799\n",
            "Epoch 50/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0047 - mse: 0.0047 - val_loss: 1.2107 - val_mse: 1.2106\n",
            "Infering 252 values\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Epoch 1/50\n",
            "200/200 [==============================] - 7s 23ms/step - loss: 0.1604 - mse: 0.1604 - val_loss: 0.6717 - val_mse: 0.6717\n",
            "Epoch 2/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0052 - mse: 0.0052 - val_loss: 0.3659 - val_mse: 0.3659\n",
            "Epoch 3/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0040 - mse: 0.0040 - val_loss: 0.2031 - val_mse: 0.2031\n",
            "Epoch 4/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.1346 - val_mse: 0.1346\n",
            "Epoch 5/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0031 - mse: 0.0031 - val_loss: 0.0760 - val_mse: 0.0760\n",
            "Epoch 6/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0028 - mse: 0.0028 - val_loss: 0.0703 - val_mse: 0.0703\n",
            "Epoch 7/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0027 - mse: 0.0027 - val_loss: 0.0391 - val_mse: 0.0391\n",
            "Epoch 8/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 0.0510 - val_mse: 0.0510\n",
            "Epoch 9/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 0.0622 - val_mse: 0.0622\n",
            "Epoch 10/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.0419 - val_mse: 0.0419\n",
            "Epoch 11/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.0359 - val_mse: 0.0359\n",
            "Epoch 12/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0374 - val_mse: 0.0374\n",
            "Epoch 13/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0312 - val_mse: 0.0312\n",
            "Epoch 14/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0346 - val_mse: 0.0346\n",
            "Epoch 15/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0230 - val_mse: 0.0230\n",
            "Epoch 16/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0266 - val_mse: 0.0266\n",
            "Epoch 17/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0262 - val_mse: 0.0262\n",
            "Epoch 18/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0319 - val_mse: 0.0319\n",
            "Epoch 19/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0185 - val_mse: 0.0185\n",
            "Epoch 20/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0191 - val_mse: 0.0191\n",
            "Epoch 21/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0241 - val_mse: 0.0241\n",
            "Epoch 22/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0291 - val_mse: 0.0291\n",
            "Epoch 23/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0182 - val_mse: 0.0182\n",
            "Epoch 24/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0201 - val_mse: 0.0201\n",
            "Epoch 25/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0192 - val_mse: 0.0192\n",
            "Epoch 26/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0174 - val_mse: 0.0174\n",
            "Epoch 27/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0172 - val_mse: 0.0172\n",
            "Epoch 28/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0098 - val_mse: 0.0098\n",
            "Epoch 29/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0250 - val_mse: 0.0250\n",
            "Epoch 30/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0206 - val_mse: 0.0206\n",
            "Epoch 31/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0182 - val_mse: 0.0182\n",
            "Epoch 32/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0133 - val_mse: 0.0133\n",
            "Epoch 33/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0165 - val_mse: 0.0165\n",
            "Epoch 34/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0229 - val_mse: 0.0229\n",
            "Epoch 35/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0201 - val_mse: 0.0201\n",
            "Epoch 36/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0158 - val_mse: 0.0158\n",
            "Epoch 37/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0252 - val_mse: 0.0252\n",
            "Epoch 38/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0235 - val_mse: 0.0235\n",
            "Epoch 39/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0171 - val_mse: 0.0171\n",
            "Epoch 40/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0210 - val_mse: 0.0210\n",
            "Epoch 41/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0201 - val_mse: 0.0201\n",
            "Epoch 42/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0279 - val_mse: 0.0279\n",
            "Epoch 43/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0227 - val_mse: 0.0227\n",
            "Epoch 44/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0138 - val_mse: 0.0138\n",
            "Epoch 45/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0217 - val_mse: 0.0217\n",
            "Epoch 46/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0240 - val_mse: 0.0240\n",
            "Epoch 47/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0190 - val_mse: 0.0190\n",
            "Epoch 48/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0204 - val_mse: 0.0204\n",
            "Epoch 49/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0186 - val_mse: 0.0186\n",
            "Epoch 50/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0146 - val_mse: 0.0146\n",
            "Infering 252 values\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Epoch 1/50\n",
            "200/200 [==============================] - 7s 23ms/step - loss: 0.1609 - mse: 0.1609 - val_loss: 0.0064 - val_mse: 0.0064\n",
            "Epoch 2/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0041 - mse: 0.0041 - val_loss: 0.0047 - val_mse: 0.0047\n",
            "Epoch 3/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0031 - mse: 0.0031 - val_loss: 0.0040 - val_mse: 0.0040\n",
            "Epoch 4/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 0.0037 - val_mse: 0.0037\n",
            "Epoch 5/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 0.0033 - val_mse: 0.0033\n",
            "Epoch 6/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0032 - val_mse: 0.0032\n",
            "Epoch 7/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0037 - val_mse: 0.0037\n",
            "Epoch 8/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0029 - val_mse: 0.0029\n",
            "Epoch 9/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0029 - val_mse: 0.0029\n",
            "Epoch 10/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0025 - val_mse: 0.0025\n",
            "Epoch 11/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0024 - val_mse: 0.0024\n",
            "Epoch 12/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0027 - val_mse: 0.0027\n",
            "Epoch 13/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0023 - val_mse: 0.0023\n",
            "Epoch 14/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0023 - val_mse: 0.0023\n",
            "Epoch 15/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0029 - val_mse: 0.0029\n",
            "Epoch 16/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0021 - val_mse: 0.0021\n",
            "Epoch 17/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0023 - val_mse: 0.0023\n",
            "Epoch 18/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0020 - val_mse: 0.0020\n",
            "Epoch 19/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0020 - val_mse: 0.0020\n",
            "Epoch 20/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0020 - val_mse: 0.0020\n",
            "Epoch 21/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0021 - val_mse: 0.0021\n",
            "Epoch 22/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0023 - val_mse: 0.0023\n",
            "Epoch 23/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0019 - val_mse: 0.0019\n",
            "Epoch 24/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0019 - val_mse: 0.0019\n",
            "Epoch 25/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0027 - val_mse: 0.0027\n",
            "Epoch 26/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0019 - val_mse: 0.0019\n",
            "Epoch 27/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0021 - val_mse: 0.0021\n",
            "Epoch 28/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0028 - val_mse: 0.0028\n",
            "Epoch 29/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0018 - val_mse: 0.0018\n",
            "Epoch 30/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0024 - val_mse: 0.0024\n",
            "Epoch 31/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0019 - val_mse: 0.0019\n",
            "Epoch 32/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0021 - val_mse: 0.0021\n",
            "Epoch 33/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0018 - val_mse: 0.0018\n",
            "Epoch 34/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0020 - val_mse: 0.0020\n",
            "Epoch 35/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0018 - val_mse: 0.0018\n",
            "Epoch 36/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0019 - val_mse: 0.0019\n",
            "Epoch 37/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0018 - val_mse: 0.0018\n",
            "Epoch 38/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0019 - val_mse: 0.0019\n",
            "Epoch 39/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0020 - val_mse: 0.0020\n",
            "Epoch 40/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0018 - val_mse: 0.0018\n",
            "Epoch 41/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0019 - val_mse: 0.0019\n",
            "Epoch 42/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0018 - val_mse: 0.0018\n",
            "Epoch 43/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0018 - val_mse: 0.0018\n",
            "Epoch 44/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0020 - val_mse: 0.0020\n",
            "Epoch 45/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0018 - val_mse: 0.0018\n",
            "Epoch 46/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0022 - val_mse: 0.0022\n",
            "Epoch 47/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0020 - val_mse: 0.0020\n",
            "Epoch 48/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0019 - val_mse: 0.0019\n",
            "Epoch 49/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0024 - val_mse: 0.0024\n",
            "Epoch 50/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 0.0019 - val_mse: 0.0019\n",
            "Infering 252 values\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Epoch 1/50\n",
            "200/200 [==============================] - 8s 24ms/step - loss: 0.1694 - mse: 0.1694 - val_loss: 0.5773 - val_mse: 0.5773\n",
            "Epoch 2/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.2866 - val_mse: 0.2866\n",
            "Epoch 3/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0033 - mse: 0.0033 - val_loss: 0.1935 - val_mse: 0.1935\n",
            "Epoch 4/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0027 - mse: 0.0027 - val_loss: 0.1280 - val_mse: 0.1280\n",
            "Epoch 5/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0026 - mse: 0.0026 - val_loss: 0.0846 - val_mse: 0.0846\n",
            "Epoch 6/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 0.0660 - val_mse: 0.0660\n",
            "Epoch 7/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 0.0710 - val_mse: 0.0710\n",
            "Epoch 8/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0569 - val_mse: 0.0569\n",
            "Epoch 9/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0723 - val_mse: 0.0723\n",
            "Epoch 10/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0483 - val_mse: 0.0483\n",
            "Epoch 11/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0404 - val_mse: 0.0404\n",
            "Epoch 12/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0375 - val_mse: 0.0375\n",
            "Epoch 13/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0338 - val_mse: 0.0338\n",
            "Epoch 14/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0399 - val_mse: 0.0399\n",
            "Epoch 15/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0209 - val_mse: 0.0209\n",
            "Epoch 16/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0377 - val_mse: 0.0377\n",
            "Epoch 17/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0256 - val_mse: 0.0256\n",
            "Epoch 18/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0300 - val_mse: 0.0300\n",
            "Epoch 19/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0316 - val_mse: 0.0316\n",
            "Epoch 20/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0283 - val_mse: 0.0283\n",
            "Epoch 21/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0221 - val_mse: 0.0221\n",
            "Epoch 22/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0294 - val_mse: 0.0294\n",
            "Epoch 23/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0320 - val_mse: 0.0320\n",
            "Epoch 24/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0268 - val_mse: 0.0268\n",
            "Epoch 25/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0198 - val_mse: 0.0198\n",
            "Epoch 26/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0177 - val_mse: 0.0177\n",
            "Epoch 27/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0179 - val_mse: 0.0179\n",
            "Epoch 28/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0173 - val_mse: 0.0173\n",
            "Epoch 29/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0196 - val_mse: 0.0196\n",
            "Epoch 30/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0130 - val_mse: 0.0130\n",
            "Epoch 31/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0182 - val_mse: 0.0182\n",
            "Epoch 32/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0264 - val_mse: 0.0264\n",
            "Epoch 33/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0145 - val_mse: 0.0145\n",
            "Epoch 34/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0197 - val_mse: 0.0197\n",
            "Epoch 35/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0180 - val_mse: 0.0180\n",
            "Epoch 36/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0227 - val_mse: 0.0227\n",
            "Epoch 37/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0190 - val_mse: 0.0190\n",
            "Epoch 38/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0168 - val_mse: 0.0168\n",
            "Epoch 39/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0170 - val_mse: 0.0170\n",
            "Epoch 40/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0169 - val_mse: 0.0169\n",
            "Epoch 41/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0345 - val_mse: 0.0345\n",
            "Epoch 42/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0267 - val_mse: 0.0267\n",
            "Epoch 43/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0222 - val_mse: 0.0222\n",
            "Epoch 44/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0158 - val_mse: 0.0158\n",
            "Epoch 45/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0199 - val_mse: 0.0199\n",
            "Epoch 46/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0260 - val_mse: 0.0260\n",
            "Epoch 47/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0312 - val_mse: 0.0312\n",
            "Epoch 48/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0212 - val_mse: 0.0212\n",
            "Epoch 49/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0252 - val_mse: 0.0252\n",
            "Epoch 50/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0215 - val_mse: 0.0215\n",
            "Infering 252 values\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Epoch 1/50\n",
            "200/200 [==============================] - 7s 23ms/step - loss: 0.1679 - mse: 0.1679 - val_loss: 0.1890 - val_mse: 0.1890\n",
            "Epoch 2/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0145 - mse: 0.0145 - val_loss: 0.0689 - val_mse: 0.0689\n",
            "Epoch 3/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0360 - val_mse: 0.0360\n",
            "Epoch 4/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0082 - mse: 0.0082 - val_loss: 0.0279 - val_mse: 0.0279\n",
            "Epoch 5/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0077 - mse: 0.0077 - val_loss: 0.0117 - val_mse: 0.0117\n",
            "Epoch 6/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0073 - mse: 0.0073 - val_loss: 0.0181 - val_mse: 0.0181\n",
            "Epoch 7/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0071 - mse: 0.0071 - val_loss: 0.0120 - val_mse: 0.0120\n",
            "Epoch 8/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0065 - mse: 0.0065 - val_loss: 0.0113 - val_mse: 0.0113\n",
            "Epoch 9/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0067 - mse: 0.0067 - val_loss: 0.0136 - val_mse: 0.0136\n",
            "Epoch 10/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0060 - mse: 0.0060 - val_loss: 0.0112 - val_mse: 0.0112\n",
            "Epoch 11/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0063 - mse: 0.0063 - val_loss: 0.0120 - val_mse: 0.0120\n",
            "Epoch 12/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0061 - mse: 0.0061 - val_loss: 0.0097 - val_mse: 0.0097\n",
            "Epoch 13/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0062 - mse: 0.0062 - val_loss: 0.0130 - val_mse: 0.0130\n",
            "Epoch 14/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0062 - mse: 0.0062 - val_loss: 0.0124 - val_mse: 0.0124\n",
            "Epoch 15/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0060 - mse: 0.0060 - val_loss: 0.0073 - val_mse: 0.0073\n",
            "Epoch 16/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0061 - mse: 0.0061 - val_loss: 0.0133 - val_mse: 0.0133\n",
            "Epoch 17/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0061 - mse: 0.0061 - val_loss: 0.0087 - val_mse: 0.0087\n",
            "Epoch 18/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0061 - mse: 0.0061 - val_loss: 0.0109 - val_mse: 0.0109\n",
            "Epoch 19/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0059 - mse: 0.0059 - val_loss: 0.0125 - val_mse: 0.0125\n",
            "Epoch 20/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0058 - mse: 0.0058 - val_loss: 0.0131 - val_mse: 0.0131\n",
            "Epoch 21/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0059 - mse: 0.0059 - val_loss: 0.0109 - val_mse: 0.0109\n",
            "Epoch 22/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0060 - mse: 0.0060 - val_loss: 0.0146 - val_mse: 0.0146\n",
            "Epoch 23/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0059 - mse: 0.0059 - val_loss: 0.0114 - val_mse: 0.0114\n",
            "Epoch 24/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0060 - mse: 0.0060 - val_loss: 0.0090 - val_mse: 0.0090\n",
            "Epoch 25/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0058 - mse: 0.0058 - val_loss: 0.0094 - val_mse: 0.0094\n",
            "Epoch 26/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0060 - mse: 0.0060 - val_loss: 0.0111 - val_mse: 0.0111\n",
            "Epoch 27/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0058 - mse: 0.0058 - val_loss: 0.0097 - val_mse: 0.0097\n",
            "Epoch 28/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0057 - mse: 0.0057 - val_loss: 0.0167 - val_mse: 0.0167\n",
            "Epoch 29/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0060 - mse: 0.0060 - val_loss: 0.0234 - val_mse: 0.0234\n",
            "Epoch 30/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0059 - mse: 0.0059 - val_loss: 0.0124 - val_mse: 0.0124\n",
            "Epoch 31/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0060 - mse: 0.0060 - val_loss: 0.0279 - val_mse: 0.0279\n",
            "Epoch 32/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0056 - mse: 0.0056 - val_loss: 0.0770 - val_mse: 0.0770\n",
            "Epoch 33/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0056 - mse: 0.0056 - val_loss: 0.0323 - val_mse: 0.0323\n",
            "Epoch 34/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0059 - mse: 0.0059 - val_loss: 0.0141 - val_mse: 0.0141\n",
            "Epoch 35/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0053 - mse: 0.0053 - val_loss: 0.0305 - val_mse: 0.0305\n",
            "Epoch 36/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0057 - mse: 0.0057 - val_loss: 0.0170 - val_mse: 0.0170\n",
            "Epoch 37/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0056 - mse: 0.0056 - val_loss: 0.0458 - val_mse: 0.0458\n",
            "Epoch 38/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0056 - mse: 0.0056 - val_loss: 0.0283 - val_mse: 0.0283\n",
            "Epoch 39/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0055 - mse: 0.0055 - val_loss: 0.0675 - val_mse: 0.0675\n",
            "Epoch 40/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0055 - mse: 0.0055 - val_loss: 0.0191 - val_mse: 0.0191\n",
            "Epoch 41/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0055 - mse: 0.0055 - val_loss: 0.0798 - val_mse: 0.0798\n",
            "Epoch 42/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0052 - mse: 0.0052 - val_loss: 0.1396 - val_mse: 0.1396\n",
            "Epoch 43/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0054 - mse: 0.0054 - val_loss: 0.1223 - val_mse: 0.1223\n",
            "Epoch 44/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0054 - mse: 0.0054 - val_loss: 0.0715 - val_mse: 0.0715\n",
            "Epoch 45/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0053 - mse: 0.0053 - val_loss: 0.0127 - val_mse: 0.0127\n",
            "Epoch 46/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0053 - mse: 0.0053 - val_loss: 0.0508 - val_mse: 0.0508\n",
            "Epoch 47/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0053 - mse: 0.0053 - val_loss: 0.0346 - val_mse: 0.0346\n",
            "Epoch 48/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0053 - mse: 0.0053 - val_loss: 0.0695 - val_mse: 0.0695\n",
            "Epoch 49/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0053 - mse: 0.0053 - val_loss: 0.0884 - val_mse: 0.0884\n",
            "Epoch 50/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0050 - mse: 0.0050 - val_loss: 0.0767 - val_mse: 0.0767\n",
            "Infering 252 values\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Epoch 1/50\n",
            "200/200 [==============================] - 8s 24ms/step - loss: 0.1670 - mse: 0.1670 - val_loss: 0.0095 - val_mse: 0.0095\n",
            "Epoch 2/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0151 - mse: 0.0151 - val_loss: 0.0075 - val_mse: 0.0075\n",
            "Epoch 3/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0121 - mse: 0.0121 - val_loss: 0.0057 - val_mse: 0.0057\n",
            "Epoch 4/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0050 - val_mse: 0.0050\n",
            "Epoch 5/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0088 - mse: 0.0088 - val_loss: 0.0045 - val_mse: 0.0045\n",
            "Epoch 6/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0082 - mse: 0.0082 - val_loss: 0.0046 - val_mse: 0.0046\n",
            "Epoch 7/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0076 - mse: 0.0076 - val_loss: 0.0042 - val_mse: 0.0042\n",
            "Epoch 8/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0074 - mse: 0.0074 - val_loss: 0.0041 - val_mse: 0.0041\n",
            "Epoch 9/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0071 - mse: 0.0071 - val_loss: 0.0042 - val_mse: 0.0042\n",
            "Epoch 10/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0068 - mse: 0.0068 - val_loss: 0.0041 - val_mse: 0.0041\n",
            "Epoch 11/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0071 - mse: 0.0071 - val_loss: 0.0040 - val_mse: 0.0040\n",
            "Epoch 12/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0068 - mse: 0.0068 - val_loss: 0.0045 - val_mse: 0.0045\n",
            "Epoch 13/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0069 - mse: 0.0069 - val_loss: 0.0040 - val_mse: 0.0040\n",
            "Epoch 14/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0067 - mse: 0.0067 - val_loss: 0.0040 - val_mse: 0.0040\n",
            "Epoch 15/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0068 - mse: 0.0068 - val_loss: 0.0039 - val_mse: 0.0039\n",
            "Epoch 16/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0067 - mse: 0.0067 - val_loss: 0.0039 - val_mse: 0.0039\n",
            "Epoch 17/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0068 - mse: 0.0068 - val_loss: 0.0039 - val_mse: 0.0039\n",
            "Epoch 18/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0068 - mse: 0.0068 - val_loss: 0.0039 - val_mse: 0.0039\n",
            "Epoch 19/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0066 - mse: 0.0066 - val_loss: 0.0039 - val_mse: 0.0039\n",
            "Epoch 20/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0066 - mse: 0.0066 - val_loss: 0.0039 - val_mse: 0.0039\n",
            "Epoch 21/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0065 - mse: 0.0065 - val_loss: 0.0040 - val_mse: 0.0040\n",
            "Epoch 22/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0066 - mse: 0.0066 - val_loss: 0.0040 - val_mse: 0.0040\n",
            "Epoch 23/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0067 - mse: 0.0067 - val_loss: 0.0046 - val_mse: 0.0046\n",
            "Epoch 24/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0065 - mse: 0.0065 - val_loss: 0.0039 - val_mse: 0.0039\n",
            "Epoch 25/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0066 - mse: 0.0066 - val_loss: 0.0039 - val_mse: 0.0039\n",
            "Epoch 26/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0067 - mse: 0.0067 - val_loss: 0.0039 - val_mse: 0.0039\n",
            "Epoch 27/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0067 - mse: 0.0067 - val_loss: 0.0039 - val_mse: 0.0039\n",
            "Epoch 28/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0064 - mse: 0.0064 - val_loss: 0.0039 - val_mse: 0.0039\n",
            "Epoch 29/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0068 - mse: 0.0068 - val_loss: 0.0040 - val_mse: 0.0040\n",
            "Epoch 30/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0066 - mse: 0.0066 - val_loss: 0.0039 - val_mse: 0.0039\n",
            "Epoch 31/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0067 - mse: 0.0067 - val_loss: 0.0041 - val_mse: 0.0041\n",
            "Epoch 32/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0067 - mse: 0.0067 - val_loss: 0.0039 - val_mse: 0.0039\n",
            "Epoch 33/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0065 - mse: 0.0065 - val_loss: 0.0039 - val_mse: 0.0039\n",
            "Epoch 34/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0068 - mse: 0.0068 - val_loss: 0.0040 - val_mse: 0.0040\n",
            "Epoch 35/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0063 - mse: 0.0063 - val_loss: 0.0040 - val_mse: 0.0040\n",
            "Epoch 36/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0065 - mse: 0.0065 - val_loss: 0.0043 - val_mse: 0.0043\n",
            "Epoch 37/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0064 - mse: 0.0064 - val_loss: 0.0039 - val_mse: 0.0039\n",
            "Epoch 38/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0067 - mse: 0.0067 - val_loss: 0.0040 - val_mse: 0.0040\n",
            "Epoch 39/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0064 - mse: 0.0064 - val_loss: 0.0040 - val_mse: 0.0040\n",
            "Epoch 40/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0065 - mse: 0.0065 - val_loss: 0.0040 - val_mse: 0.0040\n",
            "Epoch 41/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0066 - mse: 0.0066 - val_loss: 0.0044 - val_mse: 0.0044\n",
            "Epoch 42/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0063 - mse: 0.0063 - val_loss: 0.0039 - val_mse: 0.0039\n",
            "Epoch 43/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0065 - mse: 0.0065 - val_loss: 0.0039 - val_mse: 0.0039\n",
            "Epoch 44/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0066 - mse: 0.0066 - val_loss: 0.0040 - val_mse: 0.0040\n",
            "Epoch 45/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0065 - mse: 0.0065 - val_loss: 0.0041 - val_mse: 0.0041\n",
            "Epoch 46/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0065 - mse: 0.0065 - val_loss: 0.0040 - val_mse: 0.0040\n",
            "Epoch 47/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0064 - mse: 0.0064 - val_loss: 0.0043 - val_mse: 0.0043\n",
            "Epoch 48/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0066 - mse: 0.0066 - val_loss: 0.0042 - val_mse: 0.0042\n",
            "Epoch 49/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0065 - mse: 0.0065 - val_loss: 0.0040 - val_mse: 0.0040\n",
            "Epoch 50/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0064 - mse: 0.0064 - val_loss: 0.0041 - val_mse: 0.0041\n",
            "Infering 252 values\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Epoch 1/50\n",
            "200/200 [==============================] - 8s 24ms/step - loss: 0.1569 - mse: 0.1569 - val_loss: 1.1261 - val_mse: 1.1261\n",
            "Epoch 2/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0038 - mse: 0.0038 - val_loss: 0.6546 - val_mse: 0.6546\n",
            "Epoch 3/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 0.4792 - val_mse: 0.4792\n",
            "Epoch 4/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0027 - mse: 0.0027 - val_loss: 0.3476 - val_mse: 0.3476\n",
            "Epoch 5/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 0.2666 - val_mse: 0.2666\n",
            "Epoch 6/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 0.2055 - val_mse: 0.2055\n",
            "Epoch 7/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 0.1583 - val_mse: 0.1583\n",
            "Epoch 8/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.1305 - val_mse: 0.1305\n",
            "Epoch 9/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.1169 - val_mse: 0.1169\n",
            "Epoch 10/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.1036 - val_mse: 0.1036\n",
            "Epoch 11/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0886 - val_mse: 0.0886\n",
            "Epoch 12/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0941 - val_mse: 0.0941\n",
            "Epoch 13/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0801 - val_mse: 0.0801\n",
            "Epoch 14/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0925 - val_mse: 0.0925\n",
            "Epoch 15/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0603 - val_mse: 0.0603\n",
            "Epoch 16/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0891 - val_mse: 0.0891\n",
            "Epoch 17/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0687 - val_mse: 0.0687\n",
            "Epoch 18/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0990 - val_mse: 0.0990\n",
            "Epoch 19/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0690 - val_mse: 0.0690\n",
            "Epoch 20/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0617 - val_mse: 0.0617\n",
            "Epoch 21/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0798 - val_mse: 0.0798\n",
            "Epoch 22/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0814 - val_mse: 0.0814\n",
            "Epoch 23/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0833 - val_mse: 0.0833\n",
            "Epoch 24/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0657 - val_mse: 0.0657\n",
            "Epoch 25/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0755 - val_mse: 0.0755\n",
            "Epoch 26/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0662 - val_mse: 0.0662\n",
            "Epoch 27/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0791 - val_mse: 0.0791\n",
            "Epoch 28/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0510 - val_mse: 0.0510\n",
            "Epoch 29/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0698 - val_mse: 0.0698\n",
            "Epoch 30/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0590 - val_mse: 0.0590\n",
            "Epoch 31/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0715 - val_mse: 0.0715\n",
            "Epoch 32/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0621 - val_mse: 0.0621\n",
            "Epoch 33/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0674 - val_mse: 0.0674\n",
            "Epoch 34/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0713 - val_mse: 0.0713\n",
            "Epoch 35/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0721 - val_mse: 0.0721\n",
            "Epoch 36/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0728 - val_mse: 0.0728\n",
            "Epoch 37/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0711 - val_mse: 0.0711\n",
            "Epoch 38/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0650 - val_mse: 0.0650\n",
            "Epoch 39/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0705 - val_mse: 0.0705\n",
            "Epoch 40/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0714 - val_mse: 0.0714\n",
            "Epoch 41/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0740 - val_mse: 0.0740\n",
            "Epoch 42/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0756 - val_mse: 0.0756\n",
            "Epoch 43/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0732 - val_mse: 0.0732\n",
            "Epoch 44/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0710 - val_mse: 0.0710\n",
            "Epoch 45/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0642 - val_mse: 0.0642\n",
            "Epoch 46/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0807 - val_mse: 0.0807\n",
            "Epoch 47/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0784 - val_mse: 0.0784\n",
            "Epoch 48/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0726 - val_mse: 0.0726\n",
            "Epoch 49/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0857 - val_mse: 0.0857\n",
            "Epoch 50/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0722 - val_mse: 0.0722\n",
            "Infering 252 values\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Epoch 1/50\n",
            "200/200 [==============================] - 7s 23ms/step - loss: 0.1573 - mse: 0.1573 - val_loss: 0.2219 - val_mse: 0.2219\n",
            "Epoch 2/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0058 - mse: 0.0058 - val_loss: 0.0914 - val_mse: 0.0914\n",
            "Epoch 3/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0523 - val_mse: 0.0523\n",
            "Epoch 4/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0037 - mse: 0.0037 - val_loss: 0.0293 - val_mse: 0.0293\n",
            "Epoch 5/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0032 - mse: 0.0032 - val_loss: 0.0204 - val_mse: 0.0204\n",
            "Epoch 6/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 0.0240 - val_mse: 0.0240\n",
            "Epoch 7/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0028 - mse: 0.0028 - val_loss: 0.0163 - val_mse: 0.0163\n",
            "Epoch 8/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 0.0187 - val_mse: 0.0187\n",
            "Epoch 9/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 0.0245 - val_mse: 0.0245\n",
            "Epoch 10/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0205 - val_mse: 0.0205\n",
            "Epoch 11/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 0.0131 - val_mse: 0.0131\n",
            "Epoch 12/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0127 - val_mse: 0.0127\n",
            "Epoch 13/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0122 - val_mse: 0.0122\n",
            "Epoch 14/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0103 - val_mse: 0.0103\n",
            "Epoch 15/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.0074 - val_mse: 0.0074\n",
            "Epoch 16/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.0158 - val_mse: 0.0158\n",
            "Epoch 17/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0071 - val_mse: 0.0071\n",
            "Epoch 18/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.0137 - val_mse: 0.0137\n",
            "Epoch 19/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0074 - val_mse: 0.0074\n",
            "Epoch 20/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0084 - val_mse: 0.0084\n",
            "Epoch 21/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0123 - val_mse: 0.0123\n",
            "Epoch 22/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0106 - val_mse: 0.0106\n",
            "Epoch 23/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0178 - val_mse: 0.0178\n",
            "Epoch 24/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0097 - val_mse: 0.0097\n",
            "Epoch 25/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0091 - val_mse: 0.0091\n",
            "Epoch 26/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0114 - val_mse: 0.0114\n",
            "Epoch 27/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0079 - val_mse: 0.0079\n",
            "Epoch 28/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0052 - val_mse: 0.0052\n",
            "Epoch 29/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0112 - val_mse: 0.0112\n",
            "Epoch 30/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0099 - val_mse: 0.0099\n",
            "Epoch 31/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0051 - val_mse: 0.0051\n",
            "Epoch 32/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0046 - val_mse: 0.0046\n",
            "Epoch 33/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0057 - val_mse: 0.0057\n",
            "Epoch 34/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0120 - val_mse: 0.0120\n",
            "Epoch 35/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0042 - val_mse: 0.0042\n",
            "Epoch 36/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0080 - val_mse: 0.0080\n",
            "Epoch 37/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0096 - val_mse: 0.0096\n",
            "Epoch 38/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0150 - val_mse: 0.0150\n",
            "Epoch 39/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0093 - val_mse: 0.0093\n",
            "Epoch 40/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0147 - val_mse: 0.0147\n",
            "Epoch 41/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0196 - val_mse: 0.0196\n",
            "Epoch 42/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0084 - val_mse: 0.0084\n",
            "Epoch 43/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0130 - val_mse: 0.0130\n",
            "Epoch 44/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0081 - val_mse: 0.0081\n",
            "Epoch 45/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0121 - val_mse: 0.0121\n",
            "Epoch 46/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0077 - val_mse: 0.0077\n",
            "Epoch 47/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0200 - val_mse: 0.0200\n",
            "Epoch 48/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0121 - val_mse: 0.0121\n",
            "Epoch 49/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0135 - val_mse: 0.0135\n",
            "Epoch 50/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0060 - val_mse: 0.0060\n",
            "Infering 252 values\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Epoch 1/50\n",
            "200/200 [==============================] - 8s 24ms/step - loss: 0.1608 - mse: 0.1608 - val_loss: 0.3794 - val_mse: 0.3794\n",
            "Epoch 2/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0081 - mse: 0.0081 - val_loss: 0.2680 - val_mse: 0.2680\n",
            "Epoch 3/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0064 - mse: 0.0064 - val_loss: 0.1958 - val_mse: 0.1958\n",
            "Epoch 4/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0054 - mse: 0.0054 - val_loss: 0.1688 - val_mse: 0.1688\n",
            "Epoch 5/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0048 - mse: 0.0048 - val_loss: 0.1327 - val_mse: 0.1327\n",
            "Epoch 6/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0043 - mse: 0.0043 - val_loss: 0.1227 - val_mse: 0.1227\n",
            "Epoch 7/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0042 - mse: 0.0042 - val_loss: 0.1127 - val_mse: 0.1127\n",
            "Epoch 8/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0037 - mse: 0.0037 - val_loss: 0.1103 - val_mse: 0.1103\n",
            "Epoch 9/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0036 - mse: 0.0036 - val_loss: 0.1036 - val_mse: 0.1036\n",
            "Epoch 10/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0933 - val_mse: 0.0933\n",
            "Epoch 11/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0033 - mse: 0.0033 - val_loss: 0.0934 - val_mse: 0.0934\n",
            "Epoch 12/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0032 - mse: 0.0032 - val_loss: 0.0875 - val_mse: 0.0875\n",
            "Epoch 13/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0031 - mse: 0.0031 - val_loss: 0.0895 - val_mse: 0.0895\n",
            "Epoch 14/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0031 - mse: 0.0031 - val_loss: 0.0871 - val_mse: 0.0871\n",
            "Epoch 15/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0031 - mse: 0.0031 - val_loss: 0.0777 - val_mse: 0.0777\n",
            "Epoch 16/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0031 - mse: 0.0031 - val_loss: 0.0832 - val_mse: 0.0832\n",
            "Epoch 17/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0031 - mse: 0.0031 - val_loss: 0.0741 - val_mse: 0.0741\n",
            "Epoch 18/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 0.0829 - val_mse: 0.0829\n",
            "Epoch 19/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 0.0758 - val_mse: 0.0758\n",
            "Epoch 20/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0028 - mse: 0.0028 - val_loss: 0.0636 - val_mse: 0.0636\n",
            "Epoch 21/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0031 - mse: 0.0031 - val_loss: 0.0666 - val_mse: 0.0666\n",
            "Epoch 22/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 0.0764 - val_mse: 0.0764\n",
            "Epoch 23/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0029 - mse: 0.0029 - val_loss: 0.0721 - val_mse: 0.0721\n",
            "Epoch 24/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 0.0711 - val_mse: 0.0711\n",
            "Epoch 25/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0029 - mse: 0.0029 - val_loss: 0.0655 - val_mse: 0.0655\n",
            "Epoch 26/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0029 - mse: 0.0029 - val_loss: 0.0707 - val_mse: 0.0707\n",
            "Epoch 27/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0029 - mse: 0.0029 - val_loss: 0.0656 - val_mse: 0.0656\n",
            "Epoch 28/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0029 - mse: 0.0029 - val_loss: 0.0594 - val_mse: 0.0594\n",
            "Epoch 29/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 0.0607 - val_mse: 0.0607\n",
            "Epoch 30/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0029 - mse: 0.0029 - val_loss: 0.0612 - val_mse: 0.0612\n",
            "Epoch 31/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0029 - mse: 0.0029 - val_loss: 0.0523 - val_mse: 0.0523\n",
            "Epoch 32/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0029 - mse: 0.0029 - val_loss: 0.0557 - val_mse: 0.0557\n",
            "Epoch 33/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0029 - mse: 0.0029 - val_loss: 0.0578 - val_mse: 0.0578\n",
            "Epoch 34/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0029 - mse: 0.0029 - val_loss: 0.0628 - val_mse: 0.0628\n",
            "Epoch 35/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0028 - mse: 0.0028 - val_loss: 0.0535 - val_mse: 0.0535\n",
            "Epoch 36/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0029 - mse: 0.0029 - val_loss: 0.0560 - val_mse: 0.0560\n",
            "Epoch 37/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0029 - mse: 0.0029 - val_loss: 0.0521 - val_mse: 0.0521\n",
            "Epoch 38/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0028 - mse: 0.0028 - val_loss: 0.0578 - val_mse: 0.0578\n",
            "Epoch 39/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0028 - mse: 0.0028 - val_loss: 0.0583 - val_mse: 0.0583\n",
            "Epoch 40/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0029 - mse: 0.0029 - val_loss: 0.0555 - val_mse: 0.0555\n",
            "Epoch 41/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0029 - mse: 0.0029 - val_loss: 0.0585 - val_mse: 0.0585\n",
            "Epoch 42/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0028 - mse: 0.0028 - val_loss: 0.0545 - val_mse: 0.0545\n",
            "Epoch 43/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0028 - mse: 0.0028 - val_loss: 0.0513 - val_mse: 0.0513\n",
            "Epoch 44/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0029 - mse: 0.0029 - val_loss: 0.0538 - val_mse: 0.0538\n",
            "Epoch 45/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0028 - mse: 0.0028 - val_loss: 0.0561 - val_mse: 0.0561\n",
            "Epoch 46/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0029 - mse: 0.0029 - val_loss: 0.0529 - val_mse: 0.0529\n",
            "Epoch 47/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0029 - mse: 0.0029 - val_loss: 0.0601 - val_mse: 0.0601\n",
            "Epoch 48/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0028 - mse: 0.0028 - val_loss: 0.0513 - val_mse: 0.0513\n",
            "Epoch 49/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0029 - mse: 0.0029 - val_loss: 0.0597 - val_mse: 0.0597\n",
            "Epoch 50/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0028 - mse: 0.0028 - val_loss: 0.0474 - val_mse: 0.0474\n",
            "Infering 252 values\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Epoch 1/50\n",
            "200/200 [==============================] - 8s 25ms/step - loss: 0.1588 - mse: 0.1588 - val_loss: 0.9242 - val_mse: 0.9242\n",
            "Epoch 2/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0047 - mse: 0.0047 - val_loss: 0.5226 - val_mse: 0.5226\n",
            "Epoch 3/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0036 - mse: 0.0036 - val_loss: 0.3183 - val_mse: 0.3183\n",
            "Epoch 4/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 0.2232 - val_mse: 0.2232\n",
            "Epoch 5/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0027 - mse: 0.0027 - val_loss: 0.1368 - val_mse: 0.1368\n",
            "Epoch 6/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 0.1144 - val_mse: 0.1144\n",
            "Epoch 7/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 0.0782 - val_mse: 0.0782\n",
            "Epoch 8/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.0923 - val_mse: 0.0923\n",
            "Epoch 9/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0900 - val_mse: 0.0900\n",
            "Epoch 10/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0843 - val_mse: 0.0843\n",
            "Epoch 11/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0687 - val_mse: 0.0687\n",
            "Epoch 12/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0699 - val_mse: 0.0699\n",
            "Epoch 13/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0608 - val_mse: 0.0608\n",
            "Epoch 14/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0714 - val_mse: 0.0714\n",
            "Epoch 15/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0443 - val_mse: 0.0443\n",
            "Epoch 16/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0549 - val_mse: 0.0549\n",
            "Epoch 17/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0455 - val_mse: 0.0455\n",
            "Epoch 18/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0642 - val_mse: 0.0642\n",
            "Epoch 19/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0455 - val_mse: 0.0455\n",
            "Epoch 20/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0367 - val_mse: 0.0367\n",
            "Epoch 21/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0442 - val_mse: 0.0442\n",
            "Epoch 22/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0537 - val_mse: 0.0537\n",
            "Epoch 23/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0432 - val_mse: 0.0432\n",
            "Epoch 24/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0537 - val_mse: 0.0537\n",
            "Epoch 25/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0406 - val_mse: 0.0406\n",
            "Epoch 26/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0491 - val_mse: 0.0491\n",
            "Epoch 27/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0361 - val_mse: 0.0361\n",
            "Epoch 28/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0254 - val_mse: 0.0254\n",
            "Epoch 29/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0475 - val_mse: 0.0475\n",
            "Epoch 30/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0353 - val_mse: 0.0353\n",
            "Epoch 31/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0401 - val_mse: 0.0401\n",
            "Epoch 32/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0291 - val_mse: 0.0291\n",
            "Epoch 33/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0340 - val_mse: 0.0340\n",
            "Epoch 34/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0371 - val_mse: 0.0371\n",
            "Epoch 35/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0342 - val_mse: 0.0342\n",
            "Epoch 36/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0365 - val_mse: 0.0365\n",
            "Epoch 37/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0307 - val_mse: 0.0307\n",
            "Epoch 38/50\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0406 - val_mse: 0.0406\n",
            "Epoch 39/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0346 - val_mse: 0.0346\n",
            "Epoch 40/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0412 - val_mse: 0.0412\n",
            "Epoch 41/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0330 - val_mse: 0.0330\n",
            "Epoch 42/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0479 - val_mse: 0.0479\n",
            "Epoch 43/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0340 - val_mse: 0.0340\n",
            "Epoch 44/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0328 - val_mse: 0.0328\n",
            "Epoch 45/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0387 - val_mse: 0.0387\n",
            "Epoch 46/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0334 - val_mse: 0.0334\n",
            "Epoch 47/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0385 - val_mse: 0.0385\n",
            "Epoch 48/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0369 - val_mse: 0.0369\n",
            "Epoch 49/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0359 - val_mse: 0.0359\n",
            "Epoch 50/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0260 - val_mse: 0.0260\n",
            "Infering 252 values\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Epoch 1/50\n",
            "200/200 [==============================] - 7s 24ms/step - loss: 0.1635 - mse: 0.1635 - val_loss: 0.1747 - val_mse: 0.1747\n",
            "Epoch 2/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 0.0780 - val_mse: 0.0780\n",
            "Epoch 3/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 0.0442 - val_mse: 0.0442\n",
            "Epoch 4/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0402 - val_mse: 0.0402\n",
            "Epoch 5/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0214 - val_mse: 0.0214\n",
            "Epoch 6/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0186 - val_mse: 0.0186\n",
            "Epoch 7/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0124 - val_mse: 0.0124\n",
            "Epoch 8/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0162 - val_mse: 0.0162\n",
            "Epoch 9/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0165 - val_mse: 0.0165\n",
            "Epoch 10/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0145 - val_mse: 0.0145\n",
            "Epoch 11/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0085 - val_mse: 0.0085\n",
            "Epoch 12/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0112 - val_mse: 0.0112\n",
            "Epoch 13/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0085 - val_mse: 0.0085\n",
            "Epoch 14/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0150 - val_mse: 0.0150\n",
            "Epoch 15/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0066 - val_mse: 0.0066\n",
            "Epoch 16/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0088 - val_mse: 0.0088\n",
            "Epoch 17/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0089 - val_mse: 0.0089\n",
            "Epoch 18/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0088 - val_mse: 0.0088\n",
            "Epoch 19/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0061 - val_mse: 0.0061\n",
            "Epoch 20/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0063 - val_mse: 0.0063\n",
            "Epoch 21/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0055 - val_mse: 0.0055\n",
            "Epoch 22/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0109 - val_mse: 0.0109\n",
            "Epoch 23/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0093 - val_mse: 0.0093\n",
            "Epoch 24/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0062 - val_mse: 0.0062\n",
            "Epoch 25/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0049 - val_mse: 0.0049\n",
            "Epoch 26/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0056 - val_mse: 0.0056\n",
            "Epoch 27/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0050 - val_mse: 0.0050\n",
            "Epoch 28/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0046 - val_mse: 0.0046\n",
            "Epoch 29/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0050 - val_mse: 0.0050\n",
            "Epoch 30/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0043 - val_mse: 0.0043\n",
            "Epoch 31/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0077 - val_mse: 0.0077\n",
            "Epoch 32/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0054 - val_mse: 0.0054\n",
            "Epoch 33/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0046 - val_mse: 0.0046\n",
            "Epoch 34/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0052 - val_mse: 0.0052\n",
            "Epoch 35/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 0.0054 - val_mse: 0.0054\n",
            "Epoch 36/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0060 - val_mse: 0.0060\n",
            "Epoch 37/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 0.0046 - val_mse: 0.0046\n",
            "Epoch 38/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 0.0047 - val_mse: 0.0047\n",
            "Epoch 39/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0058 - val_mse: 0.0058\n",
            "Epoch 40/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 0.0075 - val_mse: 0.0075\n",
            "Epoch 41/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0058 - val_mse: 0.0058\n",
            "Epoch 42/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 0.0069 - val_mse: 0.0069\n",
            "Epoch 43/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0058 - val_mse: 0.0058\n",
            "Epoch 44/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0046 - val_mse: 0.0046\n",
            "Epoch 45/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 0.0086 - val_mse: 0.0086\n",
            "Epoch 46/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 0.0074 - val_mse: 0.0074\n",
            "Epoch 47/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0055 - val_mse: 0.0055\n",
            "Epoch 48/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 0.0044 - val_mse: 0.0044\n",
            "Epoch 49/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0082 - val_mse: 0.0082\n",
            "Epoch 50/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 9.9296e-04 - mse: 9.9296e-04 - val_loss: 0.0042 - val_mse: 0.0042\n",
            "Infering 252 values\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Epoch 1/50\n",
            "200/200 [==============================] - 8s 25ms/step - loss: 0.1730 - mse: 0.1730 - val_loss: 0.6340 - val_mse: 0.6340\n",
            "Epoch 2/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0070 - mse: 0.0070 - val_loss: 0.2310 - val_mse: 0.2310\n",
            "Epoch 3/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0051 - mse: 0.0051 - val_loss: 0.1041 - val_mse: 0.1041\n",
            "Epoch 4/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0043 - mse: 0.0043 - val_loss: 0.0556 - val_mse: 0.0556\n",
            "Epoch 5/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0038 - mse: 0.0038 - val_loss: 0.0305 - val_mse: 0.0305\n",
            "Epoch 6/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0299 - val_mse: 0.0299\n",
            "Epoch 7/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0186 - val_mse: 0.0186\n",
            "Epoch 8/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 0.0263 - val_mse: 0.0263\n",
            "Epoch 9/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0029 - mse: 0.0029 - val_loss: 0.0331 - val_mse: 0.0331\n",
            "Epoch 10/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0028 - mse: 0.0028 - val_loss: 0.0199 - val_mse: 0.0199\n",
            "Epoch 11/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0028 - mse: 0.0028 - val_loss: 0.0141 - val_mse: 0.0141\n",
            "Epoch 12/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0027 - mse: 0.0027 - val_loss: 0.0129 - val_mse: 0.0129\n",
            "Epoch 13/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0027 - mse: 0.0027 - val_loss: 0.0141 - val_mse: 0.0141\n",
            "Epoch 14/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0026 - mse: 0.0026 - val_loss: 0.0166 - val_mse: 0.0166\n",
            "Epoch 15/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 0.0076 - val_mse: 0.0076\n",
            "Epoch 16/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 0.0110 - val_mse: 0.0110\n",
            "Epoch 17/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0026 - mse: 0.0026 - val_loss: 0.0081 - val_mse: 0.0081\n",
            "Epoch 18/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 0.0090 - val_mse: 0.0090\n",
            "Epoch 19/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 0.0079 - val_mse: 0.0079\n",
            "Epoch 20/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 0.0066 - val_mse: 0.0066\n",
            "Epoch 21/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 0.0095 - val_mse: 0.0095\n",
            "Epoch 22/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 0.0119 - val_mse: 0.0119\n",
            "Epoch 23/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 0.0123 - val_mse: 0.0123\n",
            "Epoch 24/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 0.0098 - val_mse: 0.0098\n",
            "Epoch 25/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 0.0078 - val_mse: 0.0078\n",
            "Epoch 26/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 0.0089 - val_mse: 0.0089\n",
            "Epoch 27/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 0.0065 - val_mse: 0.0065\n",
            "Epoch 28/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 0.0074 - val_mse: 0.0074\n",
            "Epoch 29/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 0.0118 - val_mse: 0.0118\n",
            "Epoch 30/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 0.0059 - val_mse: 0.0059\n",
            "Epoch 31/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 0.0072 - val_mse: 0.0072\n",
            "Epoch 32/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 0.0061 - val_mse: 0.0061\n",
            "Epoch 33/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 0.0058 - val_mse: 0.0058\n",
            "Epoch 34/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 0.0070 - val_mse: 0.0070\n",
            "Epoch 35/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0061 - val_mse: 0.0061\n",
            "Epoch 36/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 0.0057 - val_mse: 0.0057\n",
            "Epoch 37/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0057 - val_mse: 0.0057\n",
            "Epoch 38/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0089 - val_mse: 0.0089\n",
            "Epoch 39/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0063 - val_mse: 0.0063\n",
            "Epoch 40/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0066 - val_mse: 0.0066\n",
            "Epoch 41/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0069 - val_mse: 0.0069\n",
            "Epoch 42/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.0140 - val_mse: 0.0140\n",
            "Epoch 43/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0072 - val_mse: 0.0072\n",
            "Epoch 44/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 0.0059 - val_mse: 0.0059\n",
            "Epoch 45/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0098 - val_mse: 0.0098\n",
            "Epoch 46/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0092 - val_mse: 0.0092\n",
            "Epoch 47/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0088 - val_mse: 0.0088\n",
            "Epoch 48/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0072 - val_mse: 0.0072\n",
            "Epoch 49/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.0087 - val_mse: 0.0087\n",
            "Epoch 50/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.0088 - val_mse: 0.0088\n",
            "Infering 252 values\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Epoch 1/50\n",
            "200/200 [==============================] - 8s 25ms/step - loss: 0.1725 - mse: 0.1725 - val_loss: 0.1203 - val_mse: 0.1203\n",
            "Epoch 2/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0031 - mse: 0.0031 - val_loss: 0.0490 - val_mse: 0.0490\n",
            "Epoch 3/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 0.0370 - val_mse: 0.0370\n",
            "Epoch 4/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0432 - val_mse: 0.0432\n",
            "Epoch 5/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.0469 - val_mse: 0.0469\n",
            "Epoch 6/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0227 - val_mse: 0.0227\n",
            "Epoch 7/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0176 - val_mse: 0.0176\n",
            "Epoch 8/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0179 - val_mse: 0.0179\n",
            "Epoch 9/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0160 - val_mse: 0.0160\n",
            "Epoch 10/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0133 - val_mse: 0.0133\n",
            "Epoch 11/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0121 - val_mse: 0.0121\n",
            "Epoch 12/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0103 - val_mse: 0.0103\n",
            "Epoch 13/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0102 - val_mse: 0.0102\n",
            "Epoch 14/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0165 - val_mse: 0.0165\n",
            "Epoch 15/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0089 - val_mse: 0.0089\n",
            "Epoch 16/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0088 - val_mse: 0.0088\n",
            "Epoch 17/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0094 - val_mse: 0.0094\n",
            "Epoch 18/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0082 - val_mse: 0.0082\n",
            "Epoch 19/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0104 - val_mse: 0.0104\n",
            "Epoch 20/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0127 - val_mse: 0.0127\n",
            "Epoch 21/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0087 - val_mse: 0.0087\n",
            "Epoch 22/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0134 - val_mse: 0.0134\n",
            "Epoch 23/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0084 - val_mse: 0.0084\n",
            "Epoch 24/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0080 - val_mse: 0.0080\n",
            "Epoch 25/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0071 - val_mse: 0.0071\n",
            "Epoch 26/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0083 - val_mse: 0.0083\n",
            "Epoch 27/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0089 - val_mse: 0.0089\n",
            "Epoch 28/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 9.8568e-04 - mse: 9.8568e-04 - val_loss: 0.0065 - val_mse: 0.0065\n",
            "Epoch 29/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0098 - val_mse: 0.0098\n",
            "Epoch 30/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 0.0085 - val_mse: 0.0085\n",
            "Epoch 31/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0100 - val_mse: 0.0100\n",
            "Epoch 32/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 0.0107 - val_mse: 0.0107\n",
            "Epoch 33/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 0.0077 - val_mse: 0.0077\n",
            "Epoch 34/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 0.0085 - val_mse: 0.0085\n",
            "Epoch 35/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0075 - val_mse: 0.0075\n",
            "Epoch 36/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0069 - val_mse: 0.0069\n",
            "Epoch 37/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 0.0067 - val_mse: 0.0067\n",
            "Epoch 38/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 9.9495e-04 - mse: 9.9495e-04 - val_loss: 0.0066 - val_mse: 0.0066\n",
            "Epoch 39/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 0.0103 - val_mse: 0.0103\n",
            "Epoch 40/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 0.0095 - val_mse: 0.0095\n",
            "Epoch 41/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 0.0073 - val_mse: 0.0073\n",
            "Epoch 42/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0195 - val_mse: 0.0195\n",
            "Epoch 43/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0104 - val_mse: 0.0104\n",
            "Epoch 44/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0095 - val_mse: 0.0095\n",
            "Epoch 45/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 9.9155e-04 - mse: 9.9155e-04 - val_loss: 0.0116 - val_mse: 0.0116\n",
            "Epoch 46/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 0.0132 - val_mse: 0.0132\n",
            "Epoch 47/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 9.9305e-04 - mse: 9.9305e-04 - val_loss: 0.0084 - val_mse: 0.0084\n",
            "Epoch 48/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 0.0087 - val_mse: 0.0087\n",
            "Epoch 49/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 0.0133 - val_mse: 0.0133\n",
            "Epoch 50/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 9.6933e-04 - mse: 9.6933e-04 - val_loss: 0.0072 - val_mse: 0.0072\n",
            "Infering 252 values\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Epoch 1/50\n",
            "200/200 [==============================] - 8s 24ms/step - loss: 0.3217 - mse: 0.3217 - val_loss: 49.7464 - val_mse: 49.7464\n",
            "Epoch 2/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0139 - mse: 0.0139 - val_loss: 37.9500 - val_mse: 37.9500\n",
            "Epoch 3/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0086 - mse: 0.0086 - val_loss: 34.7070 - val_mse: 34.7070\n",
            "Epoch 4/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0072 - mse: 0.0072 - val_loss: 33.2495 - val_mse: 33.2495\n",
            "Epoch 5/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0058 - mse: 0.0058 - val_loss: 32.7609 - val_mse: 32.7609\n",
            "Epoch 6/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0066 - mse: 0.0066 - val_loss: 31.7152 - val_mse: 31.7152\n",
            "Epoch 7/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0056 - mse: 0.0056 - val_loss: 30.7188 - val_mse: 30.7188\n",
            "Epoch 8/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0057 - mse: 0.0057 - val_loss: 29.8135 - val_mse: 29.8135\n",
            "Epoch 9/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0055 - mse: 0.0055 - val_loss: 28.8551 - val_mse: 28.8551\n",
            "Epoch 10/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0051 - mse: 0.0051 - val_loss: 28.0145 - val_mse: 28.0145\n",
            "Epoch 11/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0052 - mse: 0.0052 - val_loss: 26.6348 - val_mse: 26.6348\n",
            "Epoch 12/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0049 - mse: 0.0049 - val_loss: 26.3690 - val_mse: 26.3690\n",
            "Epoch 13/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0046 - mse: 0.0046 - val_loss: 25.9425 - val_mse: 25.9425\n",
            "Epoch 14/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0054 - mse: 0.0054 - val_loss: 24.5222 - val_mse: 24.5222\n",
            "Epoch 15/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 24.2099 - val_mse: 24.2099\n",
            "Epoch 16/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 23.7377 - val_mse: 23.7377\n",
            "Epoch 17/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0040 - mse: 0.0040 - val_loss: 23.6488 - val_mse: 23.6488\n",
            "Epoch 18/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0043 - mse: 0.0043 - val_loss: 23.5631 - val_mse: 23.5631\n",
            "Epoch 19/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0040 - mse: 0.0040 - val_loss: 23.4635 - val_mse: 23.4635\n",
            "Epoch 20/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0046 - mse: 0.0046 - val_loss: 23.1071 - val_mse: 23.1071\n",
            "Epoch 21/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0039 - mse: 0.0039 - val_loss: 22.8317 - val_mse: 22.8317\n",
            "Epoch 22/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0041 - mse: 0.0041 - val_loss: 23.1799 - val_mse: 23.1799\n",
            "Epoch 23/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0037 - mse: 0.0037 - val_loss: 22.7251 - val_mse: 22.7251\n",
            "Epoch 24/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0039 - mse: 0.0039 - val_loss: 22.5195 - val_mse: 22.5195\n",
            "Epoch 25/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0038 - mse: 0.0038 - val_loss: 22.6152 - val_mse: 22.6152\n",
            "Epoch 26/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0047 - mse: 0.0047 - val_loss: 21.2311 - val_mse: 21.2311\n",
            "Epoch 27/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0038 - mse: 0.0038 - val_loss: 21.6370 - val_mse: 21.6370\n",
            "Epoch 28/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0040 - mse: 0.0040 - val_loss: 21.7716 - val_mse: 21.7716\n",
            "Epoch 29/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0039 - mse: 0.0039 - val_loss: 22.2167 - val_mse: 22.2167\n",
            "Epoch 30/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0040 - mse: 0.0040 - val_loss: 22.0952 - val_mse: 22.0952\n",
            "Epoch 31/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0041 - mse: 0.0041 - val_loss: 22.0023 - val_mse: 22.0023\n",
            "Epoch 32/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 21.9234 - val_mse: 21.9234\n",
            "Epoch 33/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0041 - mse: 0.0041 - val_loss: 21.9661 - val_mse: 21.9661\n",
            "Epoch 34/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 22.3139 - val_mse: 22.3139\n",
            "Epoch 35/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0038 - mse: 0.0038 - val_loss: 22.0636 - val_mse: 22.0636\n",
            "Epoch 36/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 22.9810 - val_mse: 22.9810\n",
            "Epoch 37/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0033 - mse: 0.0033 - val_loss: 21.9729 - val_mse: 21.9729\n",
            "Epoch 38/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 21.7447 - val_mse: 21.7447\n",
            "Epoch 39/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0037 - mse: 0.0037 - val_loss: 22.5780 - val_mse: 22.5780\n",
            "Epoch 40/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0031 - mse: 0.0031 - val_loss: 21.7248 - val_mse: 21.7248\n",
            "Epoch 41/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 21.8688 - val_mse: 21.8688\n",
            "Epoch 42/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0036 - mse: 0.0036 - val_loss: 22.3802 - val_mse: 22.3802\n",
            "Epoch 43/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0031 - mse: 0.0031 - val_loss: 22.1166 - val_mse: 22.1166\n",
            "Epoch 44/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 22.2904 - val_mse: 22.2904\n",
            "Epoch 45/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0029 - mse: 0.0029 - val_loss: 22.3037 - val_mse: 22.3037\n",
            "Epoch 46/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0036 - mse: 0.0036 - val_loss: 22.6443 - val_mse: 22.6443\n",
            "Epoch 47/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 22.3987 - val_mse: 22.3987\n",
            "Epoch 48/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 23.6792 - val_mse: 23.6792\n",
            "Epoch 49/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0032 - mse: 0.0032 - val_loss: 22.3731 - val_mse: 22.3731\n",
            "Epoch 50/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0042 - mse: 0.0042 - val_loss: 21.3786 - val_mse: 21.3786\n",
            "Infering 239 values\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Epoch 1/50\n",
            "200/200 [==============================] - 8s 24ms/step - loss: 0.1613 - mse: 0.1613 - val_loss: 0.7874 - val_mse: 0.7874\n",
            "Epoch 2/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0037 - mse: 0.0037 - val_loss: 0.3952 - val_mse: 0.3952\n",
            "Epoch 3/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0027 - mse: 0.0027 - val_loss: 0.2630 - val_mse: 0.2630\n",
            "Epoch 4/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 0.1943 - val_mse: 0.1943\n",
            "Epoch 5/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.1470 - val_mse: 0.1470\n",
            "Epoch 6/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.1257 - val_mse: 0.1257\n",
            "Epoch 7/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.0963 - val_mse: 0.0963\n",
            "Epoch 8/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0800 - val_mse: 0.0800\n",
            "Epoch 9/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0731 - val_mse: 0.0731\n",
            "Epoch 10/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0619 - val_mse: 0.0619\n",
            "Epoch 11/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0580 - val_mse: 0.0580\n",
            "Epoch 12/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0499 - val_mse: 0.0499\n",
            "Epoch 13/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0491 - val_mse: 0.0491\n",
            "Epoch 14/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0457 - val_mse: 0.0457\n",
            "Epoch 15/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0422 - val_mse: 0.0422\n",
            "Epoch 16/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0510 - val_mse: 0.0510\n",
            "Epoch 17/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0417 - val_mse: 0.0417\n",
            "Epoch 18/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0526 - val_mse: 0.0526\n",
            "Epoch 19/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0483 - val_mse: 0.0483\n",
            "Epoch 20/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0369 - val_mse: 0.0369\n",
            "Epoch 21/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0373 - val_mse: 0.0373\n",
            "Epoch 22/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0485 - val_mse: 0.0485\n",
            "Epoch 23/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0412 - val_mse: 0.0412\n",
            "Epoch 24/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0370 - val_mse: 0.0370\n",
            "Epoch 25/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0365 - val_mse: 0.0365\n",
            "Epoch 26/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0322 - val_mse: 0.0322\n",
            "Epoch 27/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0292 - val_mse: 0.0292\n",
            "Epoch 28/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0293 - val_mse: 0.0293\n",
            "Epoch 29/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0333 - val_mse: 0.0333\n",
            "Epoch 30/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0265 - val_mse: 0.0265\n",
            "Epoch 31/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0384 - val_mse: 0.0384\n",
            "Epoch 32/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0314 - val_mse: 0.0314\n",
            "Epoch 33/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0309 - val_mse: 0.0309\n",
            "Epoch 34/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0298 - val_mse: 0.0298\n",
            "Epoch 35/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0362 - val_mse: 0.0362\n",
            "Epoch 36/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0265 - val_mse: 0.0265\n",
            "Epoch 37/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0270 - val_mse: 0.0270\n",
            "Epoch 38/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0290 - val_mse: 0.0290\n",
            "Epoch 39/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0315 - val_mse: 0.0315\n",
            "Epoch 40/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0334 - val_mse: 0.0334\n",
            "Epoch 41/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0310 - val_mse: 0.0310\n",
            "Epoch 42/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0307 - val_mse: 0.0307\n",
            "Epoch 43/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0324 - val_mse: 0.0324\n",
            "Epoch 44/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0291 - val_mse: 0.0291\n",
            "Epoch 45/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0360 - val_mse: 0.0360\n",
            "Epoch 46/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0364 - val_mse: 0.0364\n",
            "Epoch 47/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0376 - val_mse: 0.0376\n",
            "Epoch 48/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0312 - val_mse: 0.0312\n",
            "Epoch 49/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0321 - val_mse: 0.0321\n",
            "Epoch 50/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 0.0355 - val_mse: 0.0355\n",
            "Infering 252 values\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Epoch 1/50\n",
            "200/200 [==============================] - 8s 26ms/step - loss: 0.1744 - mse: 0.1744 - val_loss: 0.1859 - val_mse: 0.1859\n",
            "Epoch 2/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0073 - mse: 0.0073 - val_loss: 0.0489 - val_mse: 0.0489\n",
            "Epoch 3/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0058 - mse: 0.0058 - val_loss: 0.0277 - val_mse: 0.0277\n",
            "Epoch 4/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0052 - mse: 0.0052 - val_loss: 0.0232 - val_mse: 0.0232\n",
            "Epoch 5/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0048 - mse: 0.0048 - val_loss: 0.0211 - val_mse: 0.0211\n",
            "Epoch 6/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0043 - mse: 0.0043 - val_loss: 0.0166 - val_mse: 0.0166\n",
            "Epoch 7/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0042 - mse: 0.0042 - val_loss: 0.0132 - val_mse: 0.0132\n",
            "Epoch 8/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0038 - mse: 0.0038 - val_loss: 0.0158 - val_mse: 0.0158\n",
            "Epoch 9/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0038 - mse: 0.0038 - val_loss: 0.0125 - val_mse: 0.0125\n",
            "Epoch 10/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0036 - mse: 0.0036 - val_loss: 0.0098 - val_mse: 0.0098\n",
            "Epoch 11/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0033 - mse: 0.0033 - val_loss: 0.0100 - val_mse: 0.0100\n",
            "Epoch 12/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0088 - val_mse: 0.0088\n",
            "Epoch 13/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0033 - mse: 0.0033 - val_loss: 0.0084 - val_mse: 0.0084\n",
            "Epoch 14/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0033 - mse: 0.0033 - val_loss: 0.0093 - val_mse: 0.0093\n",
            "Epoch 15/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0032 - mse: 0.0032 - val_loss: 0.0075 - val_mse: 0.0075\n",
            "Epoch 16/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0031 - mse: 0.0031 - val_loss: 0.0073 - val_mse: 0.0073\n",
            "Epoch 17/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0032 - mse: 0.0032 - val_loss: 0.0076 - val_mse: 0.0076\n",
            "Epoch 18/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 0.0101 - val_mse: 0.0101\n",
            "Epoch 19/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0029 - mse: 0.0029 - val_loss: 0.0073 - val_mse: 0.0073\n",
            "Epoch 20/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 0.0075 - val_mse: 0.0075\n",
            "Epoch 21/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 0.0068 - val_mse: 0.0068\n",
            "Epoch 22/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 0.0077 - val_mse: 0.0077\n",
            "Epoch 23/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 0.0065 - val_mse: 0.0065\n",
            "Epoch 24/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0029 - mse: 0.0029 - val_loss: 0.0063 - val_mse: 0.0063\n",
            "Epoch 25/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0028 - mse: 0.0028 - val_loss: 0.0071 - val_mse: 0.0071\n",
            "Epoch 26/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 0.0074 - val_mse: 0.0074\n",
            "Epoch 27/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 0.0061 - val_mse: 0.0061\n",
            "Epoch 28/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0028 - mse: 0.0028 - val_loss: 0.0059 - val_mse: 0.0059\n",
            "Epoch 29/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 0.0059 - val_mse: 0.0059\n",
            "Epoch 30/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0029 - mse: 0.0029 - val_loss: 0.0117 - val_mse: 0.0117\n",
            "Epoch 31/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 0.0081 - val_mse: 0.0081\n",
            "Epoch 32/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 0.0059 - val_mse: 0.0059\n",
            "Epoch 33/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0028 - mse: 0.0028 - val_loss: 0.0072 - val_mse: 0.0072\n",
            "Epoch 34/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0029 - mse: 0.0029 - val_loss: 0.0062 - val_mse: 0.0062\n",
            "Epoch 35/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0029 - mse: 0.0029 - val_loss: 0.0066 - val_mse: 0.0066\n",
            "Epoch 36/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0028 - mse: 0.0028 - val_loss: 0.0072 - val_mse: 0.0072\n",
            "Epoch 37/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0028 - mse: 0.0028 - val_loss: 0.0058 - val_mse: 0.0058\n",
            "Epoch 38/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0028 - mse: 0.0028 - val_loss: 0.0072 - val_mse: 0.0072\n",
            "Epoch 39/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 0.0076 - val_mse: 0.0076\n",
            "Epoch 40/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0028 - mse: 0.0028 - val_loss: 0.0109 - val_mse: 0.0109\n",
            "Epoch 41/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0027 - mse: 0.0027 - val_loss: 0.0075 - val_mse: 0.0075\n",
            "Epoch 42/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0028 - mse: 0.0028 - val_loss: 0.0081 - val_mse: 0.0081\n",
            "Epoch 43/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0029 - mse: 0.0029 - val_loss: 0.0056 - val_mse: 0.0056\n",
            "Epoch 44/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0028 - mse: 0.0028 - val_loss: 0.0083 - val_mse: 0.0083\n",
            "Epoch 45/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0028 - mse: 0.0028 - val_loss: 0.0059 - val_mse: 0.0059\n",
            "Epoch 46/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0028 - mse: 0.0028 - val_loss: 0.0056 - val_mse: 0.0056\n",
            "Epoch 47/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0029 - mse: 0.0029 - val_loss: 0.0068 - val_mse: 0.0068\n",
            "Epoch 48/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0028 - mse: 0.0028 - val_loss: 0.0067 - val_mse: 0.0067\n",
            "Epoch 49/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0028 - mse: 0.0028 - val_loss: 0.0176 - val_mse: 0.0176\n",
            "Epoch 50/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0028 - mse: 0.0028 - val_loss: 0.0055 - val_mse: 0.0055\n",
            "Infering 252 values\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Epoch 1/50\n",
            "200/200 [==============================] - 8s 25ms/step - loss: 0.1790 - mse: 0.1790 - val_loss: 0.2800 - val_mse: 0.2800\n",
            "Epoch 2/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0077 - mse: 0.0077 - val_loss: 0.0959 - val_mse: 0.0959\n",
            "Epoch 3/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0063 - mse: 0.0063 - val_loss: 0.0744 - val_mse: 0.0744\n",
            "Epoch 4/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0056 - mse: 0.0056 - val_loss: 0.0689 - val_mse: 0.0689\n",
            "Epoch 5/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0053 - mse: 0.0053 - val_loss: 0.0503 - val_mse: 0.0503\n",
            "Epoch 6/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0050 - mse: 0.0050 - val_loss: 0.0432 - val_mse: 0.0432\n",
            "Epoch 7/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0047 - mse: 0.0047 - val_loss: 0.0375 - val_mse: 0.0375\n",
            "Epoch 8/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0043 - mse: 0.0043 - val_loss: 0.0382 - val_mse: 0.0382\n",
            "Epoch 9/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0043 - mse: 0.0043 - val_loss: 0.0411 - val_mse: 0.0411\n",
            "Epoch 10/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0039 - mse: 0.0039 - val_loss: 0.0267 - val_mse: 0.0267\n",
            "Epoch 11/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0040 - mse: 0.0040 - val_loss: 0.0252 - val_mse: 0.0252\n",
            "Epoch 12/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0038 - mse: 0.0038 - val_loss: 0.0220 - val_mse: 0.0220\n",
            "Epoch 13/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0039 - mse: 0.0039 - val_loss: 0.0221 - val_mse: 0.0221\n",
            "Epoch 14/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0037 - mse: 0.0037 - val_loss: 0.0240 - val_mse: 0.0240\n",
            "Epoch 15/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0203 - val_mse: 0.0203\n",
            "Epoch 16/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0037 - mse: 0.0037 - val_loss: 0.0216 - val_mse: 0.0216\n",
            "Epoch 17/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0037 - mse: 0.0037 - val_loss: 0.0219 - val_mse: 0.0219\n",
            "Epoch 18/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0036 - mse: 0.0036 - val_loss: 0.0225 - val_mse: 0.0225\n",
            "Epoch 19/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0190 - val_mse: 0.0190\n",
            "Epoch 20/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0186 - val_mse: 0.0186\n",
            "Epoch 21/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0207 - val_mse: 0.0207\n",
            "Epoch 22/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0225 - val_mse: 0.0225\n",
            "Epoch 23/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0206 - val_mse: 0.0206\n",
            "Epoch 24/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0036 - mse: 0.0036 - val_loss: 0.0193 - val_mse: 0.0193\n",
            "Epoch 25/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0032 - mse: 0.0032 - val_loss: 0.0190 - val_mse: 0.0190\n",
            "Epoch 26/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0259 - val_mse: 0.0259\n",
            "Epoch 27/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0184 - val_mse: 0.0184\n",
            "Epoch 28/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0032 - mse: 0.0032 - val_loss: 0.0198 - val_mse: 0.0198\n",
            "Epoch 29/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0037 - mse: 0.0037 - val_loss: 0.0216 - val_mse: 0.0216\n",
            "Epoch 30/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0032 - mse: 0.0032 - val_loss: 0.0222 - val_mse: 0.0222\n",
            "Epoch 31/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0036 - mse: 0.0036 - val_loss: 0.0264 - val_mse: 0.0264\n",
            "Epoch 32/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0274 - val_mse: 0.0274\n",
            "Epoch 33/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0032 - mse: 0.0032 - val_loss: 0.0196 - val_mse: 0.0196\n",
            "Epoch 34/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0215 - val_mse: 0.0215\n",
            "Epoch 35/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0032 - mse: 0.0032 - val_loss: 0.0238 - val_mse: 0.0238\n",
            "Epoch 36/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0208 - val_mse: 0.0208\n",
            "Epoch 37/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0032 - mse: 0.0032 - val_loss: 0.0225 - val_mse: 0.0225\n",
            "Epoch 38/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0033 - mse: 0.0033 - val_loss: 0.0214 - val_mse: 0.0214\n",
            "Epoch 39/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0033 - mse: 0.0033 - val_loss: 0.0221 - val_mse: 0.0221\n",
            "Epoch 40/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0033 - mse: 0.0033 - val_loss: 0.0284 - val_mse: 0.0284\n",
            "Epoch 41/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0033 - mse: 0.0033 - val_loss: 0.0266 - val_mse: 0.0266\n",
            "Epoch 42/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0031 - mse: 0.0031 - val_loss: 0.0271 - val_mse: 0.0271\n",
            "Epoch 43/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0032 - mse: 0.0032 - val_loss: 0.0216 - val_mse: 0.0216\n",
            "Epoch 44/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0244 - val_mse: 0.0244\n",
            "Epoch 45/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0033 - mse: 0.0033 - val_loss: 0.0234 - val_mse: 0.0234\n",
            "Epoch 46/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0032 - mse: 0.0032 - val_loss: 0.0237 - val_mse: 0.0237\n",
            "Epoch 47/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0033 - mse: 0.0033 - val_loss: 0.0286 - val_mse: 0.0286\n",
            "Epoch 48/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0032 - mse: 0.0032 - val_loss: 0.0242 - val_mse: 0.0242\n",
            "Epoch 49/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0033 - mse: 0.0033 - val_loss: 0.0355 - val_mse: 0.0355\n",
            "Epoch 50/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0031 - mse: 0.0031 - val_loss: 0.0255 - val_mse: 0.0255\n",
            "Infering 252 values\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Epoch 1/50\n",
            "200/200 [==============================] - 8s 25ms/step - loss: 0.1784 - mse: 0.1784 - val_loss: 0.0400 - val_mse: 0.0400\n",
            "Epoch 2/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0140 - mse: 0.0140 - val_loss: 0.0211 - val_mse: 0.0211\n",
            "Epoch 3/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0161 - val_mse: 0.0161\n",
            "Epoch 4/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0080 - mse: 0.0080 - val_loss: 0.0102 - val_mse: 0.0102\n",
            "Epoch 5/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0070 - mse: 0.0070 - val_loss: 0.0089 - val_mse: 0.0089\n",
            "Epoch 6/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0069 - mse: 0.0069 - val_loss: 0.0080 - val_mse: 0.0080\n",
            "Epoch 7/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0065 - mse: 0.0065 - val_loss: 0.0106 - val_mse: 0.0106\n",
            "Epoch 8/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0061 - mse: 0.0061 - val_loss: 0.0084 - val_mse: 0.0084\n",
            "Epoch 9/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0060 - mse: 0.0060 - val_loss: 0.0122 - val_mse: 0.0122\n",
            "Epoch 10/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0056 - mse: 0.0056 - val_loss: 0.0076 - val_mse: 0.0076\n",
            "Epoch 11/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0060 - mse: 0.0060 - val_loss: 0.0074 - val_mse: 0.0074\n",
            "Epoch 12/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0055 - mse: 0.0055 - val_loss: 0.0074 - val_mse: 0.0074\n",
            "Epoch 13/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0057 - mse: 0.0057 - val_loss: 0.0075 - val_mse: 0.0075\n",
            "Epoch 14/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0057 - mse: 0.0057 - val_loss: 0.0069 - val_mse: 0.0069\n",
            "Epoch 15/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0054 - mse: 0.0054 - val_loss: 0.0067 - val_mse: 0.0067\n",
            "Epoch 16/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0056 - mse: 0.0056 - val_loss: 0.0071 - val_mse: 0.0071\n",
            "Epoch 17/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0055 - mse: 0.0055 - val_loss: 0.0070 - val_mse: 0.0070\n",
            "Epoch 18/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0056 - mse: 0.0056 - val_loss: 0.0086 - val_mse: 0.0086\n",
            "Epoch 19/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0055 - mse: 0.0055 - val_loss: 0.0069 - val_mse: 0.0069\n",
            "Epoch 20/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0053 - mse: 0.0053 - val_loss: 0.0067 - val_mse: 0.0067\n",
            "Epoch 21/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0054 - mse: 0.0054 - val_loss: 0.0069 - val_mse: 0.0069\n",
            "Epoch 22/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0054 - mse: 0.0054 - val_loss: 0.0072 - val_mse: 0.0072\n",
            "Epoch 23/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0054 - mse: 0.0054 - val_loss: 0.0070 - val_mse: 0.0070\n",
            "Epoch 24/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0053 - mse: 0.0053 - val_loss: 0.0072 - val_mse: 0.0072\n",
            "Epoch 25/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0053 - mse: 0.0053 - val_loss: 0.0079 - val_mse: 0.0079\n",
            "Epoch 26/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0054 - mse: 0.0054 - val_loss: 0.0070 - val_mse: 0.0070\n",
            "Epoch 27/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0052 - mse: 0.0052 - val_loss: 0.0075 - val_mse: 0.0075\n",
            "Epoch 28/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0052 - mse: 0.0052 - val_loss: 0.0079 - val_mse: 0.0079\n",
            "Epoch 29/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0054 - mse: 0.0054 - val_loss: 0.0076 - val_mse: 0.0076\n",
            "Epoch 30/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0051 - mse: 0.0051 - val_loss: 0.0080 - val_mse: 0.0080\n",
            "Epoch 31/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0055 - mse: 0.0055 - val_loss: 0.0119 - val_mse: 0.0119\n",
            "Epoch 32/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0050 - mse: 0.0050 - val_loss: 0.0079 - val_mse: 0.0079\n",
            "Epoch 33/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0053 - mse: 0.0053 - val_loss: 0.0078 - val_mse: 0.0078\n",
            "Epoch 34/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0051 - mse: 0.0051 - val_loss: 0.0078 - val_mse: 0.0078\n",
            "Epoch 35/50\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.0050 - mse: 0.0050 - val_loss: 0.0075 - val_mse: 0.0075\n",
            "Epoch 36/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0052 - mse: 0.0052 - val_loss: 0.0072 - val_mse: 0.0072\n",
            "Epoch 37/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0052 - mse: 0.0052 - val_loss: 0.0069 - val_mse: 0.0069\n",
            "Epoch 38/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0051 - mse: 0.0051 - val_loss: 0.0068 - val_mse: 0.0068\n",
            "Epoch 39/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0051 - mse: 0.0051 - val_loss: 0.0074 - val_mse: 0.0074\n",
            "Epoch 40/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0051 - mse: 0.0051 - val_loss: 0.0067 - val_mse: 0.0067\n",
            "Epoch 41/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0052 - mse: 0.0052 - val_loss: 0.0166 - val_mse: 0.0166\n",
            "Epoch 42/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0048 - mse: 0.0048 - val_loss: 0.0155 - val_mse: 0.0155\n",
            "Epoch 43/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0051 - mse: 0.0051 - val_loss: 0.0072 - val_mse: 0.0072\n",
            "Epoch 44/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0051 - mse: 0.0051 - val_loss: 0.0072 - val_mse: 0.0072\n",
            "Epoch 45/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0050 - mse: 0.0050 - val_loss: 0.0096 - val_mse: 0.0096\n",
            "Epoch 46/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0050 - mse: 0.0050 - val_loss: 0.0134 - val_mse: 0.0134\n",
            "Epoch 47/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0050 - mse: 0.0050 - val_loss: 0.0136 - val_mse: 0.0136\n",
            "Epoch 48/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0052 - mse: 0.0052 - val_loss: 0.0075 - val_mse: 0.0075\n",
            "Epoch 49/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0051 - mse: 0.0051 - val_loss: 0.0081 - val_mse: 0.0081\n",
            "Epoch 50/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0048 - mse: 0.0048 - val_loss: 0.0079 - val_mse: 0.0079\n",
            "Infering 252 values\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Epoch 1/50\n",
            "200/200 [==============================] - 8s 27ms/step - loss: 0.2030 - mse: 0.2030 - val_loss: 2.7727 - val_mse: 2.7727\n",
            "Epoch 2/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0056 - mse: 0.0056 - val_loss: 1.4771 - val_mse: 1.4771\n",
            "Epoch 3/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0041 - mse: 0.0041 - val_loss: 1.1916 - val_mse: 1.1916\n",
            "Epoch 4/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0036 - mse: 0.0036 - val_loss: 1.1521 - val_mse: 1.1521\n",
            "Epoch 5/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.9930 - val_mse: 0.9930\n",
            "Epoch 6/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0031 - mse: 0.0031 - val_loss: 0.7454 - val_mse: 0.7454\n",
            "Epoch 7/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0026 - mse: 0.0026 - val_loss: 0.6537 - val_mse: 0.6537\n",
            "Epoch 8/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0027 - mse: 0.0027 - val_loss: 0.6151 - val_mse: 0.6151\n",
            "Epoch 9/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 0.4743 - val_mse: 0.4743\n",
            "Epoch 10/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 0.5027 - val_mse: 0.5027\n",
            "Epoch 11/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 0.4406 - val_mse: 0.4406\n",
            "Epoch 12/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.3635 - val_mse: 0.3635\n",
            "Epoch 13/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.3384 - val_mse: 0.3384\n",
            "Epoch 14/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.4104 - val_mse: 0.4104\n",
            "Epoch 15/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.2951 - val_mse: 0.2951\n",
            "Epoch 16/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.3079 - val_mse: 0.3079\n",
            "Epoch 17/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.3111 - val_mse: 0.3111\n",
            "Epoch 18/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.3102 - val_mse: 0.3102\n",
            "Epoch 19/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.3065 - val_mse: 0.3065\n",
            "Epoch 20/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.4169 - val_mse: 0.4169\n",
            "Epoch 21/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.2716 - val_mse: 0.2716\n",
            "Epoch 22/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.3482 - val_mse: 0.3482\n",
            "Epoch 23/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.3526 - val_mse: 0.3526\n",
            "Epoch 24/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.2745 - val_mse: 0.2745\n",
            "Epoch 25/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.2540 - val_mse: 0.2540\n",
            "Epoch 26/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.2769 - val_mse: 0.2769\n",
            "Epoch 27/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.2488 - val_mse: 0.2488\n",
            "Epoch 28/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.2227 - val_mse: 0.2227\n",
            "Epoch 29/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.2621 - val_mse: 0.2621\n",
            "Epoch 30/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.2325 - val_mse: 0.2325\n",
            "Epoch 31/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.2756 - val_mse: 0.2756\n",
            "Epoch 32/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.2725 - val_mse: 0.2725\n",
            "Epoch 33/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.2884 - val_mse: 0.2884\n",
            "Epoch 34/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.2718 - val_mse: 0.2718\n",
            "Epoch 35/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.2778 - val_mse: 0.2778\n",
            "Epoch 36/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.2527 - val_mse: 0.2527\n",
            "Epoch 37/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.2869 - val_mse: 0.2869\n",
            "Epoch 38/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.2254 - val_mse: 0.2254\n",
            "Epoch 39/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.2332 - val_mse: 0.2332\n",
            "Epoch 40/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.3566 - val_mse: 0.3566\n",
            "Epoch 41/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.2823 - val_mse: 0.2823\n",
            "Epoch 42/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.4028 - val_mse: 0.4028\n",
            "Epoch 43/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.2807 - val_mse: 0.2807\n",
            "Epoch 44/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.3010 - val_mse: 0.3010\n",
            "Epoch 45/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.3304 - val_mse: 0.3304\n",
            "Epoch 46/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.3522 - val_mse: 0.3522\n",
            "Epoch 47/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.3107 - val_mse: 0.3107\n",
            "Epoch 48/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.2823 - val_mse: 0.2823\n",
            "Epoch 49/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.3968 - val_mse: 0.3968\n",
            "Epoch 50/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.2925 - val_mse: 0.2925\n",
            "Infering 252 values\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Epoch 1/50\n",
            "200/200 [==============================] - 8s 25ms/step - loss: 0.1770 - mse: 0.1770 - val_loss: 2.4514 - val_mse: 2.4514\n",
            "Epoch 2/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0033 - mse: 0.0033 - val_loss: 1.7911 - val_mse: 1.7911\n",
            "Epoch 3/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 1.5529 - val_mse: 1.5529\n",
            "Epoch 4/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 1.4024 - val_mse: 1.4024\n",
            "Epoch 5/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 1.2221 - val_mse: 1.2221\n",
            "Epoch 6/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 1.0464 - val_mse: 1.0464\n",
            "Epoch 7/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.9184 - val_mse: 0.9184\n",
            "Epoch 8/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.8730 - val_mse: 0.8730\n",
            "Epoch 9/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.7722 - val_mse: 0.7722\n",
            "Epoch 10/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.6661 - val_mse: 0.6661\n",
            "Epoch 11/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.6198 - val_mse: 0.6198\n",
            "Epoch 12/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.5354 - val_mse: 0.5354\n",
            "Epoch 13/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.5142 - val_mse: 0.5142\n",
            "Epoch 14/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.5124 - val_mse: 0.5124\n",
            "Epoch 15/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.4248 - val_mse: 0.4248\n",
            "Epoch 16/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.4597 - val_mse: 0.4597\n",
            "Epoch 17/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.4205 - val_mse: 0.4205\n",
            "Epoch 18/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.3980 - val_mse: 0.3980\n",
            "Epoch 19/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.3960 - val_mse: 0.3960\n",
            "Epoch 20/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.3886 - val_mse: 0.3886\n",
            "Epoch 21/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.3620 - val_mse: 0.3620\n",
            "Epoch 22/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.4000 - val_mse: 0.4000\n",
            "Epoch 23/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.3829 - val_mse: 0.3829\n",
            "Epoch 24/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.3734 - val_mse: 0.3734\n",
            "Epoch 25/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.3095 - val_mse: 0.3095\n",
            "Epoch 26/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.3633 - val_mse: 0.3633\n",
            "Epoch 27/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.3129 - val_mse: 0.3129\n",
            "Epoch 28/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.3052 - val_mse: 0.3052\n",
            "Epoch 29/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.2973 - val_mse: 0.2973\n",
            "Epoch 30/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.2789 - val_mse: 0.2789\n",
            "Epoch 31/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.3254 - val_mse: 0.3254\n",
            "Epoch 32/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.3410 - val_mse: 0.3410\n",
            "Epoch 33/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.3008 - val_mse: 0.3008\n",
            "Epoch 34/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.2950 - val_mse: 0.2950\n",
            "Epoch 35/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.3464 - val_mse: 0.3464\n",
            "Epoch 36/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.3180 - val_mse: 0.3180\n",
            "Epoch 37/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.2929 - val_mse: 0.2929\n",
            "Epoch 38/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.2872 - val_mse: 0.2872\n",
            "Epoch 39/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.3100 - val_mse: 0.3100\n",
            "Epoch 40/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.3821 - val_mse: 0.3821\n",
            "Epoch 41/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.3434 - val_mse: 0.3434\n",
            "Epoch 42/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.3913 - val_mse: 0.3913\n",
            "Epoch 43/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.3861 - val_mse: 0.3861\n",
            "Epoch 44/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.3442 - val_mse: 0.3442\n",
            "Epoch 45/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.4368 - val_mse: 0.4368\n",
            "Epoch 46/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.3898 - val_mse: 0.3898\n",
            "Epoch 47/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.3657 - val_mse: 0.3657\n",
            "Epoch 48/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.4377 - val_mse: 0.4377\n",
            "Epoch 49/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.4640 - val_mse: 0.4640\n",
            "Epoch 50/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.3549 - val_mse: 0.3549\n",
            "Infering 252 values\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Epoch 1/50\n",
            "200/200 [==============================] - 8s 25ms/step - loss: 0.1888 - mse: 0.1888 - val_loss: 8.2542 - val_mse: 8.2542\n",
            "Epoch 2/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0046 - mse: 0.0046 - val_loss: 5.9638 - val_mse: 5.9638\n",
            "Epoch 3/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 5.6304 - val_mse: 5.6304\n",
            "Epoch 4/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0033 - mse: 0.0033 - val_loss: 5.2400 - val_mse: 5.2400\n",
            "Epoch 5/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 5.0753 - val_mse: 5.0753\n",
            "Epoch 6/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 4.8308 - val_mse: 4.8308\n",
            "Epoch 7/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0028 - mse: 0.0028 - val_loss: 4.4684 - val_mse: 4.4684\n",
            "Epoch 8/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0027 - mse: 0.0027 - val_loss: 4.3029 - val_mse: 4.3029\n",
            "Epoch 9/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 4.0433 - val_mse: 4.0433\n",
            "Epoch 10/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 3.8457 - val_mse: 3.8457\n",
            "Epoch 11/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 3.6795 - val_mse: 3.6795\n",
            "Epoch 12/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 3.4072 - val_mse: 3.4072\n",
            "Epoch 13/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 3.3179 - val_mse: 3.3179\n",
            "Epoch 14/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 3.1436 - val_mse: 3.1436\n",
            "Epoch 15/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 2.9074 - val_mse: 2.9074\n",
            "Epoch 16/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 2.9692 - val_mse: 2.9692\n",
            "Epoch 17/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 2.8326 - val_mse: 2.8326\n",
            "Epoch 18/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 2.7963 - val_mse: 2.7963\n",
            "Epoch 19/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 2.8341 - val_mse: 2.8341\n",
            "Epoch 20/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 2.5937 - val_mse: 2.5937\n",
            "Epoch 21/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 2.5734 - val_mse: 2.5734\n",
            "Epoch 22/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 2.6921 - val_mse: 2.6921\n",
            "Epoch 23/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 2.5359 - val_mse: 2.5359\n",
            "Epoch 24/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 2.4914 - val_mse: 2.4914\n",
            "Epoch 25/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 2.6232 - val_mse: 2.6232\n",
            "Epoch 26/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 2.5838 - val_mse: 2.5838\n",
            "Epoch 27/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 2.5724 - val_mse: 2.5724\n",
            "Epoch 28/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 2.4345 - val_mse: 2.4345\n",
            "Epoch 29/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 2.5423 - val_mse: 2.5423\n",
            "Epoch 30/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 2.4816 - val_mse: 2.4816\n",
            "Epoch 31/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 2.5905 - val_mse: 2.5905\n",
            "Epoch 32/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 2.5406 - val_mse: 2.5406\n",
            "Epoch 33/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 2.5320 - val_mse: 2.5320\n",
            "Epoch 34/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 2.3994 - val_mse: 2.3994\n",
            "Epoch 35/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 2.4239 - val_mse: 2.4239\n",
            "Epoch 36/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 2.5919 - val_mse: 2.5919\n",
            "Epoch 37/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 2.5707 - val_mse: 2.5707\n",
            "Epoch 38/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 2.6777 - val_mse: 2.6777\n",
            "Epoch 39/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 2.7075 - val_mse: 2.7075\n",
            "Epoch 40/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 2.9609 - val_mse: 2.9609\n",
            "Epoch 41/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 2.6613 - val_mse: 2.6613\n",
            "Epoch 42/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 2.8001 - val_mse: 2.8001\n",
            "Epoch 43/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 2.6323 - val_mse: 2.6323\n",
            "Epoch 44/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 2.9379 - val_mse: 2.9379\n",
            "Epoch 45/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 2.7835 - val_mse: 2.7835\n",
            "Epoch 46/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 2.9690 - val_mse: 2.9690\n",
            "Epoch 47/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 2.9534 - val_mse: 2.9534\n",
            "Epoch 48/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 2.9445 - val_mse: 2.9445\n",
            "Epoch 49/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 3.0303 - val_mse: 3.0303\n",
            "Epoch 50/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 2.9732 - val_mse: 2.9732\n",
            "Infering 252 values\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Epoch 1/50\n",
            "200/200 [==============================] - 8s 26ms/step - loss: 0.1643 - mse: 0.1643 - val_loss: 7.2104 - val_mse: 7.2104\n",
            "Epoch 2/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 5.7091 - val_mse: 5.7091\n",
            "Epoch 3/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 4.7606 - val_mse: 4.7606\n",
            "Epoch 4/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 4.1295 - val_mse: 4.1295\n",
            "Epoch 5/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0028 - mse: 0.0028 - val_loss: 3.6175 - val_mse: 3.6175\n",
            "Epoch 6/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 3.1319 - val_mse: 3.1319\n",
            "Epoch 7/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 2.7506 - val_mse: 2.7506\n",
            "Epoch 8/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 2.5511 - val_mse: 2.5511\n",
            "Epoch 9/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 2.3265 - val_mse: 2.3265\n",
            "Epoch 10/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 2.0962 - val_mse: 2.0962\n",
            "Epoch 11/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 1.9765 - val_mse: 1.9765\n",
            "Epoch 12/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 1.8284 - val_mse: 1.8284\n",
            "Epoch 13/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 1.7438 - val_mse: 1.7438\n",
            "Epoch 14/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 1.7196 - val_mse: 1.7196\n",
            "Epoch 15/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 1.5739 - val_mse: 1.5739\n",
            "Epoch 16/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 1.5590 - val_mse: 1.5590\n",
            "Epoch 17/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 1.4939 - val_mse: 1.4939\n",
            "Epoch 18/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 1.5485 - val_mse: 1.5485\n",
            "Epoch 19/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 1.4560 - val_mse: 1.4560\n",
            "Epoch 20/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 1.4213 - val_mse: 1.4213\n",
            "Epoch 21/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 1.3364 - val_mse: 1.3364\n",
            "Epoch 22/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 1.3991 - val_mse: 1.3991\n",
            "Epoch 23/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 1.3114 - val_mse: 1.3114\n",
            "Epoch 24/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 1.2915 - val_mse: 1.2915\n",
            "Epoch 25/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 1.2268 - val_mse: 1.2268\n",
            "Epoch 26/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 1.2413 - val_mse: 1.2413\n",
            "Epoch 27/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 1.2298 - val_mse: 1.2298\n",
            "Epoch 28/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 1.1400 - val_mse: 1.1400\n",
            "Epoch 29/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 1.1766 - val_mse: 1.1766\n",
            "Epoch 30/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 1.1780 - val_mse: 1.1780\n",
            "Epoch 31/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 1.2163 - val_mse: 1.2163\n",
            "Epoch 32/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 1.1471 - val_mse: 1.1471\n",
            "Epoch 33/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 1.1642 - val_mse: 1.1642\n",
            "Epoch 34/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 1.1170 - val_mse: 1.1170\n",
            "Epoch 35/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 1.1282 - val_mse: 1.1282\n",
            "Epoch 36/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 1.1051 - val_mse: 1.1051\n",
            "Epoch 37/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 1.0667 - val_mse: 1.0667\n",
            "Epoch 38/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 1.0581 - val_mse: 1.0581\n",
            "Epoch 39/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 1.0899 - val_mse: 1.0899\n",
            "Epoch 40/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 1.1338 - val_mse: 1.1338\n",
            "Epoch 41/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 1.1156 - val_mse: 1.1156\n",
            "Epoch 42/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 1.1936 - val_mse: 1.1936\n",
            "Epoch 43/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 1.1165 - val_mse: 1.1165\n",
            "Epoch 44/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 1.1066 - val_mse: 1.1066\n",
            "Epoch 45/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 1.1569 - val_mse: 1.1569\n",
            "Epoch 46/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 1.1964 - val_mse: 1.1964\n",
            "Epoch 47/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 1.1360 - val_mse: 1.1360\n",
            "Epoch 48/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 1.1846 - val_mse: 1.1846\n",
            "Epoch 49/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 1.2065 - val_mse: 1.2065\n",
            "Epoch 50/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 1.1530 - val_mse: 1.1530\n",
            "Infering 252 values\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Epoch 1/50\n",
            "200/200 [==============================] - 8s 25ms/step - loss: 0.1595 - mse: 0.1595 - val_loss: 0.6663 - val_mse: 0.6663\n",
            "Epoch 2/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0056 - mse: 0.0056 - val_loss: 0.3152 - val_mse: 0.3152\n",
            "Epoch 3/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0040 - mse: 0.0040 - val_loss: 0.1532 - val_mse: 0.1532\n",
            "Epoch 4/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.1196 - val_mse: 0.1196\n",
            "Epoch 5/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0031 - mse: 0.0031 - val_loss: 0.0725 - val_mse: 0.0725\n",
            "Epoch 6/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0029 - mse: 0.0029 - val_loss: 0.0956 - val_mse: 0.0956\n",
            "Epoch 7/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0028 - mse: 0.0028 - val_loss: 0.0610 - val_mse: 0.0610\n",
            "Epoch 8/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0026 - mse: 0.0026 - val_loss: 0.0677 - val_mse: 0.0677\n",
            "Epoch 9/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 0.0746 - val_mse: 0.0746\n",
            "Epoch 10/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 0.0558 - val_mse: 0.0558\n",
            "Epoch 11/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 0.0597 - val_mse: 0.0597\n",
            "Epoch 12/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0487 - val_mse: 0.0487\n",
            "Epoch 13/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 0.0642 - val_mse: 0.0642\n",
            "Epoch 14/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.0560 - val_mse: 0.0560\n",
            "Epoch 15/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.0430 - val_mse: 0.0430\n",
            "Epoch 16/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.0484 - val_mse: 0.0484\n",
            "Epoch 17/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.0367 - val_mse: 0.0367\n",
            "Epoch 18/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.0310 - val_mse: 0.0310\n",
            "Epoch 19/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0389 - val_mse: 0.0389\n",
            "Epoch 20/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0323 - val_mse: 0.0323\n",
            "Epoch 21/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0357 - val_mse: 0.0357\n",
            "Epoch 22/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0361 - val_mse: 0.0361\n",
            "Epoch 23/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0330 - val_mse: 0.0330\n",
            "Epoch 24/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0337 - val_mse: 0.0337\n",
            "Epoch 25/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0304 - val_mse: 0.0304\n",
            "Epoch 26/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0313 - val_mse: 0.0313\n",
            "Epoch 27/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0255 - val_mse: 0.0255\n",
            "Epoch 28/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0308 - val_mse: 0.0308\n",
            "Epoch 29/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.0248 - val_mse: 0.0248\n",
            "Epoch 30/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0275 - val_mse: 0.0275\n",
            "Epoch 31/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0257 - val_mse: 0.0257\n",
            "Epoch 32/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0268 - val_mse: 0.0268\n",
            "Epoch 33/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0216 - val_mse: 0.0216\n",
            "Epoch 34/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0303 - val_mse: 0.0303\n",
            "Epoch 35/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0214 - val_mse: 0.0214\n",
            "Epoch 36/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0264 - val_mse: 0.0264\n",
            "Epoch 37/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0245 - val_mse: 0.0245\n",
            "Epoch 38/50\n",
            "200/200 [==============================] - 5s 23ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0219 - val_mse: 0.0219\n",
            "Epoch 39/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0264 - val_mse: 0.0264\n",
            "Epoch 40/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0289 - val_mse: 0.0289\n",
            "Epoch 41/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0178 - val_mse: 0.0178\n",
            "Epoch 42/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0226 - val_mse: 0.0226\n",
            "Epoch 43/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0188 - val_mse: 0.0188\n",
            "Epoch 44/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0184 - val_mse: 0.0184\n",
            "Epoch 45/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0180 - val_mse: 0.0180\n",
            "Epoch 46/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0247 - val_mse: 0.0247\n",
            "Epoch 47/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0258 - val_mse: 0.0258\n",
            "Epoch 48/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0192 - val_mse: 0.0192\n",
            "Epoch 49/50\n",
            "200/200 [==============================] - 5s 23ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0230 - val_mse: 0.0230\n",
            "Epoch 50/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0163 - val_mse: 0.0163\n",
            "Infering 252 values\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Epoch 1/50\n",
            "200/200 [==============================] - 8s 25ms/step - loss: 0.1586 - mse: 0.1586 - val_loss: 0.0116 - val_mse: 0.0116\n",
            "Epoch 2/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0079 - val_mse: 0.0079\n",
            "Epoch 3/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0084 - mse: 0.0084 - val_loss: 0.0067 - val_mse: 0.0067\n",
            "Epoch 4/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0069 - mse: 0.0069 - val_loss: 0.0062 - val_mse: 0.0062\n",
            "Epoch 5/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0062 - mse: 0.0062 - val_loss: 0.0057 - val_mse: 0.0057\n",
            "Epoch 6/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0058 - mse: 0.0058 - val_loss: 0.0052 - val_mse: 0.0052\n",
            "Epoch 7/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0055 - mse: 0.0055 - val_loss: 0.0049 - val_mse: 0.0049\n",
            "Epoch 8/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0050 - mse: 0.0050 - val_loss: 0.0047 - val_mse: 0.0047\n",
            "Epoch 9/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0051 - mse: 0.0051 - val_loss: 0.0051 - val_mse: 0.0051\n",
            "Epoch 10/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0049 - mse: 0.0049 - val_loss: 0.0046 - val_mse: 0.0046\n",
            "Epoch 11/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0048 - mse: 0.0048 - val_loss: 0.0043 - val_mse: 0.0043\n",
            "Epoch 12/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0046 - mse: 0.0046 - val_loss: 0.0044 - val_mse: 0.0044\n",
            "Epoch 13/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0047 - mse: 0.0047 - val_loss: 0.0041 - val_mse: 0.0041\n",
            "Epoch 14/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0046 - mse: 0.0046 - val_loss: 0.0041 - val_mse: 0.0041\n",
            "Epoch 15/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0046 - mse: 0.0046 - val_loss: 0.0042 - val_mse: 0.0042\n",
            "Epoch 16/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0046 - mse: 0.0046 - val_loss: 0.0042 - val_mse: 0.0042\n",
            "Epoch 17/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0046 - mse: 0.0046 - val_loss: 0.0039 - val_mse: 0.0039\n",
            "Epoch 18/50\n",
            "200/200 [==============================] - 5s 23ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0039 - val_mse: 0.0039\n",
            "Epoch 19/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0042 - val_mse: 0.0042\n",
            "Epoch 20/50\n",
            "200/200 [==============================] - 5s 23ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0039 - val_mse: 0.0039\n",
            "Epoch 21/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0050 - val_mse: 0.0050\n",
            "Epoch 22/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0038 - val_mse: 0.0038\n",
            "Epoch 23/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0041 - val_mse: 0.0041\n",
            "Epoch 24/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0046 - mse: 0.0046 - val_loss: 0.0041 - val_mse: 0.0041\n",
            "Epoch 25/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0042 - mse: 0.0042 - val_loss: 0.0043 - val_mse: 0.0043\n",
            "Epoch 26/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0047 - val_mse: 0.0047\n",
            "Epoch 27/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0039 - val_mse: 0.0039\n",
            "Epoch 28/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0039 - val_mse: 0.0039\n",
            "Epoch 29/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0038 - val_mse: 0.0038\n",
            "Epoch 30/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0047 - val_mse: 0.0047\n",
            "Epoch 31/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0038 - val_mse: 0.0038\n",
            "Epoch 32/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0043 - mse: 0.0043 - val_loss: 0.0038 - val_mse: 0.0038\n",
            "Epoch 33/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0042 - val_mse: 0.0042\n",
            "Epoch 34/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0039 - val_mse: 0.0039\n",
            "Epoch 35/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0043 - mse: 0.0043 - val_loss: 0.0041 - val_mse: 0.0041\n",
            "Epoch 36/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0037 - val_mse: 0.0037\n",
            "Epoch 37/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0041 - val_mse: 0.0041\n",
            "Epoch 38/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0042 - mse: 0.0042 - val_loss: 0.0037 - val_mse: 0.0037\n",
            "Epoch 39/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0039 - val_mse: 0.0039\n",
            "Epoch 40/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0038 - val_mse: 0.0038\n",
            "Epoch 41/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0039 - val_mse: 0.0039\n",
            "Epoch 42/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0041 - mse: 0.0041 - val_loss: 0.0039 - val_mse: 0.0039\n",
            "Epoch 43/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0037 - val_mse: 0.0037\n",
            "Epoch 44/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0037 - val_mse: 0.0037\n",
            "Epoch 45/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0043 - mse: 0.0043 - val_loss: 0.0043 - val_mse: 0.0043\n",
            "Epoch 46/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0042 - mse: 0.0042 - val_loss: 0.0038 - val_mse: 0.0038\n",
            "Epoch 47/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0042 - val_mse: 0.0042\n",
            "Epoch 48/50\n",
            "200/200 [==============================] - 5s 22ms/step - loss: 0.0043 - mse: 0.0043 - val_loss: 0.0039 - val_mse: 0.0039\n",
            "Epoch 49/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0039 - val_mse: 0.0039\n",
            "Epoch 50/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0042 - mse: 0.0042 - val_loss: 0.0038 - val_mse: 0.0038\n",
            "Infering 252 values\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Epoch 1/50\n",
            "200/200 [==============================] - 8s 26ms/step - loss: 0.1705 - mse: 0.1705 - val_loss: 2.1986 - val_mse: 2.1986\n",
            "Epoch 2/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0073 - mse: 0.0073 - val_loss: 1.4334 - val_mse: 1.4334\n",
            "Epoch 3/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0054 - mse: 0.0054 - val_loss: 1.0945 - val_mse: 1.0945\n",
            "Epoch 4/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0047 - mse: 0.0047 - val_loss: 0.8624 - val_mse: 0.8624\n",
            "Epoch 5/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0043 - mse: 0.0043 - val_loss: 0.6969 - val_mse: 0.6969\n",
            "Epoch 6/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0038 - mse: 0.0038 - val_loss: 0.5932 - val_mse: 0.5932\n",
            "Epoch 7/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0038 - mse: 0.0038 - val_loss: 0.4996 - val_mse: 0.4996\n",
            "Epoch 8/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0033 - mse: 0.0033 - val_loss: 0.4579 - val_mse: 0.4579\n",
            "Epoch 9/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0033 - mse: 0.0033 - val_loss: 0.4281 - val_mse: 0.4281\n",
            "Epoch 10/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0031 - mse: 0.0031 - val_loss: 0.3954 - val_mse: 0.3954\n",
            "Epoch 11/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0031 - mse: 0.0031 - val_loss: 0.3601 - val_mse: 0.3601\n",
            "Epoch 12/50\n",
            "200/200 [==============================] - 5s 23ms/step - loss: 0.0029 - mse: 0.0029 - val_loss: 0.3291 - val_mse: 0.3291\n",
            "Epoch 13/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0029 - mse: 0.0029 - val_loss: 0.3205 - val_mse: 0.3205\n",
            "Epoch 14/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 0.3203 - val_mse: 0.3203\n",
            "Epoch 15/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0027 - mse: 0.0027 - val_loss: 0.2619 - val_mse: 0.2619\n",
            "Epoch 16/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0028 - mse: 0.0028 - val_loss: 0.2915 - val_mse: 0.2915\n",
            "Epoch 17/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0027 - mse: 0.0027 - val_loss: 0.2737 - val_mse: 0.2737\n",
            "Epoch 18/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0026 - mse: 0.0026 - val_loss: 0.2548 - val_mse: 0.2548\n",
            "Epoch 19/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0026 - mse: 0.0026 - val_loss: 0.2792 - val_mse: 0.2792\n",
            "Epoch 20/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 0.2554 - val_mse: 0.2554\n",
            "Epoch 21/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0026 - mse: 0.0026 - val_loss: 0.2423 - val_mse: 0.2423\n",
            "Epoch 22/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0027 - mse: 0.0027 - val_loss: 0.2656 - val_mse: 0.2656\n",
            "Epoch 23/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0027 - mse: 0.0027 - val_loss: 0.2422 - val_mse: 0.2422\n",
            "Epoch 24/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0027 - mse: 0.0027 - val_loss: 0.2407 - val_mse: 0.2407\n",
            "Epoch 25/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 0.2266 - val_mse: 0.2266\n",
            "Epoch 26/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0026 - mse: 0.0026 - val_loss: 0.2398 - val_mse: 0.2398\n",
            "Epoch 27/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 0.2176 - val_mse: 0.2176\n",
            "Epoch 28/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 0.2248 - val_mse: 0.2248\n",
            "Epoch 29/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 0.2268 - val_mse: 0.2268\n",
            "Epoch 30/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 0.1789 - val_mse: 0.1789\n",
            "Epoch 31/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 0.2127 - val_mse: 0.2127\n",
            "Epoch 32/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 0.2163 - val_mse: 0.2163\n",
            "Epoch 33/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 0.2169 - val_mse: 0.2169\n",
            "Epoch 34/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 0.1735 - val_mse: 0.1735\n",
            "Epoch 35/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 0.2121 - val_mse: 0.2121\n",
            "Epoch 36/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0026 - mse: 0.0026 - val_loss: 0.2172 - val_mse: 0.2172\n",
            "Epoch 37/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 0.2223 - val_mse: 0.2223\n",
            "Epoch 38/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 0.1858 - val_mse: 0.1858\n",
            "Epoch 39/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 0.1964 - val_mse: 0.1964\n",
            "Epoch 40/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 0.2057 - val_mse: 0.2057\n",
            "Epoch 41/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 0.2112 - val_mse: 0.2112\n",
            "Epoch 42/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 0.2197 - val_mse: 0.2197\n",
            "Epoch 43/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 0.1984 - val_mse: 0.1984\n",
            "Epoch 44/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 0.1840 - val_mse: 0.1840\n",
            "Epoch 45/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 0.2065 - val_mse: 0.2065\n",
            "Epoch 46/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 0.2089 - val_mse: 0.2089\n",
            "Epoch 47/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 0.2011 - val_mse: 0.2011\n",
            "Epoch 48/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 0.1866 - val_mse: 0.1866\n",
            "Epoch 49/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 0.1887 - val_mse: 0.1887\n",
            "Epoch 50/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 0.1890 - val_mse: 0.1890\n",
            "Infering 252 values\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Epoch 1/50\n",
            "200/200 [==============================] - 8s 26ms/step - loss: 0.1666 - mse: 0.1666 - val_loss: 0.1107 - val_mse: 0.1107\n",
            "Epoch 2/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0043 - mse: 0.0043 - val_loss: 0.0481 - val_mse: 0.0481\n",
            "Epoch 3/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0036 - mse: 0.0036 - val_loss: 0.0404 - val_mse: 0.0404\n",
            "Epoch 4/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0031 - mse: 0.0031 - val_loss: 0.0256 - val_mse: 0.0256\n",
            "Epoch 5/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0029 - mse: 0.0029 - val_loss: 0.0220 - val_mse: 0.0220\n",
            "Epoch 6/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0027 - mse: 0.0027 - val_loss: 0.0154 - val_mse: 0.0154\n",
            "Epoch 7/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0026 - mse: 0.0026 - val_loss: 0.0114 - val_mse: 0.0114\n",
            "Epoch 8/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 0.0122 - val_mse: 0.0122\n",
            "Epoch 9/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 0.0148 - val_mse: 0.0148\n",
            "Epoch 10/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0143 - val_mse: 0.0143\n",
            "Epoch 11/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0071 - val_mse: 0.0071\n",
            "Epoch 12/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0102 - val_mse: 0.0102\n",
            "Epoch 13/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0063 - val_mse: 0.0063\n",
            "Epoch 14/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0161 - val_mse: 0.0161\n",
            "Epoch 15/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0056 - val_mse: 0.0056\n",
            "Epoch 16/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0087 - val_mse: 0.0087\n",
            "Epoch 17/50\n",
            "200/200 [==============================] - 5s 23ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0063 - val_mse: 0.0063\n",
            "Epoch 18/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0079 - val_mse: 0.0079\n",
            "Epoch 19/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0059 - val_mse: 0.0059\n",
            "Epoch 20/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0065 - val_mse: 0.0065\n",
            "Epoch 21/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0097 - val_mse: 0.0097\n",
            "Epoch 22/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0127 - val_mse: 0.0127\n",
            "Epoch 23/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0089 - val_mse: 0.0089\n",
            "Epoch 24/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0054 - val_mse: 0.0054\n",
            "Epoch 25/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0047 - val_mse: 0.0047\n",
            "Epoch 26/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0066 - val_mse: 0.0066\n",
            "Epoch 27/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0048 - val_mse: 0.0048\n",
            "Epoch 28/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0059 - val_mse: 0.0059\n",
            "Epoch 29/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0078 - val_mse: 0.0078\n",
            "Epoch 30/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0045 - val_mse: 0.0045\n",
            "Epoch 31/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0069 - val_mse: 0.0069\n",
            "Epoch 32/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0100 - val_mse: 0.0100\n",
            "Epoch 33/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0049 - val_mse: 0.0049\n",
            "Epoch 34/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0045 - val_mse: 0.0045\n",
            "Epoch 35/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0052 - val_mse: 0.0052\n",
            "Epoch 36/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0075 - val_mse: 0.0075\n",
            "Epoch 37/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0047 - val_mse: 0.0047\n",
            "Epoch 38/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0046 - val_mse: 0.0046\n",
            "Epoch 39/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0060 - val_mse: 0.0060\n",
            "Epoch 40/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0079 - val_mse: 0.0079\n",
            "Epoch 41/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0087 - val_mse: 0.0087\n",
            "Epoch 42/50\n",
            "200/200 [==============================] - 5s 23ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0059 - val_mse: 0.0059\n",
            "Epoch 43/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0057 - val_mse: 0.0057\n",
            "Epoch 44/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0061 - val_mse: 0.0061\n",
            "Epoch 45/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0044 - val_mse: 0.0044\n",
            "Epoch 46/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0054 - val_mse: 0.0054\n",
            "Epoch 47/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0078 - val_mse: 0.0078\n",
            "Epoch 48/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0056 - val_mse: 0.0056\n",
            "Epoch 49/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0070 - val_mse: 0.0070\n",
            "Epoch 50/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0043 - val_mse: 0.0043\n",
            "Infering 252 values\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Epoch 1/50\n",
            "200/200 [==============================] - 8s 26ms/step - loss: 0.1685 - mse: 0.1685 - val_loss: 0.2973 - val_mse: 0.2973\n",
            "Epoch 2/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0057 - mse: 0.0057 - val_loss: 0.1646 - val_mse: 0.1646\n",
            "Epoch 3/50\n",
            "200/200 [==============================] - 5s 23ms/step - loss: 0.0048 - mse: 0.0048 - val_loss: 0.1129 - val_mse: 0.1129\n",
            "Epoch 4/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0042 - mse: 0.0042 - val_loss: 0.0723 - val_mse: 0.0723\n",
            "Epoch 5/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0038 - mse: 0.0038 - val_loss: 0.0504 - val_mse: 0.0504\n",
            "Epoch 6/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0036 - mse: 0.0036 - val_loss: 0.0372 - val_mse: 0.0372\n",
            "Epoch 7/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0357 - val_mse: 0.0357\n",
            "Epoch 8/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0029 - mse: 0.0029 - val_loss: 0.0426 - val_mse: 0.0426\n",
            "Epoch 9/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 0.0413 - val_mse: 0.0413\n",
            "Epoch 10/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0026 - mse: 0.0026 - val_loss: 0.0374 - val_mse: 0.0374\n",
            "Epoch 11/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0028 - mse: 0.0028 - val_loss: 0.0261 - val_mse: 0.0261\n",
            "Epoch 12/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 0.0294 - val_mse: 0.0294\n",
            "Epoch 13/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0027 - mse: 0.0027 - val_loss: 0.0174 - val_mse: 0.0174\n",
            "Epoch 14/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0026 - mse: 0.0026 - val_loss: 0.0392 - val_mse: 0.0392\n",
            "Epoch 15/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 0.0128 - val_mse: 0.0128\n",
            "Epoch 16/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0026 - mse: 0.0026 - val_loss: 0.0306 - val_mse: 0.0306\n",
            "Epoch 17/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 0.0201 - val_mse: 0.0201\n",
            "Epoch 18/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 0.0246 - val_mse: 0.0246\n",
            "Epoch 19/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 0.0267 - val_mse: 0.0267\n",
            "Epoch 20/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0211 - val_mse: 0.0211\n",
            "Epoch 21/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 0.0303 - val_mse: 0.0303\n",
            "Epoch 22/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 0.0325 - val_mse: 0.0325\n",
            "Epoch 23/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 0.0284 - val_mse: 0.0284\n",
            "Epoch 24/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 0.0201 - val_mse: 0.0201\n",
            "Epoch 25/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0138 - val_mse: 0.0138\n",
            "Epoch 26/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 0.0209 - val_mse: 0.0209\n",
            "Epoch 27/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0196 - val_mse: 0.0196\n",
            "Epoch 28/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0250 - val_mse: 0.0250\n",
            "Epoch 29/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 0.0292 - val_mse: 0.0292\n",
            "Epoch 30/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0186 - val_mse: 0.0186\n",
            "Epoch 31/50\n",
            "200/200 [==============================] - 5s 23ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 0.0256 - val_mse: 0.0256\n",
            "Epoch 32/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0352 - val_mse: 0.0352\n",
            "Epoch 33/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0220 - val_mse: 0.0220\n",
            "Epoch 34/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 0.0201 - val_mse: 0.0201\n",
            "Epoch 35/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.0194 - val_mse: 0.0194\n",
            "Epoch 36/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 0.0277 - val_mse: 0.0277\n",
            "Epoch 37/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 0.0222 - val_mse: 0.0222\n",
            "Epoch 38/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0248 - val_mse: 0.0248\n",
            "Epoch 39/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0261 - val_mse: 0.0261\n",
            "Epoch 40/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0277 - val_mse: 0.0277\n",
            "Epoch 41/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 0.0355 - val_mse: 0.0355\n",
            "Epoch 42/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.0327 - val_mse: 0.0327\n",
            "Epoch 43/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0338 - val_mse: 0.0338\n",
            "Epoch 44/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0312 - val_mse: 0.0312\n",
            "Epoch 45/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.0296 - val_mse: 0.0296\n",
            "Epoch 46/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0464 - val_mse: 0.0464\n",
            "Epoch 47/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0378 - val_mse: 0.0378\n",
            "Epoch 48/50\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0234 - val_mse: 0.0234\n",
            "Epoch 49/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 0.0416 - val_mse: 0.0416\n",
            "Epoch 50/50\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.0368 - val_mse: 0.0368\n",
            "Infering 252 values\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(diz).sort_values(by='roi').tail(20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 677
        },
        "id": "D0T-Lc-op1_8",
        "outputId": "be796797-95a2-40ea-cc61-2eeb4faa157a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    stock       roi           gain\n",
              "21    WMT -0.009049     142.994181\n",
              "3     BHP -0.007398      60.211340\n",
              "29     VZ  0.025880      60.485872\n",
              "4     LIN  0.051107     280.340689\n",
              "8     LMT  0.052405     373.582643\n",
              "22     PG  0.066605     148.962079\n",
              "1     CVX  0.074463      91.490491\n",
              "25   MSFT  0.102404     245.318061\n",
              "13    JNJ  0.107032     174.069753\n",
              "14    PFE  0.113480      41.054023\n",
              "31    AMT  0.118116     252.772380\n",
              "32    CCI  0.153343     182.631867\n",
              "24   AAPL  0.180042     157.559225\n",
              "12    UNH  0.253683     440.606967\n",
              "20     HD  0.267831     337.255651\n",
              "10    DUK  0.274362     116.960944\n",
              "15  BRK-A  0.280069  445240.082670\n",
              "17    JPM  0.357540     173.086305\n",
              "28    DIS  0.437770     262.047916\n",
              "26    TSM  0.513219     168.678525"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b20841a0-cd34-4e46-868f-163290567e8d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>stock</th>\n",
              "      <th>roi</th>\n",
              "      <th>gain</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>WMT</td>\n",
              "      <td>-0.009049</td>\n",
              "      <td>142.994181</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>BHP</td>\n",
              "      <td>-0.007398</td>\n",
              "      <td>60.211340</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>VZ</td>\n",
              "      <td>0.025880</td>\n",
              "      <td>60.485872</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>LIN</td>\n",
              "      <td>0.051107</td>\n",
              "      <td>280.340689</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>LMT</td>\n",
              "      <td>0.052405</td>\n",
              "      <td>373.582643</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>PG</td>\n",
              "      <td>0.066605</td>\n",
              "      <td>148.962079</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>CVX</td>\n",
              "      <td>0.074463</td>\n",
              "      <td>91.490491</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>MSFT</td>\n",
              "      <td>0.102404</td>\n",
              "      <td>245.318061</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>JNJ</td>\n",
              "      <td>0.107032</td>\n",
              "      <td>174.069753</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>PFE</td>\n",
              "      <td>0.113480</td>\n",
              "      <td>41.054023</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>AMT</td>\n",
              "      <td>0.118116</td>\n",
              "      <td>252.772380</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>CCI</td>\n",
              "      <td>0.153343</td>\n",
              "      <td>182.631867</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.180042</td>\n",
              "      <td>157.559225</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>UNH</td>\n",
              "      <td>0.253683</td>\n",
              "      <td>440.606967</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>HD</td>\n",
              "      <td>0.267831</td>\n",
              "      <td>337.255651</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>DUK</td>\n",
              "      <td>0.274362</td>\n",
              "      <td>116.960944</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>BRK-A</td>\n",
              "      <td>0.280069</td>\n",
              "      <td>445240.082670</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>JPM</td>\n",
              "      <td>0.357540</td>\n",
              "      <td>173.086305</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>DIS</td>\n",
              "      <td>0.437770</td>\n",
              "      <td>262.047916</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>TSM</td>\n",
              "      <td>0.513219</td>\n",
              "      <td>168.678525</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b20841a0-cd34-4e46-868f-163290567e8d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b20841a0-cd34-4e46-868f-163290567e8d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b20841a0-cd34-4e46-868f-163290567e8d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "modello, encoder, decoder = LSTM_1.get_model()"
      ],
      "metadata": {
        "id": "cPArDzs09D5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.utils.plot_model(\n",
        "    modello,\n",
        "    #to_file=\"model.png\",\n",
        "    show_shapes=True,\n",
        "    show_dtype=False,\n",
        "    show_layer_names=False,\n",
        "    rankdir=\"TB\",\n",
        "    expand_nested=False,\n",
        "    dpi=96,\n",
        "    layer_range=None,\n",
        "    show_layer_activations=False,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        },
        "id": "xpbj_gPC86Pp",
        "outputId": "f7505fd1-34ae-457e-87bb-1a341fdaf625"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stock_list_exam = ['TSM' ,'DIS', 'JPM' ,'BRK-A', 'DUK' ,'HD', 'UNH' ,'AAPL' ,'CCI', 'AMT']"
      ],
      "metadata": {
        "id": "TsA7HgYpYB74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stock_list_exam = ['^GSPC', '^FTSE', '^GDAXI','^NSEI'] # '^NSEI',"
      ],
      "metadata": {
        "id": "CfaosTJn-AwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = '2010-01-01'\n",
        "end = '2022-01-01'"
      ],
      "metadata": {
        "id": "vyXWzz_IZBA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prelevo i dati\n",
        "td, _ = get_categorical_tickers()\n",
        "td.pop('all')\n",
        "diz = {}\n",
        "diz['stock'] = []\n",
        "diz['roi'] = []\n",
        "diz['gain'] = []\n",
        "diz['RMS'] = []\n",
        "\n",
        "\n",
        "\n",
        "for stock in stock_list_exam:\n",
        "  # scarico i dati\n",
        "  df = yf.download(stock, start, end)\n",
        "\n",
        "  # creo il modello\n",
        "  LSTM_1 = LSTM_ED_Model(tickerSymbol = stock , start = start, end = end, values=252, depth = 0, epochs=50, naive = True, verbose = True, train_test_split=0.8967)\n",
        "  LSTM_1.full_workflow_and_plot()\n",
        "\n",
        "  error = LSTM_1.RMS_error\n",
        "  \n",
        "\n",
        "\n",
        "  xt, yt, pred, pred_update = LSTM_1.get_predicted_values()\n",
        "  df_test = df[-pred.shape[0]:].copy()\n",
        "\n",
        "  pred_r = pd.Series(pred.reshape(-1), index=df_test.index)\n",
        "  df_test['pred'] = pred_r\n",
        "\n",
        "  mean_open = df[:-pred.shape[0]].Open.mean()\n",
        "  std_open = df[:-pred.shape[0]].Open.std()\n",
        "\n",
        "  open_norm = (df_test.Open - mean_open) / std_open\n",
        "  df_test['open_norm'] = open_norm\n",
        "  df_test['operation'] = np.where(df_test['pred'] > df_test['open_norm'], 1, 0 )\n",
        "  tot, stats = calculate_investment(df_test,df_test.Open.iloc[0])\n",
        "\n",
        "  roi = (tot - df_test.Open.iloc[0])/ df_test.Open.iloc[0]\n",
        "\n",
        "  diz['stock'].append(stock)\n",
        "  diz['roi'].append(roi)\n",
        "  diz['gain'].append(tot)\n",
        "  diz['RMS'].append(error)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8ERNvoCw_UH-",
        "outputId": "7a5bccdf-83ca-45e7-fe55-8bf9cd1d642e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Epoch 1/50\n",
            "200/200 [==============================] - 7s 21ms/step - loss: 0.1617 - mse: 0.1617 - val_loss: 1.7442 - val_mse: 1.7442\n",
            "Epoch 2/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0049 - mse: 0.0049 - val_loss: 1.0163 - val_mse: 1.0163\n",
            "Epoch 3/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0036 - mse: 0.0036 - val_loss: 0.6505 - val_mse: 0.6505\n",
            "Epoch 4/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 0.4712 - val_mse: 0.4712\n",
            "Epoch 5/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0027 - mse: 0.0027 - val_loss: 0.3404 - val_mse: 0.3404\n",
            "Epoch 6/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 0.2780 - val_mse: 0.2780\n",
            "Epoch 7/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 0.2098 - val_mse: 0.2098\n",
            "Epoch 8/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.2052 - val_mse: 0.2052\n",
            "Epoch 9/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.1921 - val_mse: 0.1921\n",
            "Epoch 10/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.1526 - val_mse: 0.1526\n",
            "Epoch 11/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.1522 - val_mse: 0.1522\n",
            "Epoch 12/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.1301 - val_mse: 0.1301\n",
            "Epoch 13/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.1263 - val_mse: 0.1263\n",
            "Epoch 14/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.1419 - val_mse: 0.1419\n",
            "Epoch 15/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0945 - val_mse: 0.0945\n",
            "Epoch 16/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.1196 - val_mse: 0.1196\n",
            "Epoch 17/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.1069 - val_mse: 0.1069\n",
            "Epoch 18/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.1232 - val_mse: 0.1232\n",
            "Epoch 19/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0979 - val_mse: 0.0979\n",
            "Epoch 20/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0991 - val_mse: 0.0991\n",
            "Epoch 21/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0933 - val_mse: 0.0933\n",
            "Epoch 22/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.1204 - val_mse: 0.1204\n",
            "Epoch 23/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.1039 - val_mse: 0.1039\n",
            "Epoch 24/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0917 - val_mse: 0.0917\n",
            "Epoch 25/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0852 - val_mse: 0.0852\n",
            "Epoch 26/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0987 - val_mse: 0.0987\n",
            "Epoch 27/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0796 - val_mse: 0.0796\n",
            "Epoch 28/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0616 - val_mse: 0.0616\n",
            "Epoch 29/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0793 - val_mse: 0.0793\n",
            "Epoch 30/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0739 - val_mse: 0.0739\n",
            "Epoch 31/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0841 - val_mse: 0.0841\n",
            "Epoch 32/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0682 - val_mse: 0.0682\n",
            "Epoch 33/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0741 - val_mse: 0.0741\n",
            "Epoch 34/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0823 - val_mse: 0.0823\n",
            "Epoch 35/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0771 - val_mse: 0.0771\n",
            "Epoch 36/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0683 - val_mse: 0.0683\n",
            "Epoch 37/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0667 - val_mse: 0.0667\n",
            "Epoch 38/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0717 - val_mse: 0.0717\n",
            "Epoch 39/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0681 - val_mse: 0.0681\n",
            "Epoch 40/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0742 - val_mse: 0.0742\n",
            "Epoch 41/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0678 - val_mse: 0.0678\n",
            "Epoch 42/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0911 - val_mse: 0.0911\n",
            "Epoch 43/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0693 - val_mse: 0.0693\n",
            "Epoch 44/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0625 - val_mse: 0.0625\n",
            "Epoch 45/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0747 - val_mse: 0.0747\n",
            "Epoch 46/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0778 - val_mse: 0.0778\n",
            "Epoch 47/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0660 - val_mse: 0.0660\n",
            "Epoch 48/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0695 - val_mse: 0.0695\n",
            "Epoch 49/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0738 - val_mse: 0.0738\n",
            "Epoch 50/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0557 - val_mse: 0.0557\n",
            "Infering 252 values\n",
            "The relative RMS error is 0.001438\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Epoch 1/50\n",
            "200/200 [==============================] - 7s 21ms/step - loss: 0.1767 - mse: 0.1767 - val_loss: 0.0115 - val_mse: 0.0115\n",
            "Epoch 2/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0176 - mse: 0.0176 - val_loss: 0.0087 - val_mse: 0.0087\n",
            "Epoch 3/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0133 - mse: 0.0133 - val_loss: 0.0090 - val_mse: 0.0090\n",
            "Epoch 4/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0111 - mse: 0.0111 - val_loss: 0.0076 - val_mse: 0.0076\n",
            "Epoch 5/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0065 - val_mse: 0.0065\n",
            "Epoch 6/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0067 - val_mse: 0.0067\n",
            "Epoch 7/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0091 - mse: 0.0091 - val_loss: 0.0064 - val_mse: 0.0064\n",
            "Epoch 8/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0088 - mse: 0.0088 - val_loss: 0.0066 - val_mse: 0.0066\n",
            "Epoch 9/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0086 - mse: 0.0086 - val_loss: 0.0063 - val_mse: 0.0063\n",
            "Epoch 10/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0086 - mse: 0.0086 - val_loss: 0.0063 - val_mse: 0.0063\n",
            "Epoch 11/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0086 - mse: 0.0086 - val_loss: 0.0062 - val_mse: 0.0062\n",
            "Epoch 12/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0086 - mse: 0.0086 - val_loss: 0.0064 - val_mse: 0.0064\n",
            "Epoch 13/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0081 - mse: 0.0081 - val_loss: 0.0066 - val_mse: 0.0066\n",
            "Epoch 14/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0083 - mse: 0.0083 - val_loss: 0.0063 - val_mse: 0.0063\n",
            "Epoch 15/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0083 - mse: 0.0083 - val_loss: 0.0068 - val_mse: 0.0068\n",
            "Epoch 16/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0086 - mse: 0.0086 - val_loss: 0.0074 - val_mse: 0.0074\n",
            "Epoch 17/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0082 - mse: 0.0082 - val_loss: 0.0063 - val_mse: 0.0063\n",
            "Epoch 18/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0082 - mse: 0.0082 - val_loss: 0.0065 - val_mse: 0.0065\n",
            "Epoch 19/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0083 - mse: 0.0083 - val_loss: 0.0061 - val_mse: 0.0061\n",
            "Epoch 20/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0081 - mse: 0.0081 - val_loss: 0.0065 - val_mse: 0.0065\n",
            "Epoch 21/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0083 - mse: 0.0083 - val_loss: 0.0063 - val_mse: 0.0063\n",
            "Epoch 22/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0084 - mse: 0.0084 - val_loss: 0.0063 - val_mse: 0.0063\n",
            "Epoch 23/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0081 - mse: 0.0081 - val_loss: 0.0069 - val_mse: 0.0069\n",
            "Epoch 24/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0081 - mse: 0.0081 - val_loss: 0.0065 - val_mse: 0.0065\n",
            "Epoch 25/50\n",
            "200/200 [==============================] - 4s 17ms/step - loss: 0.0083 - mse: 0.0083 - val_loss: 0.0064 - val_mse: 0.0064\n",
            "Epoch 26/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0082 - mse: 0.0082 - val_loss: 0.0062 - val_mse: 0.0062\n",
            "Epoch 27/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0081 - mse: 0.0081 - val_loss: 0.0066 - val_mse: 0.0066\n",
            "Epoch 28/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0082 - mse: 0.0082 - val_loss: 0.0068 - val_mse: 0.0068\n",
            "Epoch 29/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0083 - mse: 0.0083 - val_loss: 0.0062 - val_mse: 0.0062\n",
            "Epoch 30/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0079 - mse: 0.0079 - val_loss: 0.0064 - val_mse: 0.0064\n",
            "Epoch 31/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0083 - mse: 0.0083 - val_loss: 0.0064 - val_mse: 0.0064\n",
            "Epoch 32/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0080 - mse: 0.0080 - val_loss: 0.0065 - val_mse: 0.0065\n",
            "Epoch 33/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0081 - mse: 0.0081 - val_loss: 0.0066 - val_mse: 0.0066\n",
            "Epoch 34/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0082 - mse: 0.0082 - val_loss: 0.0066 - val_mse: 0.0066\n",
            "Epoch 35/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0081 - mse: 0.0081 - val_loss: 0.0062 - val_mse: 0.0062\n",
            "Epoch 36/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0082 - mse: 0.0082 - val_loss: 0.0065 - val_mse: 0.0065\n",
            "Epoch 37/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0081 - mse: 0.0081 - val_loss: 0.0065 - val_mse: 0.0065\n",
            "Epoch 38/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0081 - mse: 0.0081 - val_loss: 0.0065 - val_mse: 0.0065\n",
            "Epoch 39/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0084 - mse: 0.0084 - val_loss: 0.0063 - val_mse: 0.0063\n",
            "Epoch 40/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0082 - mse: 0.0082 - val_loss: 0.0065 - val_mse: 0.0065\n",
            "Epoch 41/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0080 - mse: 0.0080 - val_loss: 0.0065 - val_mse: 0.0065\n",
            "Epoch 42/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0083 - mse: 0.0083 - val_loss: 0.0065 - val_mse: 0.0065\n",
            "Epoch 43/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0078 - mse: 0.0078 - val_loss: 0.0063 - val_mse: 0.0063\n",
            "Epoch 44/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0082 - mse: 0.0082 - val_loss: 0.0064 - val_mse: 0.0064\n",
            "Epoch 45/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0079 - mse: 0.0079 - val_loss: 0.0065 - val_mse: 0.0065\n",
            "Epoch 46/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0082 - mse: 0.0082 - val_loss: 0.0066 - val_mse: 0.0066\n",
            "Epoch 47/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0078 - mse: 0.0078 - val_loss: 0.0063 - val_mse: 0.0063\n",
            "Epoch 48/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0081 - mse: 0.0081 - val_loss: 0.0064 - val_mse: 0.0064\n",
            "Epoch 49/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0079 - mse: 0.0079 - val_loss: 0.0063 - val_mse: 0.0063\n",
            "Epoch 50/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0081 - mse: 0.0081 - val_loss: 0.0063 - val_mse: 0.0063\n",
            "Infering 253 values\n",
            "The relative RMS error is 0.008058\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Epoch 1/50\n",
            "200/200 [==============================] - 7s 22ms/step - loss: 0.1574 - mse: 0.1574 - val_loss: 0.3988 - val_mse: 0.3988\n",
            "Epoch 2/50\n",
            "200/200 [==============================] - 4s 17ms/step - loss: 0.0084 - mse: 0.0084 - val_loss: 0.1459 - val_mse: 0.1459\n",
            "Epoch 3/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0063 - mse: 0.0063 - val_loss: 0.0664 - val_mse: 0.0664\n",
            "Epoch 4/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0051 - mse: 0.0051 - val_loss: 0.0620 - val_mse: 0.0620\n",
            "Epoch 5/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0043 - mse: 0.0043 - val_loss: 0.0603 - val_mse: 0.0603\n",
            "Epoch 6/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0038 - mse: 0.0038 - val_loss: 0.0448 - val_mse: 0.0448\n",
            "Epoch 7/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0037 - mse: 0.0037 - val_loss: 0.0495 - val_mse: 0.0495\n",
            "Epoch 8/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0359 - val_mse: 0.0359\n",
            "Epoch 9/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0032 - mse: 0.0032 - val_loss: 0.0344 - val_mse: 0.0344\n",
            "Epoch 10/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0032 - mse: 0.0032 - val_loss: 0.0374 - val_mse: 0.0374\n",
            "Epoch 11/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 0.0392 - val_mse: 0.0392\n",
            "Epoch 12/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 0.0404 - val_mse: 0.0404\n",
            "Epoch 13/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 0.0231 - val_mse: 0.0231\n",
            "Epoch 14/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0029 - mse: 0.0029 - val_loss: 0.0301 - val_mse: 0.0301\n",
            "Epoch 15/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0029 - mse: 0.0029 - val_loss: 0.0348 - val_mse: 0.0348\n",
            "Epoch 16/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0028 - mse: 0.0028 - val_loss: 0.0322 - val_mse: 0.0322\n",
            "Epoch 17/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0028 - mse: 0.0028 - val_loss: 0.0327 - val_mse: 0.0327\n",
            "Epoch 18/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0028 - mse: 0.0028 - val_loss: 0.0340 - val_mse: 0.0340\n",
            "Epoch 19/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0027 - mse: 0.0027 - val_loss: 0.0391 - val_mse: 0.0391\n",
            "Epoch 20/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0027 - mse: 0.0027 - val_loss: 0.0211 - val_mse: 0.0211\n",
            "Epoch 21/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0028 - mse: 0.0028 - val_loss: 0.0287 - val_mse: 0.0287\n",
            "Epoch 22/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0028 - mse: 0.0028 - val_loss: 0.0304 - val_mse: 0.0304\n",
            "Epoch 23/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0027 - mse: 0.0027 - val_loss: 0.0267 - val_mse: 0.0267\n",
            "Epoch 24/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0028 - mse: 0.0028 - val_loss: 0.0254 - val_mse: 0.0254\n",
            "Epoch 25/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0027 - mse: 0.0027 - val_loss: 0.0322 - val_mse: 0.0322\n",
            "Epoch 26/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0027 - mse: 0.0027 - val_loss: 0.0244 - val_mse: 0.0244\n",
            "Epoch 27/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0027 - mse: 0.0027 - val_loss: 0.0284 - val_mse: 0.0284\n",
            "Epoch 28/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0028 - mse: 0.0028 - val_loss: 0.0219 - val_mse: 0.0219\n",
            "Epoch 29/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0027 - mse: 0.0027 - val_loss: 0.0271 - val_mse: 0.0271\n",
            "Epoch 30/50\n",
            "200/200 [==============================] - 4s 17ms/step - loss: 0.0027 - mse: 0.0027 - val_loss: 0.0209 - val_mse: 0.0209\n",
            "Epoch 31/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0027 - mse: 0.0027 - val_loss: 0.0228 - val_mse: 0.0228\n",
            "Epoch 32/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0027 - mse: 0.0027 - val_loss: 0.0291 - val_mse: 0.0291\n",
            "Epoch 33/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0027 - mse: 0.0027 - val_loss: 0.0215 - val_mse: 0.0215\n",
            "Epoch 34/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0027 - mse: 0.0027 - val_loss: 0.0254 - val_mse: 0.0254\n",
            "Epoch 35/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0026 - mse: 0.0026 - val_loss: 0.0312 - val_mse: 0.0312\n",
            "Epoch 36/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0028 - mse: 0.0028 - val_loss: 0.0193 - val_mse: 0.0193\n",
            "Epoch 37/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0026 - mse: 0.0026 - val_loss: 0.0282 - val_mse: 0.0282\n",
            "Epoch 38/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0028 - mse: 0.0028 - val_loss: 0.0194 - val_mse: 0.0194\n",
            "Epoch 39/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0026 - mse: 0.0026 - val_loss: 0.0189 - val_mse: 0.0189\n",
            "Epoch 40/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0026 - mse: 0.0026 - val_loss: 0.0188 - val_mse: 0.0188\n",
            "Epoch 41/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0027 - mse: 0.0027 - val_loss: 0.0231 - val_mse: 0.0231\n",
            "Epoch 42/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0027 - mse: 0.0027 - val_loss: 0.0210 - val_mse: 0.0210\n",
            "Epoch 43/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0026 - mse: 0.0026 - val_loss: 0.0160 - val_mse: 0.0160\n",
            "Epoch 44/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0028 - mse: 0.0028 - val_loss: 0.0214 - val_mse: 0.0214\n",
            "Epoch 45/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0027 - mse: 0.0027 - val_loss: 0.0242 - val_mse: 0.0242\n",
            "Epoch 46/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0026 - mse: 0.0026 - val_loss: 0.0223 - val_mse: 0.0223\n",
            "Epoch 47/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0027 - mse: 0.0027 - val_loss: 0.0304 - val_mse: 0.0304\n",
            "Epoch 48/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0026 - mse: 0.0026 - val_loss: 0.0266 - val_mse: 0.0266\n",
            "Epoch 49/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0026 - mse: 0.0026 - val_loss: 0.0219 - val_mse: 0.0219\n",
            "Epoch 50/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0026 - mse: 0.0026 - val_loss: 0.0280 - val_mse: 0.0280\n",
            "Infering 254 values\n",
            "The relative RMS error is 0.002577\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Epoch 1/50\n",
            "200/200 [==============================] - 7s 21ms/step - loss: 0.1552 - mse: 0.1552 - val_loss: 2.5328 - val_mse: 2.5328\n",
            "Epoch 2/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0057 - mse: 0.0057 - val_loss: 1.6396 - val_mse: 1.6396\n",
            "Epoch 3/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 1.1680 - val_mse: 1.1680\n",
            "Epoch 4/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0036 - mse: 0.0036 - val_loss: 0.9094 - val_mse: 0.9094\n",
            "Epoch 5/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0031 - mse: 0.0031 - val_loss: 0.7168 - val_mse: 0.7168\n",
            "Epoch 6/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0027 - mse: 0.0027 - val_loss: 0.6623 - val_mse: 0.6623\n",
            "Epoch 7/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 0.5982 - val_mse: 0.5982\n",
            "Epoch 8/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 0.5287 - val_mse: 0.5287\n",
            "Epoch 9/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.4971 - val_mse: 0.4971\n",
            "Epoch 10/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.4523 - val_mse: 0.4523\n",
            "Epoch 11/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.4478 - val_mse: 0.4478\n",
            "Epoch 12/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.4225 - val_mse: 0.4225\n",
            "Epoch 13/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.4211 - val_mse: 0.4211\n",
            "Epoch 14/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.3840 - val_mse: 0.3840\n",
            "Epoch 15/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.3614 - val_mse: 0.3614\n",
            "Epoch 16/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.3625 - val_mse: 0.3625\n",
            "Epoch 17/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.3709 - val_mse: 0.3709\n",
            "Epoch 18/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.3665 - val_mse: 0.3665\n",
            "Epoch 19/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.3425 - val_mse: 0.3425\n",
            "Epoch 20/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.3216 - val_mse: 0.3216\n",
            "Epoch 21/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.3340 - val_mse: 0.3340\n",
            "Epoch 22/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.3322 - val_mse: 0.3322\n",
            "Epoch 23/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.3327 - val_mse: 0.3327\n",
            "Epoch 24/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.2986 - val_mse: 0.2986\n",
            "Epoch 25/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.3053 - val_mse: 0.3053\n",
            "Epoch 26/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.3355 - val_mse: 0.3355\n",
            "Epoch 27/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.3245 - val_mse: 0.3245\n",
            "Epoch 28/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.2980 - val_mse: 0.2980\n",
            "Epoch 29/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.2879 - val_mse: 0.2879\n",
            "Epoch 30/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.2845 - val_mse: 0.2845\n",
            "Epoch 31/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.2896 - val_mse: 0.2896\n",
            "Epoch 32/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.2754 - val_mse: 0.2754\n",
            "Epoch 33/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.2704 - val_mse: 0.2704\n",
            "Epoch 34/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.2668 - val_mse: 0.2668\n",
            "Epoch 35/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.2825 - val_mse: 0.2825\n",
            "Epoch 36/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.2742 - val_mse: 0.2742\n",
            "Epoch 37/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.2713 - val_mse: 0.2713\n",
            "Epoch 38/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.2671 - val_mse: 0.2671\n",
            "Epoch 39/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.2878 - val_mse: 0.2878\n",
            "Epoch 40/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.2497 - val_mse: 0.2497\n",
            "Epoch 41/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.2478 - val_mse: 0.2478\n",
            "Epoch 42/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.2450 - val_mse: 0.2450\n",
            "Epoch 43/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.2438 - val_mse: 0.2438\n",
            "Epoch 44/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.2589 - val_mse: 0.2589\n",
            "Epoch 45/50\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.2612 - val_mse: 0.2612\n",
            "Epoch 46/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.2578 - val_mse: 0.2578\n",
            "Epoch 47/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.2783 - val_mse: 0.2783\n",
            "Epoch 48/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.2362 - val_mse: 0.2362\n",
            "Epoch 49/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.2326 - val_mse: 0.2326\n",
            "Epoch 50/50\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.2583 - val_mse: 0.2583\n",
            "Infering 243 values\n",
            "The relative RMS error is 0.001567\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(diz)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "3JZQhrSh_fKj",
        "outputId": "367e1802-7028-4f83-b483-7aa964ea3e0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    stock       roi          gain       RMS\n",
              "0   ^GSPC  0.038739   3910.448698  0.001438\n",
              "1   ^FTSE  0.185620   7659.698716  0.008058\n",
              "2  ^GDAXI  0.133582  15508.188678  0.002577\n",
              "3   ^NSEI  0.002008  14287.035446  0.001567"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0f5baf2c-1033-49c9-8f38-ecc3978edc8f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>stock</th>\n",
              "      <th>roi</th>\n",
              "      <th>gain</th>\n",
              "      <th>RMS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>^GSPC</td>\n",
              "      <td>0.038739</td>\n",
              "      <td>3910.448698</td>\n",
              "      <td>0.001438</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>^FTSE</td>\n",
              "      <td>0.185620</td>\n",
              "      <td>7659.698716</td>\n",
              "      <td>0.008058</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>^GDAXI</td>\n",
              "      <td>0.133582</td>\n",
              "      <td>15508.188678</td>\n",
              "      <td>0.002577</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>^NSEI</td>\n",
              "      <td>0.002008</td>\n",
              "      <td>14287.035446</td>\n",
              "      <td>0.001567</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0f5baf2c-1033-49c9-8f38-ecc3978edc8f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0f5baf2c-1033-49c9-8f38-ecc3978edc8f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0f5baf2c-1033-49c9-8f38-ecc3978edc8f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(6,5))\n",
        "plt.bar(diz['stock'],diz['RMS'])\n",
        "plt.xlabel('Stock')\n",
        "plt.ylabel('RMS error')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "id": "X7w56XDURT-a",
        "outputId": "7bd6df14-d65b-4353-8619-f509533524b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'RMS error')"
            ]
          },
          "metadata": {},
          "execution_count": 30
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}